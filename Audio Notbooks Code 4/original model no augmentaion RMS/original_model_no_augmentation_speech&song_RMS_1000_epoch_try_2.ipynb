{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "original_model_no_augmentation_speech&song_RMS_1000 epoch try 2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install librosa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oi8pWV3yBVjO",
        "outputId": "6b5e28f2-2565-4a5b-f819-c5495c1ef84e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.7/dist-packages (0.8.1)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.6.0)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.4.1)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.51.2)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (2.1.9)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.21.6)\n",
            "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.10.3.post1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (21.3)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.2.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa) (0.34.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->librosa) (3.0.9)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa) (2.23.0)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa) (1.4.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (1.24.3)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.7/dist-packages (from resampy>=0.2.2->librosa) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.10.2->librosa) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa) (2.21)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJjcbxwy46bG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "779e7c79-79b2-4f3f-eed7-f0ab67822507"
      },
      "source": [
        "# Orignial Notebook: https://github.com/MITESHPUTHRANNEU/Speech-Emotion-Analyzer/blob/master/final_results_gender_test.ipynb\n",
        "# This notebook author: Reza Chu\n",
        "# Last Editing Date: 31st May 2019\n",
        "\n",
        "## Python\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "\n",
        "import IPython\n",
        "from IPython.display import Audio\n",
        "from IPython.display import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "## Package\n",
        "import glob \n",
        "import keras\n",
        "import IPython.display as ipd\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.graph_objs as go\n",
        "import plotly.offline as py\n",
        "import plotly.tools as tls\n",
        "import seaborn as sns\n",
        "import scipy.io.wavfile\n",
        "import tensorflow as tf\n",
        "py.init_notebook_mode(connected=True)\n",
        "\n",
        "## Keras\n",
        "from keras import regularizers\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping\n",
        "from keras.callbacks import  History, ReduceLROnPlateau, CSVLogger\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM\n",
        "from keras.layers import Input, Flatten, Dropout, Activation, BatchNormalization\n",
        "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import np_utils\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "## Sklearn\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "## Rest\n",
        "from scipy.fftpack import fft\n",
        "from scipy import signal\n",
        "from scipy.io import wavfile\n",
        "from tqdm import tqdm\n",
        "\n",
        "input_duration=3\n",
        "# % pylab inline"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "        <script type=\"text/javascript\">\n",
              "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
              "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
              "        if (typeof require !== 'undefined') {\n",
              "        require.undef(\"plotly\");\n",
              "        requirejs.config({\n",
              "            paths: {\n",
              "                'plotly': ['https://cdn.plot.ly/plotly-2.8.3.min']\n",
              "            }\n",
              "        });\n",
              "        require(['plotly'], function(Plotly) {\n",
              "            window._Plotly = Plotly;\n",
              "        });\n",
              "        }\n",
              "        </script>\n",
              "        "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#radvass speech and song\n",
        "import time\n",
        "import os\n",
        "import librosa\n",
        "from librosa import display\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "count=0\n",
        "path1 = '/content/drive/MyDrive/RAVDESS_speech'\n",
        "path2 = '/content/drive/MyDrive/RAVDESS_song'\n",
        "\n",
        "lst = []\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for subdir, dirs, files in os.walk(path2):\n",
        "  #print(dirs)\n",
        "  #print(subdir)\n",
        "  #print(files)\n",
        "  for file in files:\n",
        "      try:\n",
        "        count +=1\n",
        "        #Load librosa array, obtain mfcss, store the file and the mcss information in a new array\n",
        "        X, sample_rate = librosa.load(os.path.join(subdir,file), res_type='kaiser_fast')\n",
        "        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0) \n",
        "        # The instruction below converts the labels (from 1 to 8) to a series from 0 to 7\n",
        "        # This is because our predictor needs to start from 0 otherwise it will try to predict also 0.\n",
        "        file = int(file[7:8]) - 1 \n",
        "        arr = mfccs, file\n",
        "        lst.append(arr)\n",
        "      # If the file is not valid, skip it\n",
        "      except ValueError:\n",
        "        continue\n",
        "\n",
        "for subdir, dirs, files in os.walk(path1):\n",
        "  #print(dirs)\n",
        "  #print(subdir)\n",
        " # print(files)\n",
        "  for file in files:\n",
        "      try:\n",
        "        #Load librosa array, obtain mfcss, store the file and the mcss information in a new array\n",
        "        X, sample_rate = librosa.load(os.path.join(subdir,file), res_type='kaiser_fast')\n",
        "        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0) \n",
        "        # The instruction below converts the labels (from 1 to 8) to a series from 0 to 7\n",
        "        # This is because our predictor needs to start from 0 otherwise it will try to predict also 0.\n",
        "        file = int(file[7:8]) - 1 \n",
        "        if(file ==6 or file==7):\n",
        "          continue\n",
        "        count +=1\n",
        "        arr = mfccs, file\n",
        "        lst.append(arr)\n",
        "      # If the file is not valid, skip it\n",
        "      except ValueError:\n",
        "        continue\n",
        "\n",
        "\n",
        "\n",
        "print(\"--- Data loaded. Loading time: %s seconds ---\" % (time.time() - start_time))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfuyRZl42ccZ",
        "outputId": "395a11c3-0048-4d0d-8a86-f39934a67330"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Data loaded. Loading time: 861.8856067657471 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count"
      ],
      "metadata": {
        "id": "Ltlhi25L2AI5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81e23488-b55d-4b17-9b95-4948bfd16ae4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2068"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.core.fromnumeric import size\n",
        "size(lst)"
      ],
      "metadata": {
        "id": "HMOH7d7mzHB7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0038e90e-7509-437d-84c0-70a1d4b3a18b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3208: VisibleDeprecationWarning:\n",
            "\n",
            "Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4136"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating X and y: zip makes a list of all the first elements, and a list of all the second elements.\n",
        "X, y = zip(*lst)"
      ],
      "metadata": {
        "id": "ocdL7tSTBoMt"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "X = np.asarray(X)\n",
        "y = np.asarray(y)\n",
        "X.shape, y.shape"
      ],
      "metadata": {
        "id": "vHKwUGTbBtnF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3959f499-1b3f-4022-fa98-763ab5320874"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2068, 40), (2068,))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "id": "2sUCtbfMztE5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "611704fd-fb08-4b2b-f9c1-12e20d815bf7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2068, 40)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PSTurzjCo5K",
        "outputId": "1ecd9671-85b3-4633-dcc6-0de25cf60921"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2068, 40)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2,train_size=0.8, random_state=42)\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(X_test,y_test, test_size=0.5,train_size=0.5, random_state=42)"
      ],
      "metadata": {
        "id": "Ai4Fy5cPCiq1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "x_traincnn = np.expand_dims(X_train, axis=2)\n",
        "x_testcnn = np.expand_dims(X_test, axis=2)\n",
        "X_valid= np.expand_dims(X_valid, axis=2)"
      ],
      "metadata": {
        "id": "tp1Fm5K3CEXu"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_traincnn.shape, x_testcnn.shape , X_valid.shape"
      ],
      "metadata": {
        "id": "RI0MxoIPBws5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "041b772c-81a9-4a55-ae18-081bebb32948"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1654, 40, 1), (207, 40, 1), (207, 40, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "goxL4Y3jKICC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e91d17a-0684-40f0-c066-3b6680567b7e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.8.2+zzzcolab20220527125636)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.0.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (14.0.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.26.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (4.2.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.21.6)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.46.3)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.3.7)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.11.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras"
      ],
      "metadata": {
        "id": "EkcmN9UrKSxj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ba41a8c-ce43-4009-fc78-9e19713543e3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.8.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.layers import Input, Flatten, Dropout, Activation\n",
        "from keras.layers import Conv1D, MaxPooling1D\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras import optimizers\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv1D(128, 8,padding='same', #classifier.add(Convolution2D(64, (3, 3), padding = 'same', input_shape = (128, 128, 3), activation = 'relu'))\n",
        "                 input_shape=(40,1)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(MaxPooling1D(pool_size=(5)))\n",
        "\n",
        "\n",
        "model.add(Conv1D(256,8,padding='same',))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(MaxPooling1D(pool_size=(5)))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(256))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(256))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Dense(6))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "#opt = tf.keras.optimizers.Adam(lr=0.00002)\n",
        "opt = tf.keras.optimizers.RMSprop(lr=0.00002, rho=0.9, epsilon=1e-07, decay=0.0)\n",
        "#opt = tf.keras.optimizers.SGD(lr=0.001, momentum=0.0, decay=0.0, nesterov=False)"
      ],
      "metadata": {
        "id": "g74fXWVAC4Cr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd7f804e-aace-4a12-98dc-b37e83e3360a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning:\n",
            "\n",
            "The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6ukOxAGC_I4",
        "outputId": "f0fb4646-5b23-4ce0-fbc6-dce4b16ee445"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d (Conv1D)             (None, 40, 128)           1152      \n",
            "                                                                 \n",
            " activation (Activation)     (None, 40, 128)           0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 40, 128)           0         \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, 8, 128)           0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 8, 256)            262400    \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 8, 256)            0         \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 8, 256)            0         \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPooling  (None, 1, 256)           0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               65792     \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 256)               65792     \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 256)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 6)                 1542      \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 6)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 396,678\n",
            "Trainable params: 396,678\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "AbMlLNk4DCBM"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cnnhistory=model.fit(x_traincnn, y_train, batch_size=16, epochs=700, validation_data=(x_testcnn, y_test))\n",
        "cnnhistory=model.fit(x_traincnn, y_train, batch_size=16, epochs=1000 , validation_data=(X_valid, y_valid))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RI1v2AuADFhy",
        "outputId": "174df86d-585f-4e5f-aec4-66f8d4b9d578"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "104/104 [==============================] - 16s 12ms/step - loss: 5.4613 - accuracy: 0.1886 - val_loss: 1.8532 - val_accuracy: 0.2029\n",
            "Epoch 2/1000\n",
            "104/104 [==============================] - 1s 8ms/step - loss: 4.5148 - accuracy: 0.1753 - val_loss: 1.8944 - val_accuracy: 0.2077\n",
            "Epoch 3/1000\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 3.9054 - accuracy: 0.1808 - val_loss: 1.8434 - val_accuracy: 0.1981\n",
            "Epoch 4/1000\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 3.3067 - accuracy: 0.1953 - val_loss: 1.8268 - val_accuracy: 0.2367\n",
            "Epoch 5/1000\n",
            "104/104 [==============================] - 1s 9ms/step - loss: 2.9600 - accuracy: 0.1953 - val_loss: 1.7517 - val_accuracy: 0.2560\n",
            "Epoch 6/1000\n",
            "104/104 [==============================] - 1s 9ms/step - loss: 2.8576 - accuracy: 0.1923 - val_loss: 1.8307 - val_accuracy: 0.2271\n",
            "Epoch 7/1000\n",
            "104/104 [==============================] - 1s 8ms/step - loss: 2.6962 - accuracy: 0.2080 - val_loss: 1.8636 - val_accuracy: 0.1449\n",
            "Epoch 8/1000\n",
            "104/104 [==============================] - 1s 8ms/step - loss: 2.5639 - accuracy: 0.1947 - val_loss: 1.7839 - val_accuracy: 0.2174\n",
            "Epoch 9/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 2.3988 - accuracy: 0.2068 - val_loss: 1.8171 - val_accuracy: 0.2174\n",
            "Epoch 10/1000\n",
            "104/104 [==============================] - 1s 8ms/step - loss: 2.2959 - accuracy: 0.2092 - val_loss: 1.7941 - val_accuracy: 0.2512\n",
            "Epoch 11/1000\n",
            "104/104 [==============================] - 1s 9ms/step - loss: 2.2339 - accuracy: 0.1965 - val_loss: 1.7201 - val_accuracy: 0.2415\n",
            "Epoch 12/1000\n",
            "104/104 [==============================] - 1s 8ms/step - loss: 2.2340 - accuracy: 0.2068 - val_loss: 1.7826 - val_accuracy: 0.2609\n",
            "Epoch 13/1000\n",
            "104/104 [==============================] - 1s 9ms/step - loss: 2.1690 - accuracy: 0.2086 - val_loss: 1.7740 - val_accuracy: 0.2415\n",
            "Epoch 14/1000\n",
            "104/104 [==============================] - 1s 8ms/step - loss: 2.1196 - accuracy: 0.2019 - val_loss: 1.7084 - val_accuracy: 0.2464\n",
            "Epoch 15/1000\n",
            "104/104 [==============================] - 1s 8ms/step - loss: 2.1039 - accuracy: 0.2037 - val_loss: 1.7427 - val_accuracy: 0.2415\n",
            "Epoch 16/1000\n",
            "104/104 [==============================] - 1s 8ms/step - loss: 2.0119 - accuracy: 0.2164 - val_loss: 1.7924 - val_accuracy: 0.1691\n",
            "Epoch 17/1000\n",
            "104/104 [==============================] - 1s 8ms/step - loss: 2.0093 - accuracy: 0.2122 - val_loss: 1.7362 - val_accuracy: 0.2512\n",
            "Epoch 18/1000\n",
            "104/104 [==============================] - 1s 8ms/step - loss: 1.9907 - accuracy: 0.2122 - val_loss: 1.7143 - val_accuracy: 0.2802\n",
            "Epoch 19/1000\n",
            "104/104 [==============================] - 1s 9ms/step - loss: 1.9827 - accuracy: 0.2062 - val_loss: 1.6977 - val_accuracy: 0.2754\n",
            "Epoch 20/1000\n",
            "104/104 [==============================] - 1s 9ms/step - loss: 1.9248 - accuracy: 0.2201 - val_loss: 1.6900 - val_accuracy: 0.2609\n",
            "Epoch 21/1000\n",
            "104/104 [==============================] - 1s 9ms/step - loss: 1.9404 - accuracy: 0.2189 - val_loss: 1.6826 - val_accuracy: 0.2464\n",
            "Epoch 22/1000\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 1.9113 - accuracy: 0.2255 - val_loss: 1.7398 - val_accuracy: 0.2464\n",
            "Epoch 23/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 1.9098 - accuracy: 0.2134 - val_loss: 1.7066 - val_accuracy: 0.2415\n",
            "Epoch 24/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 1.8942 - accuracy: 0.2340 - val_loss: 1.7061 - val_accuracy: 0.2850\n",
            "Epoch 25/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.8757 - accuracy: 0.2231 - val_loss: 1.7194 - val_accuracy: 0.2271\n",
            "Epoch 26/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.8404 - accuracy: 0.2449 - val_loss: 1.6814 - val_accuracy: 0.3043\n",
            "Epoch 27/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.8650 - accuracy: 0.2310 - val_loss: 1.6827 - val_accuracy: 0.2319\n",
            "Epoch 28/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.8155 - accuracy: 0.2515 - val_loss: 1.6662 - val_accuracy: 0.2705\n",
            "Epoch 29/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.8215 - accuracy: 0.2449 - val_loss: 1.6890 - val_accuracy: 0.2995\n",
            "Epoch 30/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.8141 - accuracy: 0.2515 - val_loss: 1.6515 - val_accuracy: 0.3816\n",
            "Epoch 31/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.8183 - accuracy: 0.2515 - val_loss: 1.6467 - val_accuracy: 0.2947\n",
            "Epoch 32/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.8134 - accuracy: 0.2406 - val_loss: 1.6452 - val_accuracy: 0.3092\n",
            "Epoch 33/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.7798 - accuracy: 0.2461 - val_loss: 1.6861 - val_accuracy: 0.3140\n",
            "Epoch 34/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.8136 - accuracy: 0.2304 - val_loss: 1.6664 - val_accuracy: 0.3527\n",
            "Epoch 35/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.7818 - accuracy: 0.2648 - val_loss: 1.6658 - val_accuracy: 0.2899\n",
            "Epoch 36/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.7593 - accuracy: 0.2582 - val_loss: 1.6939 - val_accuracy: 0.2899\n",
            "Epoch 37/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.7968 - accuracy: 0.2491 - val_loss: 1.6557 - val_accuracy: 0.3720\n",
            "Epoch 38/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.7780 - accuracy: 0.2473 - val_loss: 1.6896 - val_accuracy: 0.2512\n",
            "Epoch 39/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.7207 - accuracy: 0.2612 - val_loss: 1.6624 - val_accuracy: 0.2850\n",
            "Epoch 40/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.7378 - accuracy: 0.2817 - val_loss: 1.6830 - val_accuracy: 0.2657\n",
            "Epoch 41/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.7544 - accuracy: 0.2666 - val_loss: 1.6363 - val_accuracy: 0.3575\n",
            "Epoch 42/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.7200 - accuracy: 0.2769 - val_loss: 1.6137 - val_accuracy: 0.3671\n",
            "Epoch 43/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.7141 - accuracy: 0.2636 - val_loss: 1.7185 - val_accuracy: 0.2029\n",
            "Epoch 44/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.7286 - accuracy: 0.2715 - val_loss: 1.6300 - val_accuracy: 0.2560\n",
            "Epoch 45/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.7202 - accuracy: 0.2745 - val_loss: 1.6019 - val_accuracy: 0.3237\n",
            "Epoch 46/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.6960 - accuracy: 0.3035 - val_loss: 1.6260 - val_accuracy: 0.2947\n",
            "Epoch 47/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.6892 - accuracy: 0.2932 - val_loss: 1.6292 - val_accuracy: 0.2754\n",
            "Epoch 48/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.7079 - accuracy: 0.2878 - val_loss: 1.6172 - val_accuracy: 0.3188\n",
            "Epoch 49/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.6753 - accuracy: 0.3156 - val_loss: 1.6444 - val_accuracy: 0.2754\n",
            "Epoch 50/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.6820 - accuracy: 0.2956 - val_loss: 1.6147 - val_accuracy: 0.3043\n",
            "Epoch 51/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.7067 - accuracy: 0.2878 - val_loss: 1.6290 - val_accuracy: 0.3382\n",
            "Epoch 52/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.6911 - accuracy: 0.2799 - val_loss: 1.6206 - val_accuracy: 0.2754\n",
            "Epoch 53/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.6806 - accuracy: 0.2938 - val_loss: 1.6148 - val_accuracy: 0.3092\n",
            "Epoch 54/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.6705 - accuracy: 0.3035 - val_loss: 1.5963 - val_accuracy: 0.3188\n",
            "Epoch 55/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.6448 - accuracy: 0.2999 - val_loss: 1.6281 - val_accuracy: 0.2995\n",
            "Epoch 56/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.6328 - accuracy: 0.3247 - val_loss: 1.5715 - val_accuracy: 0.3527\n",
            "Epoch 57/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.6465 - accuracy: 0.3265 - val_loss: 1.5669 - val_accuracy: 0.3865\n",
            "Epoch 58/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.6365 - accuracy: 0.2975 - val_loss: 1.5471 - val_accuracy: 0.3720\n",
            "Epoch 59/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.6371 - accuracy: 0.3192 - val_loss: 1.5676 - val_accuracy: 0.3430\n",
            "Epoch 60/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.6429 - accuracy: 0.3102 - val_loss: 1.5585 - val_accuracy: 0.3527\n",
            "Epoch 61/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.6215 - accuracy: 0.3416 - val_loss: 1.5483 - val_accuracy: 0.3237\n",
            "Epoch 62/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.6208 - accuracy: 0.3180 - val_loss: 1.5468 - val_accuracy: 0.3671\n",
            "Epoch 63/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.5969 - accuracy: 0.3392 - val_loss: 1.5291 - val_accuracy: 0.4010\n",
            "Epoch 64/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.6106 - accuracy: 0.3319 - val_loss: 1.5518 - val_accuracy: 0.3768\n",
            "Epoch 65/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.6037 - accuracy: 0.3531 - val_loss: 1.5487 - val_accuracy: 0.3816\n",
            "Epoch 66/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.6225 - accuracy: 0.3337 - val_loss: 1.5436 - val_accuracy: 0.3720\n",
            "Epoch 67/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.6061 - accuracy: 0.3271 - val_loss: 1.5169 - val_accuracy: 0.3961\n",
            "Epoch 68/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.5947 - accuracy: 0.3337 - val_loss: 1.5415 - val_accuracy: 0.3382\n",
            "Epoch 69/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.5759 - accuracy: 0.3386 - val_loss: 1.5575 - val_accuracy: 0.3671\n",
            "Epoch 70/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.5808 - accuracy: 0.3410 - val_loss: 1.5515 - val_accuracy: 0.3527\n",
            "Epoch 71/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.5740 - accuracy: 0.3501 - val_loss: 1.5261 - val_accuracy: 0.3671\n",
            "Epoch 72/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.5768 - accuracy: 0.3525 - val_loss: 1.5216 - val_accuracy: 0.4010\n",
            "Epoch 73/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.5869 - accuracy: 0.3247 - val_loss: 1.5145 - val_accuracy: 0.3961\n",
            "Epoch 74/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.5541 - accuracy: 0.3646 - val_loss: 1.5217 - val_accuracy: 0.3961\n",
            "Epoch 75/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.5785 - accuracy: 0.3489 - val_loss: 1.5161 - val_accuracy: 0.4010\n",
            "Epoch 76/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.5587 - accuracy: 0.3555 - val_loss: 1.4944 - val_accuracy: 0.4300\n",
            "Epoch 77/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.5605 - accuracy: 0.3374 - val_loss: 1.4901 - val_accuracy: 0.4203\n",
            "Epoch 78/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.5378 - accuracy: 0.3652 - val_loss: 1.4733 - val_accuracy: 0.4155\n",
            "Epoch 79/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.5512 - accuracy: 0.3476 - val_loss: 1.4920 - val_accuracy: 0.3961\n",
            "Epoch 80/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.5437 - accuracy: 0.3555 - val_loss: 1.5115 - val_accuracy: 0.3913\n",
            "Epoch 81/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.5361 - accuracy: 0.3579 - val_loss: 1.4783 - val_accuracy: 0.3816\n",
            "Epoch 82/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.5275 - accuracy: 0.3579 - val_loss: 1.4830 - val_accuracy: 0.3913\n",
            "Epoch 83/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.5085 - accuracy: 0.3761 - val_loss: 1.4776 - val_accuracy: 0.3961\n",
            "Epoch 84/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.5173 - accuracy: 0.3833 - val_loss: 1.4812 - val_accuracy: 0.4300\n",
            "Epoch 85/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.5091 - accuracy: 0.3809 - val_loss: 1.4624 - val_accuracy: 0.4010\n",
            "Epoch 86/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.5231 - accuracy: 0.3591 - val_loss: 1.4785 - val_accuracy: 0.3575\n",
            "Epoch 87/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.5088 - accuracy: 0.3748 - val_loss: 1.4768 - val_accuracy: 0.3913\n",
            "Epoch 88/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.5152 - accuracy: 0.3664 - val_loss: 1.4723 - val_accuracy: 0.4155\n",
            "Epoch 89/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.5119 - accuracy: 0.3767 - val_loss: 1.4473 - val_accuracy: 0.4251\n",
            "Epoch 90/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.5133 - accuracy: 0.3700 - val_loss: 1.4484 - val_accuracy: 0.4300\n",
            "Epoch 91/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.4946 - accuracy: 0.3845 - val_loss: 1.4455 - val_accuracy: 0.4444\n",
            "Epoch 92/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.4788 - accuracy: 0.3797 - val_loss: 1.4306 - val_accuracy: 0.4010\n",
            "Epoch 93/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 1.4791 - accuracy: 0.3948 - val_loss: 1.4301 - val_accuracy: 0.4396\n",
            "Epoch 94/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.4738 - accuracy: 0.4033 - val_loss: 1.4041 - val_accuracy: 0.4734\n",
            "Epoch 95/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.4682 - accuracy: 0.3996 - val_loss: 1.4105 - val_accuracy: 0.4348\n",
            "Epoch 96/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.4645 - accuracy: 0.3978 - val_loss: 1.4179 - val_accuracy: 0.4444\n",
            "Epoch 97/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.4886 - accuracy: 0.3930 - val_loss: 1.3848 - val_accuracy: 0.4589\n",
            "Epoch 98/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.4756 - accuracy: 0.3803 - val_loss: 1.3982 - val_accuracy: 0.4589\n",
            "Epoch 99/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.4645 - accuracy: 0.3900 - val_loss: 1.3910 - val_accuracy: 0.4783\n",
            "Epoch 100/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.4549 - accuracy: 0.3972 - val_loss: 1.4046 - val_accuracy: 0.4106\n",
            "Epoch 101/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.4678 - accuracy: 0.3996 - val_loss: 1.3803 - val_accuracy: 0.4589\n",
            "Epoch 102/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.4591 - accuracy: 0.4087 - val_loss: 1.3781 - val_accuracy: 0.4589\n",
            "Epoch 103/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.4527 - accuracy: 0.4075 - val_loss: 1.3986 - val_accuracy: 0.4734\n",
            "Epoch 104/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.4488 - accuracy: 0.3978 - val_loss: 1.3695 - val_accuracy: 0.4686\n",
            "Epoch 105/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.4190 - accuracy: 0.4172 - val_loss: 1.3805 - val_accuracy: 0.4541\n",
            "Epoch 106/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.4282 - accuracy: 0.4129 - val_loss: 1.3589 - val_accuracy: 0.4879\n",
            "Epoch 107/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.4155 - accuracy: 0.4160 - val_loss: 1.3488 - val_accuracy: 0.4879\n",
            "Epoch 108/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.4037 - accuracy: 0.4365 - val_loss: 1.3411 - val_accuracy: 0.5217\n",
            "Epoch 109/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.4199 - accuracy: 0.4184 - val_loss: 1.3678 - val_accuracy: 0.4928\n",
            "Epoch 110/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.4116 - accuracy: 0.4166 - val_loss: 1.3406 - val_accuracy: 0.5169\n",
            "Epoch 111/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.4235 - accuracy: 0.4057 - val_loss: 1.3727 - val_accuracy: 0.4396\n",
            "Epoch 112/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.3893 - accuracy: 0.4395 - val_loss: 1.3376 - val_accuracy: 0.4976\n",
            "Epoch 113/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.3915 - accuracy: 0.4184 - val_loss: 1.3753 - val_accuracy: 0.4686\n",
            "Epoch 114/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.3918 - accuracy: 0.4208 - val_loss: 1.3487 - val_accuracy: 0.5121\n",
            "Epoch 115/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.3837 - accuracy: 0.4420 - val_loss: 1.3340 - val_accuracy: 0.4444\n",
            "Epoch 116/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.3891 - accuracy: 0.4317 - val_loss: 1.3234 - val_accuracy: 0.5266\n",
            "Epoch 117/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.3853 - accuracy: 0.4299 - val_loss: 1.3406 - val_accuracy: 0.5024\n",
            "Epoch 118/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.3704 - accuracy: 0.4353 - val_loss: 1.3088 - val_accuracy: 0.5459\n",
            "Epoch 119/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.3697 - accuracy: 0.4377 - val_loss: 1.3512 - val_accuracy: 0.4734\n",
            "Epoch 120/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.3666 - accuracy: 0.4365 - val_loss: 1.2990 - val_accuracy: 0.5411\n",
            "Epoch 121/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.3692 - accuracy: 0.4504 - val_loss: 1.3004 - val_accuracy: 0.5121\n",
            "Epoch 122/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.3707 - accuracy: 0.4305 - val_loss: 1.3104 - val_accuracy: 0.4928\n",
            "Epoch 123/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.3575 - accuracy: 0.4456 - val_loss: 1.2960 - val_accuracy: 0.5459\n",
            "Epoch 124/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.3797 - accuracy: 0.4438 - val_loss: 1.2822 - val_accuracy: 0.5362\n",
            "Epoch 125/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.3384 - accuracy: 0.4510 - val_loss: 1.3031 - val_accuracy: 0.5072\n",
            "Epoch 126/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.3518 - accuracy: 0.4456 - val_loss: 1.2773 - val_accuracy: 0.5459\n",
            "Epoch 127/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.3515 - accuracy: 0.4341 - val_loss: 1.2630 - val_accuracy: 0.5507\n",
            "Epoch 128/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.3496 - accuracy: 0.4492 - val_loss: 1.2598 - val_accuracy: 0.5411\n",
            "Epoch 129/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.3250 - accuracy: 0.4559 - val_loss: 1.2603 - val_accuracy: 0.5556\n",
            "Epoch 130/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.3202 - accuracy: 0.4674 - val_loss: 1.2682 - val_accuracy: 0.5362\n",
            "Epoch 131/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.3088 - accuracy: 0.4704 - val_loss: 1.2701 - val_accuracy: 0.5217\n",
            "Epoch 132/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.3159 - accuracy: 0.4704 - val_loss: 1.2604 - val_accuracy: 0.5556\n",
            "Epoch 133/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 1.3395 - accuracy: 0.4547 - val_loss: 1.2726 - val_accuracy: 0.5797\n",
            "Epoch 134/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.3061 - accuracy: 0.4625 - val_loss: 1.2593 - val_accuracy: 0.5749\n",
            "Epoch 135/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.3130 - accuracy: 0.4516 - val_loss: 1.2542 - val_accuracy: 0.5797\n",
            "Epoch 136/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.2910 - accuracy: 0.4649 - val_loss: 1.2433 - val_accuracy: 0.5507\n",
            "Epoch 137/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.3125 - accuracy: 0.4528 - val_loss: 1.2411 - val_accuracy: 0.5459\n",
            "Epoch 138/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.2925 - accuracy: 0.4770 - val_loss: 1.2267 - val_accuracy: 0.5507\n",
            "Epoch 139/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.2956 - accuracy: 0.4619 - val_loss: 1.2186 - val_accuracy: 0.5604\n",
            "Epoch 140/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.2836 - accuracy: 0.4843 - val_loss: 1.2307 - val_accuracy: 0.5604\n",
            "Epoch 141/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.2962 - accuracy: 0.4698 - val_loss: 1.2168 - val_accuracy: 0.5604\n",
            "Epoch 142/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.3018 - accuracy: 0.4667 - val_loss: 1.2250 - val_accuracy: 0.5604\n",
            "Epoch 143/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.2804 - accuracy: 0.4722 - val_loss: 1.2149 - val_accuracy: 0.5797\n",
            "Epoch 144/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.2997 - accuracy: 0.4728 - val_loss: 1.2186 - val_accuracy: 0.5507\n",
            "Epoch 145/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.2860 - accuracy: 0.4800 - val_loss: 1.2013 - val_accuracy: 0.6087\n",
            "Epoch 146/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 1.2699 - accuracy: 0.4788 - val_loss: 1.2283 - val_accuracy: 0.5604\n",
            "Epoch 147/1000\n",
            "104/104 [==============================] - 1s 8ms/step - loss: 1.2640 - accuracy: 0.4897 - val_loss: 1.2109 - val_accuracy: 0.5507\n",
            "Epoch 148/1000\n",
            "104/104 [==============================] - 1s 8ms/step - loss: 1.2544 - accuracy: 0.4921 - val_loss: 1.2142 - val_accuracy: 0.5459\n",
            "Epoch 149/1000\n",
            "104/104 [==============================] - 1s 9ms/step - loss: 1.2875 - accuracy: 0.4643 - val_loss: 1.2271 - val_accuracy: 0.5556\n",
            "Epoch 150/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 1.2682 - accuracy: 0.4782 - val_loss: 1.2143 - val_accuracy: 0.5362\n",
            "Epoch 151/1000\n",
            "104/104 [==============================] - 1s 9ms/step - loss: 1.2607 - accuracy: 0.4831 - val_loss: 1.1934 - val_accuracy: 0.5700\n",
            "Epoch 152/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 1.2723 - accuracy: 0.4680 - val_loss: 1.2217 - val_accuracy: 0.5652\n",
            "Epoch 153/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.2325 - accuracy: 0.4988 - val_loss: 1.2083 - val_accuracy: 0.5411\n",
            "Epoch 154/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.2504 - accuracy: 0.4831 - val_loss: 1.2273 - val_accuracy: 0.5459\n",
            "Epoch 155/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.2572 - accuracy: 0.4831 - val_loss: 1.1726 - val_accuracy: 0.5990\n",
            "Epoch 156/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.2419 - accuracy: 0.5054 - val_loss: 1.1769 - val_accuracy: 0.5990\n",
            "Epoch 157/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.2337 - accuracy: 0.5109 - val_loss: 1.1777 - val_accuracy: 0.5894\n",
            "Epoch 158/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.2491 - accuracy: 0.5000 - val_loss: 1.1744 - val_accuracy: 0.6039\n",
            "Epoch 159/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.2287 - accuracy: 0.4946 - val_loss: 1.1738 - val_accuracy: 0.5700\n",
            "Epoch 160/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.2333 - accuracy: 0.4897 - val_loss: 1.1819 - val_accuracy: 0.5749\n",
            "Epoch 161/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.2259 - accuracy: 0.5036 - val_loss: 1.1683 - val_accuracy: 0.6039\n",
            "Epoch 162/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.2148 - accuracy: 0.5060 - val_loss: 1.1636 - val_accuracy: 0.6135\n",
            "Epoch 163/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 1.2119 - accuracy: 0.5060 - val_loss: 1.1514 - val_accuracy: 0.5894\n",
            "Epoch 164/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.2205 - accuracy: 0.5157 - val_loss: 1.1569 - val_accuracy: 0.5894\n",
            "Epoch 165/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.2468 - accuracy: 0.4897 - val_loss: 1.1491 - val_accuracy: 0.5990\n",
            "Epoch 166/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.2090 - accuracy: 0.5018 - val_loss: 1.1616 - val_accuracy: 0.5797\n",
            "Epoch 167/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.2028 - accuracy: 0.5091 - val_loss: 1.1654 - val_accuracy: 0.6135\n",
            "Epoch 168/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.1993 - accuracy: 0.5139 - val_loss: 1.1336 - val_accuracy: 0.6087\n",
            "Epoch 169/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.2101 - accuracy: 0.5036 - val_loss: 1.1336 - val_accuracy: 0.5990\n",
            "Epoch 170/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.2082 - accuracy: 0.5079 - val_loss: 1.1415 - val_accuracy: 0.5749\n",
            "Epoch 171/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.2079 - accuracy: 0.5266 - val_loss: 1.1326 - val_accuracy: 0.6135\n",
            "Epoch 172/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.1960 - accuracy: 0.5266 - val_loss: 1.1182 - val_accuracy: 0.6232\n",
            "Epoch 173/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.1859 - accuracy: 0.5133 - val_loss: 1.1221 - val_accuracy: 0.5990\n",
            "Epoch 174/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.1975 - accuracy: 0.5097 - val_loss: 1.1457 - val_accuracy: 0.5990\n",
            "Epoch 175/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.1775 - accuracy: 0.5193 - val_loss: 1.1151 - val_accuracy: 0.6377\n",
            "Epoch 176/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.1928 - accuracy: 0.5175 - val_loss: 1.1428 - val_accuracy: 0.5894\n",
            "Epoch 177/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.1663 - accuracy: 0.5326 - val_loss: 1.1187 - val_accuracy: 0.6184\n",
            "Epoch 178/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.1794 - accuracy: 0.5200 - val_loss: 1.1354 - val_accuracy: 0.6232\n",
            "Epoch 179/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.1635 - accuracy: 0.5387 - val_loss: 1.1384 - val_accuracy: 0.6039\n",
            "Epoch 180/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.1674 - accuracy: 0.5157 - val_loss: 1.1202 - val_accuracy: 0.6039\n",
            "Epoch 181/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 1.1648 - accuracy: 0.5272 - val_loss: 1.1159 - val_accuracy: 0.6329\n",
            "Epoch 182/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 1.1546 - accuracy: 0.5453 - val_loss: 1.1243 - val_accuracy: 0.6329\n",
            "Epoch 183/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.1748 - accuracy: 0.5260 - val_loss: 1.1086 - val_accuracy: 0.5990\n",
            "Epoch 184/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.1674 - accuracy: 0.5405 - val_loss: 1.0988 - val_accuracy: 0.6232\n",
            "Epoch 185/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.1564 - accuracy: 0.5435 - val_loss: 1.1143 - val_accuracy: 0.6184\n",
            "Epoch 186/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.1559 - accuracy: 0.5478 - val_loss: 1.1124 - val_accuracy: 0.5990\n",
            "Epoch 187/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.1589 - accuracy: 0.5381 - val_loss: 1.1045 - val_accuracy: 0.6087\n",
            "Epoch 188/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.1382 - accuracy: 0.5441 - val_loss: 1.1142 - val_accuracy: 0.5749\n",
            "Epoch 189/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.1556 - accuracy: 0.5453 - val_loss: 1.0959 - val_accuracy: 0.5894\n",
            "Epoch 190/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.1391 - accuracy: 0.5405 - val_loss: 1.1116 - val_accuracy: 0.5845\n",
            "Epoch 191/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.1347 - accuracy: 0.5526 - val_loss: 1.0798 - val_accuracy: 0.6377\n",
            "Epoch 192/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.1436 - accuracy: 0.5326 - val_loss: 1.1223 - val_accuracy: 0.5556\n",
            "Epoch 193/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.1346 - accuracy: 0.5544 - val_loss: 1.0839 - val_accuracy: 0.5990\n",
            "Epoch 194/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.1339 - accuracy: 0.5544 - val_loss: 1.0742 - val_accuracy: 0.6087\n",
            "Epoch 195/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.1470 - accuracy: 0.5435 - val_loss: 1.0813 - val_accuracy: 0.6570\n",
            "Epoch 196/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.1060 - accuracy: 0.5568 - val_loss: 1.0875 - val_accuracy: 0.6232\n",
            "Epoch 197/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.1312 - accuracy: 0.5520 - val_loss: 1.0815 - val_accuracy: 0.5990\n",
            "Epoch 198/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0929 - accuracy: 0.5417 - val_loss: 1.0693 - val_accuracy: 0.6425\n",
            "Epoch 199/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.1144 - accuracy: 0.5472 - val_loss: 1.0726 - val_accuracy: 0.6280\n",
            "Epoch 200/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.1241 - accuracy: 0.5435 - val_loss: 1.0721 - val_accuracy: 0.6377\n",
            "Epoch 201/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.1247 - accuracy: 0.5502 - val_loss: 1.0826 - val_accuracy: 0.6232\n",
            "Epoch 202/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.1214 - accuracy: 0.5508 - val_loss: 1.0804 - val_accuracy: 0.6135\n",
            "Epoch 203/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.1158 - accuracy: 0.5435 - val_loss: 1.0551 - val_accuracy: 0.6377\n",
            "Epoch 204/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.1006 - accuracy: 0.5490 - val_loss: 1.0610 - val_accuracy: 0.6329\n",
            "Epoch 205/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.1043 - accuracy: 0.5526 - val_loss: 1.0723 - val_accuracy: 0.6425\n",
            "Epoch 206/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0971 - accuracy: 0.5580 - val_loss: 1.0594 - val_accuracy: 0.6232\n",
            "Epoch 207/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0774 - accuracy: 0.5719 - val_loss: 1.0713 - val_accuracy: 0.6473\n",
            "Epoch 208/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.1047 - accuracy: 0.5635 - val_loss: 1.0319 - val_accuracy: 0.6184\n",
            "Epoch 209/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0924 - accuracy: 0.5562 - val_loss: 1.0450 - val_accuracy: 0.6280\n",
            "Epoch 210/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0917 - accuracy: 0.5689 - val_loss: 1.0322 - val_accuracy: 0.6377\n",
            "Epoch 211/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0875 - accuracy: 0.5641 - val_loss: 1.0332 - val_accuracy: 0.6522\n",
            "Epoch 212/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0783 - accuracy: 0.5732 - val_loss: 1.0447 - val_accuracy: 0.6232\n",
            "Epoch 213/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.0685 - accuracy: 0.5871 - val_loss: 1.0517 - val_accuracy: 0.5990\n",
            "Epoch 214/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.1021 - accuracy: 0.5599 - val_loss: 1.0276 - val_accuracy: 0.6232\n",
            "Epoch 215/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0936 - accuracy: 0.5496 - val_loss: 1.0486 - val_accuracy: 0.6087\n",
            "Epoch 216/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 1.0826 - accuracy: 0.5732 - val_loss: 1.0279 - val_accuracy: 0.6522\n",
            "Epoch 217/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0825 - accuracy: 0.5647 - val_loss: 1.0276 - val_accuracy: 0.6377\n",
            "Epoch 218/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0766 - accuracy: 0.5635 - val_loss: 1.0244 - val_accuracy: 0.6570\n",
            "Epoch 219/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 1.0675 - accuracy: 0.5744 - val_loss: 1.0201 - val_accuracy: 0.6377\n",
            "Epoch 220/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 1.0548 - accuracy: 0.5635 - val_loss: 1.0277 - val_accuracy: 0.6618\n",
            "Epoch 221/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 1.0731 - accuracy: 0.5635 - val_loss: 1.0297 - val_accuracy: 0.6667\n",
            "Epoch 222/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 1.0610 - accuracy: 0.5889 - val_loss: 1.0254 - val_accuracy: 0.6473\n",
            "Epoch 223/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 1.0479 - accuracy: 0.5768 - val_loss: 1.0339 - val_accuracy: 0.6377\n",
            "Epoch 224/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 1.0584 - accuracy: 0.5768 - val_loss: 1.0395 - val_accuracy: 0.6280\n",
            "Epoch 225/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0389 - accuracy: 0.5859 - val_loss: 1.0184 - val_accuracy: 0.6425\n",
            "Epoch 226/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0352 - accuracy: 0.5907 - val_loss: 1.0042 - val_accuracy: 0.6425\n",
            "Epoch 227/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0401 - accuracy: 0.5889 - val_loss: 1.0000 - val_accuracy: 0.6570\n",
            "Epoch 228/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 1.0403 - accuracy: 0.5846 - val_loss: 1.0095 - val_accuracy: 0.6473\n",
            "Epoch 229/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0458 - accuracy: 0.5641 - val_loss: 1.0113 - val_accuracy: 0.6280\n",
            "Epoch 230/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0418 - accuracy: 0.5701 - val_loss: 1.0173 - val_accuracy: 0.6184\n",
            "Epoch 231/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0367 - accuracy: 0.5967 - val_loss: 1.0076 - val_accuracy: 0.6232\n",
            "Epoch 232/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0208 - accuracy: 0.5967 - val_loss: 0.9942 - val_accuracy: 0.6522\n",
            "Epoch 233/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0300 - accuracy: 0.5871 - val_loss: 1.0079 - val_accuracy: 0.6473\n",
            "Epoch 234/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0363 - accuracy: 0.5865 - val_loss: 1.0040 - val_accuracy: 0.6280\n",
            "Epoch 235/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0233 - accuracy: 0.5973 - val_loss: 0.9974 - val_accuracy: 0.6329\n",
            "Epoch 236/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0439 - accuracy: 0.5852 - val_loss: 0.9971 - val_accuracy: 0.6667\n",
            "Epoch 237/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0131 - accuracy: 0.5877 - val_loss: 0.9912 - val_accuracy: 0.6473\n",
            "Epoch 238/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0253 - accuracy: 0.5943 - val_loss: 0.9951 - val_accuracy: 0.6570\n",
            "Epoch 239/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0311 - accuracy: 0.5931 - val_loss: 0.9881 - val_accuracy: 0.6280\n",
            "Epoch 240/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0099 - accuracy: 0.6046 - val_loss: 0.9834 - val_accuracy: 0.6667\n",
            "Epoch 241/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0147 - accuracy: 0.6070 - val_loss: 0.9836 - val_accuracy: 0.6957\n",
            "Epoch 242/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0278 - accuracy: 0.5883 - val_loss: 0.9899 - val_accuracy: 0.6570\n",
            "Epoch 243/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0224 - accuracy: 0.6046 - val_loss: 0.9778 - val_accuracy: 0.6522\n",
            "Epoch 244/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0303 - accuracy: 0.5750 - val_loss: 0.9982 - val_accuracy: 0.6618\n",
            "Epoch 245/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0052 - accuracy: 0.6022 - val_loss: 0.9787 - val_accuracy: 0.6473\n",
            "Epoch 246/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0047 - accuracy: 0.5919 - val_loss: 0.9780 - val_accuracy: 0.6618\n",
            "Epoch 247/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0134 - accuracy: 0.6028 - val_loss: 1.0075 - val_accuracy: 0.5990\n",
            "Epoch 248/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 1.0118 - accuracy: 0.6004 - val_loss: 0.9861 - val_accuracy: 0.6329\n",
            "Epoch 249/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9986 - accuracy: 0.6016 - val_loss: 0.9804 - val_accuracy: 0.6377\n",
            "Epoch 250/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9954 - accuracy: 0.6052 - val_loss: 0.9821 - val_accuracy: 0.6570\n",
            "Epoch 251/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0074 - accuracy: 0.5998 - val_loss: 0.9764 - val_accuracy: 0.6473\n",
            "Epoch 252/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 1.0037 - accuracy: 0.6004 - val_loss: 0.9803 - val_accuracy: 0.6232\n",
            "Epoch 253/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0074 - accuracy: 0.5955 - val_loss: 0.9805 - val_accuracy: 0.6522\n",
            "Epoch 254/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9641 - accuracy: 0.6215 - val_loss: 0.9711 - val_accuracy: 0.6763\n",
            "Epoch 255/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9869 - accuracy: 0.6070 - val_loss: 0.9631 - val_accuracy: 0.6522\n",
            "Epoch 256/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9822 - accuracy: 0.6088 - val_loss: 0.9742 - val_accuracy: 0.6184\n",
            "Epoch 257/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 1.0004 - accuracy: 0.5973 - val_loss: 0.9669 - val_accuracy: 0.6473\n",
            "Epoch 258/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.9799 - accuracy: 0.6324 - val_loss: 0.9480 - val_accuracy: 0.6618\n",
            "Epoch 259/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9845 - accuracy: 0.5937 - val_loss: 0.9648 - val_accuracy: 0.6667\n",
            "Epoch 260/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9916 - accuracy: 0.6052 - val_loss: 0.9695 - val_accuracy: 0.6522\n",
            "Epoch 261/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9635 - accuracy: 0.6197 - val_loss: 0.9607 - val_accuracy: 0.6329\n",
            "Epoch 262/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9908 - accuracy: 0.6046 - val_loss: 0.9713 - val_accuracy: 0.6522\n",
            "Epoch 263/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9679 - accuracy: 0.6131 - val_loss: 0.9639 - val_accuracy: 0.6280\n",
            "Epoch 264/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9602 - accuracy: 0.6203 - val_loss: 0.9614 - val_accuracy: 0.6425\n",
            "Epoch 265/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9584 - accuracy: 0.6112 - val_loss: 0.9647 - val_accuracy: 0.6667\n",
            "Epoch 266/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9548 - accuracy: 0.6161 - val_loss: 0.9397 - val_accuracy: 0.6763\n",
            "Epoch 267/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9567 - accuracy: 0.6191 - val_loss: 0.9711 - val_accuracy: 0.6329\n",
            "Epoch 268/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9868 - accuracy: 0.6088 - val_loss: 0.9580 - val_accuracy: 0.6522\n",
            "Epoch 269/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9438 - accuracy: 0.6282 - val_loss: 0.9514 - val_accuracy: 0.6715\n",
            "Epoch 270/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.9645 - accuracy: 0.6155 - val_loss: 0.9498 - val_accuracy: 0.6570\n",
            "Epoch 271/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9783 - accuracy: 0.6058 - val_loss: 0.9352 - val_accuracy: 0.6763\n",
            "Epoch 272/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.9462 - accuracy: 0.6137 - val_loss: 0.9543 - val_accuracy: 0.6425\n",
            "Epoch 273/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.9584 - accuracy: 0.6227 - val_loss: 0.9469 - val_accuracy: 0.6280\n",
            "Epoch 274/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9526 - accuracy: 0.6137 - val_loss: 0.9517 - val_accuracy: 0.6473\n",
            "Epoch 275/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9307 - accuracy: 0.6397 - val_loss: 0.9463 - val_accuracy: 0.6425\n",
            "Epoch 276/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9626 - accuracy: 0.6185 - val_loss: 0.9395 - val_accuracy: 0.6667\n",
            "Epoch 277/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.9505 - accuracy: 0.6179 - val_loss: 0.9450 - val_accuracy: 0.6570\n",
            "Epoch 278/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9247 - accuracy: 0.6385 - val_loss: 0.9339 - val_accuracy: 0.6377\n",
            "Epoch 279/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9521 - accuracy: 0.6197 - val_loss: 0.9347 - val_accuracy: 0.6473\n",
            "Epoch 280/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9471 - accuracy: 0.6094 - val_loss: 0.9240 - val_accuracy: 0.6860\n",
            "Epoch 281/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9402 - accuracy: 0.6306 - val_loss: 0.9203 - val_accuracy: 0.6812\n",
            "Epoch 282/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.9518 - accuracy: 0.6312 - val_loss: 0.9370 - val_accuracy: 0.6522\n",
            "Epoch 283/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9321 - accuracy: 0.6215 - val_loss: 0.9325 - val_accuracy: 0.6425\n",
            "Epoch 284/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.9669 - accuracy: 0.6233 - val_loss: 0.9477 - val_accuracy: 0.6570\n",
            "Epoch 285/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.9470 - accuracy: 0.6106 - val_loss: 0.9187 - val_accuracy: 0.7053\n",
            "Epoch 286/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9317 - accuracy: 0.6354 - val_loss: 0.9333 - val_accuracy: 0.6377\n",
            "Epoch 287/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9183 - accuracy: 0.6397 - val_loss: 0.9270 - val_accuracy: 0.6812\n",
            "Epoch 288/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9158 - accuracy: 0.6433 - val_loss: 0.9203 - val_accuracy: 0.6570\n",
            "Epoch 289/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9132 - accuracy: 0.6385 - val_loss: 0.9245 - val_accuracy: 0.6618\n",
            "Epoch 290/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9261 - accuracy: 0.6252 - val_loss: 0.9299 - val_accuracy: 0.6522\n",
            "Epoch 291/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.9611 - accuracy: 0.6155 - val_loss: 0.9254 - val_accuracy: 0.6522\n",
            "Epoch 292/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9275 - accuracy: 0.6221 - val_loss: 0.9249 - val_accuracy: 0.6715\n",
            "Epoch 293/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9325 - accuracy: 0.6348 - val_loss: 0.9404 - val_accuracy: 0.6522\n",
            "Epoch 294/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9270 - accuracy: 0.6330 - val_loss: 0.9207 - val_accuracy: 0.6522\n",
            "Epoch 295/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9328 - accuracy: 0.6252 - val_loss: 0.9162 - val_accuracy: 0.6763\n",
            "Epoch 296/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9036 - accuracy: 0.6469 - val_loss: 0.9400 - val_accuracy: 0.6329\n",
            "Epoch 297/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.9246 - accuracy: 0.6252 - val_loss: 0.9129 - val_accuracy: 0.6618\n",
            "Epoch 298/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.9095 - accuracy: 0.6493 - val_loss: 0.9143 - val_accuracy: 0.6667\n",
            "Epoch 299/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.8905 - accuracy: 0.6409 - val_loss: 0.9064 - val_accuracy: 0.6860\n",
            "Epoch 300/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8951 - accuracy: 0.6457 - val_loss: 0.9073 - val_accuracy: 0.6715\n",
            "Epoch 301/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.8929 - accuracy: 0.6463 - val_loss: 0.9062 - val_accuracy: 0.6667\n",
            "Epoch 302/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9174 - accuracy: 0.6415 - val_loss: 0.8978 - val_accuracy: 0.6812\n",
            "Epoch 303/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.9067 - accuracy: 0.6288 - val_loss: 0.9061 - val_accuracy: 0.6860\n",
            "Epoch 304/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9078 - accuracy: 0.6439 - val_loss: 0.9037 - val_accuracy: 0.6473\n",
            "Epoch 305/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.8916 - accuracy: 0.6481 - val_loss: 0.8961 - val_accuracy: 0.6329\n",
            "Epoch 306/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.8970 - accuracy: 0.6554 - val_loss: 0.8899 - val_accuracy: 0.6329\n",
            "Epoch 307/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.8990 - accuracy: 0.6481 - val_loss: 0.9106 - val_accuracy: 0.6522\n",
            "Epoch 308/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8923 - accuracy: 0.6378 - val_loss: 0.8911 - val_accuracy: 0.6522\n",
            "Epoch 309/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8879 - accuracy: 0.6469 - val_loss: 0.8980 - val_accuracy: 0.6618\n",
            "Epoch 310/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.8960 - accuracy: 0.6378 - val_loss: 0.9022 - val_accuracy: 0.6860\n",
            "Epoch 311/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8993 - accuracy: 0.6548 - val_loss: 0.9067 - val_accuracy: 0.6522\n",
            "Epoch 312/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8728 - accuracy: 0.6614 - val_loss: 0.8890 - val_accuracy: 0.6715\n",
            "Epoch 313/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.8998 - accuracy: 0.6451 - val_loss: 0.8976 - val_accuracy: 0.6618\n",
            "Epoch 314/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8811 - accuracy: 0.6566 - val_loss: 0.8862 - val_accuracy: 0.6860\n",
            "Epoch 315/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8996 - accuracy: 0.6378 - val_loss: 0.8885 - val_accuracy: 0.6763\n",
            "Epoch 316/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.8642 - accuracy: 0.6499 - val_loss: 0.8845 - val_accuracy: 0.7005\n",
            "Epoch 317/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.9004 - accuracy: 0.6294 - val_loss: 0.8981 - val_accuracy: 0.6908\n",
            "Epoch 318/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.8669 - accuracy: 0.6657 - val_loss: 0.8829 - val_accuracy: 0.6957\n",
            "Epoch 319/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8842 - accuracy: 0.6487 - val_loss: 0.8978 - val_accuracy: 0.6522\n",
            "Epoch 320/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8613 - accuracy: 0.6790 - val_loss: 0.8883 - val_accuracy: 0.6473\n",
            "Epoch 321/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.8672 - accuracy: 0.6638 - val_loss: 0.8889 - val_accuracy: 0.6957\n",
            "Epoch 322/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8633 - accuracy: 0.6584 - val_loss: 0.8880 - val_accuracy: 0.6812\n",
            "Epoch 323/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8847 - accuracy: 0.6596 - val_loss: 0.9118 - val_accuracy: 0.6667\n",
            "Epoch 324/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8584 - accuracy: 0.6765 - val_loss: 0.8856 - val_accuracy: 0.6618\n",
            "Epoch 325/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8516 - accuracy: 0.6675 - val_loss: 0.8825 - val_accuracy: 0.6570\n",
            "Epoch 326/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8567 - accuracy: 0.6741 - val_loss: 0.8783 - val_accuracy: 0.6812\n",
            "Epoch 327/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8694 - accuracy: 0.6457 - val_loss: 0.8957 - val_accuracy: 0.6618\n",
            "Epoch 328/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.8584 - accuracy: 0.6717 - val_loss: 0.8808 - val_accuracy: 0.6715\n",
            "Epoch 329/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8689 - accuracy: 0.6608 - val_loss: 0.8690 - val_accuracy: 0.6667\n",
            "Epoch 330/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.8572 - accuracy: 0.6626 - val_loss: 0.8970 - val_accuracy: 0.6715\n",
            "Epoch 331/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.8467 - accuracy: 0.6651 - val_loss: 0.8857 - val_accuracy: 0.6957\n",
            "Epoch 332/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8893 - accuracy: 0.6372 - val_loss: 0.8765 - val_accuracy: 0.7005\n",
            "Epoch 333/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8396 - accuracy: 0.6687 - val_loss: 0.8757 - val_accuracy: 0.6763\n",
            "Epoch 334/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.8532 - accuracy: 0.6548 - val_loss: 0.8772 - val_accuracy: 0.6908\n",
            "Epoch 335/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.8602 - accuracy: 0.6560 - val_loss: 0.8836 - val_accuracy: 0.6763\n",
            "Epoch 336/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8453 - accuracy: 0.6614 - val_loss: 0.8918 - val_accuracy: 0.6667\n",
            "Epoch 337/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8507 - accuracy: 0.6614 - val_loss: 0.8936 - val_accuracy: 0.6618\n",
            "Epoch 338/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.8263 - accuracy: 0.6705 - val_loss: 0.8687 - val_accuracy: 0.6522\n",
            "Epoch 339/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8488 - accuracy: 0.6705 - val_loss: 0.8936 - val_accuracy: 0.6812\n",
            "Epoch 340/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8310 - accuracy: 0.6620 - val_loss: 0.8865 - val_accuracy: 0.6667\n",
            "Epoch 341/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8373 - accuracy: 0.6699 - val_loss: 0.9004 - val_accuracy: 0.6667\n",
            "Epoch 342/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.8408 - accuracy: 0.6644 - val_loss: 0.8720 - val_accuracy: 0.7053\n",
            "Epoch 343/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.8459 - accuracy: 0.6572 - val_loss: 0.8744 - val_accuracy: 0.6957\n",
            "Epoch 344/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8331 - accuracy: 0.6669 - val_loss: 0.8676 - val_accuracy: 0.6377\n",
            "Epoch 345/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8541 - accuracy: 0.6663 - val_loss: 0.8579 - val_accuracy: 0.6957\n",
            "Epoch 346/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8453 - accuracy: 0.6735 - val_loss: 0.8656 - val_accuracy: 0.6957\n",
            "Epoch 347/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8552 - accuracy: 0.6669 - val_loss: 0.8687 - val_accuracy: 0.7005\n",
            "Epoch 348/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8438 - accuracy: 0.6790 - val_loss: 0.8704 - val_accuracy: 0.6522\n",
            "Epoch 349/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.8436 - accuracy: 0.6693 - val_loss: 0.8716 - val_accuracy: 0.6860\n",
            "Epoch 350/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.8550 - accuracy: 0.6620 - val_loss: 0.8552 - val_accuracy: 0.6957\n",
            "Epoch 351/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.8194 - accuracy: 0.6802 - val_loss: 0.8573 - val_accuracy: 0.6860\n",
            "Epoch 352/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8433 - accuracy: 0.6705 - val_loss: 0.8701 - val_accuracy: 0.6473\n",
            "Epoch 353/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.8214 - accuracy: 0.6868 - val_loss: 0.8702 - val_accuracy: 0.7005\n",
            "Epoch 354/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.8181 - accuracy: 0.6862 - val_loss: 0.8658 - val_accuracy: 0.6957\n",
            "Epoch 355/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8311 - accuracy: 0.6620 - val_loss: 0.8810 - val_accuracy: 0.6667\n",
            "Epoch 356/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.8140 - accuracy: 0.6826 - val_loss: 0.8661 - val_accuracy: 0.6618\n",
            "Epoch 357/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8547 - accuracy: 0.6439 - val_loss: 0.8705 - val_accuracy: 0.6957\n",
            "Epoch 358/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8204 - accuracy: 0.6814 - val_loss: 0.8557 - val_accuracy: 0.6812\n",
            "Epoch 359/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.8273 - accuracy: 0.6796 - val_loss: 0.8583 - val_accuracy: 0.6908\n",
            "Epoch 360/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7983 - accuracy: 0.6856 - val_loss: 0.8599 - val_accuracy: 0.6618\n",
            "Epoch 361/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.8017 - accuracy: 0.6838 - val_loss: 0.8551 - val_accuracy: 0.6812\n",
            "Epoch 362/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.8359 - accuracy: 0.6747 - val_loss: 0.8518 - val_accuracy: 0.7053\n",
            "Epoch 363/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8080 - accuracy: 0.6729 - val_loss: 0.8502 - val_accuracy: 0.6715\n",
            "Epoch 364/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8091 - accuracy: 0.6771 - val_loss: 0.8625 - val_accuracy: 0.6763\n",
            "Epoch 365/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.8210 - accuracy: 0.6651 - val_loss: 0.8508 - val_accuracy: 0.6522\n",
            "Epoch 366/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8074 - accuracy: 0.6868 - val_loss: 0.8470 - val_accuracy: 0.6812\n",
            "Epoch 367/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8114 - accuracy: 0.6771 - val_loss: 0.8487 - val_accuracy: 0.6860\n",
            "Epoch 368/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8170 - accuracy: 0.6699 - val_loss: 0.8387 - val_accuracy: 0.6908\n",
            "Epoch 369/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7945 - accuracy: 0.6771 - val_loss: 0.8402 - val_accuracy: 0.6957\n",
            "Epoch 370/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.8224 - accuracy: 0.6687 - val_loss: 0.8367 - val_accuracy: 0.6860\n",
            "Epoch 371/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8151 - accuracy: 0.6796 - val_loss: 0.8330 - val_accuracy: 0.7053\n",
            "Epoch 372/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.8148 - accuracy: 0.6735 - val_loss: 0.8302 - val_accuracy: 0.6908\n",
            "Epoch 373/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.8198 - accuracy: 0.6723 - val_loss: 0.8325 - val_accuracy: 0.6763\n",
            "Epoch 374/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.8055 - accuracy: 0.6790 - val_loss: 0.8376 - val_accuracy: 0.6860\n",
            "Epoch 375/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7919 - accuracy: 0.6929 - val_loss: 0.8317 - val_accuracy: 0.7005\n",
            "Epoch 376/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7970 - accuracy: 0.6856 - val_loss: 0.8246 - val_accuracy: 0.7101\n",
            "Epoch 377/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.8027 - accuracy: 0.6802 - val_loss: 0.8351 - val_accuracy: 0.6715\n",
            "Epoch 378/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8104 - accuracy: 0.6796 - val_loss: 0.8396 - val_accuracy: 0.6715\n",
            "Epoch 379/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.8067 - accuracy: 0.6771 - val_loss: 0.8373 - val_accuracy: 0.6908\n",
            "Epoch 380/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7777 - accuracy: 0.6935 - val_loss: 0.8341 - val_accuracy: 0.6908\n",
            "Epoch 381/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.8111 - accuracy: 0.6765 - val_loss: 0.8382 - val_accuracy: 0.6715\n",
            "Epoch 382/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7757 - accuracy: 0.6880 - val_loss: 0.8464 - val_accuracy: 0.6570\n",
            "Epoch 383/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7685 - accuracy: 0.6983 - val_loss: 0.8317 - val_accuracy: 0.6812\n",
            "Epoch 384/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7977 - accuracy: 0.6675 - val_loss: 0.8371 - val_accuracy: 0.6908\n",
            "Epoch 385/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7931 - accuracy: 0.6856 - val_loss: 0.8392 - val_accuracy: 0.7005\n",
            "Epoch 386/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7834 - accuracy: 0.7092 - val_loss: 0.8317 - val_accuracy: 0.7005\n",
            "Epoch 387/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7811 - accuracy: 0.6911 - val_loss: 0.8251 - val_accuracy: 0.6908\n",
            "Epoch 388/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7842 - accuracy: 0.6959 - val_loss: 0.8304 - val_accuracy: 0.7053\n",
            "Epoch 389/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8141 - accuracy: 0.6784 - val_loss: 0.8267 - val_accuracy: 0.7005\n",
            "Epoch 390/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7916 - accuracy: 0.6796 - val_loss: 0.8415 - val_accuracy: 0.6860\n",
            "Epoch 391/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7846 - accuracy: 0.6771 - val_loss: 0.8180 - val_accuracy: 0.7101\n",
            "Epoch 392/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7730 - accuracy: 0.6959 - val_loss: 0.8361 - val_accuracy: 0.6812\n",
            "Epoch 393/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7856 - accuracy: 0.6965 - val_loss: 0.8098 - val_accuracy: 0.7150\n",
            "Epoch 394/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7783 - accuracy: 0.6959 - val_loss: 0.8221 - val_accuracy: 0.7005\n",
            "Epoch 395/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 0.7601 - accuracy: 0.6923 - val_loss: 0.8245 - val_accuracy: 0.7101\n",
            "Epoch 396/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7867 - accuracy: 0.6904 - val_loss: 0.8208 - val_accuracy: 0.7053\n",
            "Epoch 397/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7597 - accuracy: 0.7189 - val_loss: 0.8288 - val_accuracy: 0.6860\n",
            "Epoch 398/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7578 - accuracy: 0.7037 - val_loss: 0.8459 - val_accuracy: 0.6812\n",
            "Epoch 399/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7524 - accuracy: 0.7001 - val_loss: 0.8258 - val_accuracy: 0.6763\n",
            "Epoch 400/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7718 - accuracy: 0.6898 - val_loss: 0.8124 - val_accuracy: 0.6812\n",
            "Epoch 401/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7532 - accuracy: 0.6995 - val_loss: 0.8018 - val_accuracy: 0.7198\n",
            "Epoch 402/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7391 - accuracy: 0.7080 - val_loss: 0.8080 - val_accuracy: 0.7101\n",
            "Epoch 403/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7696 - accuracy: 0.6923 - val_loss: 0.8170 - val_accuracy: 0.6957\n",
            "Epoch 404/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7733 - accuracy: 0.6832 - val_loss: 0.8086 - val_accuracy: 0.7101\n",
            "Epoch 405/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7652 - accuracy: 0.6947 - val_loss: 0.8216 - val_accuracy: 0.6812\n",
            "Epoch 406/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7821 - accuracy: 0.6874 - val_loss: 0.8346 - val_accuracy: 0.6957\n",
            "Epoch 407/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7812 - accuracy: 0.6826 - val_loss: 0.8229 - val_accuracy: 0.6763\n",
            "Epoch 408/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7461 - accuracy: 0.7044 - val_loss: 0.8233 - val_accuracy: 0.6812\n",
            "Epoch 409/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7548 - accuracy: 0.6850 - val_loss: 0.8135 - val_accuracy: 0.7005\n",
            "Epoch 410/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7723 - accuracy: 0.6983 - val_loss: 0.8047 - val_accuracy: 0.6957\n",
            "Epoch 411/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7741 - accuracy: 0.7019 - val_loss: 0.8172 - val_accuracy: 0.6763\n",
            "Epoch 412/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7551 - accuracy: 0.7025 - val_loss: 0.8090 - val_accuracy: 0.6908\n",
            "Epoch 413/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7699 - accuracy: 0.6886 - val_loss: 0.8056 - val_accuracy: 0.7246\n",
            "Epoch 414/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7391 - accuracy: 0.7062 - val_loss: 0.8086 - val_accuracy: 0.6812\n",
            "Epoch 415/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7588 - accuracy: 0.7019 - val_loss: 0.7968 - val_accuracy: 0.7101\n",
            "Epoch 416/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7433 - accuracy: 0.7128 - val_loss: 0.8139 - val_accuracy: 0.6860\n",
            "Epoch 417/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7551 - accuracy: 0.7013 - val_loss: 0.8045 - val_accuracy: 0.7198\n",
            "Epoch 418/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7534 - accuracy: 0.6953 - val_loss: 0.8175 - val_accuracy: 0.6715\n",
            "Epoch 419/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7332 - accuracy: 0.7207 - val_loss: 0.8095 - val_accuracy: 0.6908\n",
            "Epoch 420/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7397 - accuracy: 0.6971 - val_loss: 0.7950 - val_accuracy: 0.7101\n",
            "Epoch 421/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7512 - accuracy: 0.7007 - val_loss: 0.8000 - val_accuracy: 0.7101\n",
            "Epoch 422/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7250 - accuracy: 0.7110 - val_loss: 0.8040 - val_accuracy: 0.7101\n",
            "Epoch 423/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7439 - accuracy: 0.7146 - val_loss: 0.7844 - val_accuracy: 0.7101\n",
            "Epoch 424/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7627 - accuracy: 0.6959 - val_loss: 0.7993 - val_accuracy: 0.7101\n",
            "Epoch 425/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7308 - accuracy: 0.7213 - val_loss: 0.8001 - val_accuracy: 0.6860\n",
            "Epoch 426/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7539 - accuracy: 0.6983 - val_loss: 0.7969 - val_accuracy: 0.6908\n",
            "Epoch 427/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7430 - accuracy: 0.6995 - val_loss: 0.7934 - val_accuracy: 0.6908\n",
            "Epoch 428/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7256 - accuracy: 0.7201 - val_loss: 0.8010 - val_accuracy: 0.7053\n",
            "Epoch 429/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7476 - accuracy: 0.6953 - val_loss: 0.8006 - val_accuracy: 0.6860\n",
            "Epoch 430/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7254 - accuracy: 0.7031 - val_loss: 0.8113 - val_accuracy: 0.6812\n",
            "Epoch 431/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7290 - accuracy: 0.7158 - val_loss: 0.7864 - val_accuracy: 0.7343\n",
            "Epoch 432/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7371 - accuracy: 0.7128 - val_loss: 0.7952 - val_accuracy: 0.6957\n",
            "Epoch 433/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7331 - accuracy: 0.7037 - val_loss: 0.8034 - val_accuracy: 0.7053\n",
            "Epoch 434/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7165 - accuracy: 0.7189 - val_loss: 0.7926 - val_accuracy: 0.6908\n",
            "Epoch 435/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7256 - accuracy: 0.7316 - val_loss: 0.8039 - val_accuracy: 0.7005\n",
            "Epoch 436/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7268 - accuracy: 0.7104 - val_loss: 0.7841 - val_accuracy: 0.7101\n",
            "Epoch 437/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7157 - accuracy: 0.7170 - val_loss: 0.7892 - val_accuracy: 0.7150\n",
            "Epoch 438/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7090 - accuracy: 0.7110 - val_loss: 0.7824 - val_accuracy: 0.7246\n",
            "Epoch 439/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7298 - accuracy: 0.7122 - val_loss: 0.7766 - val_accuracy: 0.7295\n",
            "Epoch 440/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7342 - accuracy: 0.7104 - val_loss: 0.7814 - val_accuracy: 0.7246\n",
            "Epoch 441/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7067 - accuracy: 0.7255 - val_loss: 0.7777 - val_accuracy: 0.7150\n",
            "Epoch 442/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7105 - accuracy: 0.7140 - val_loss: 0.7865 - val_accuracy: 0.6908\n",
            "Epoch 443/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7063 - accuracy: 0.7207 - val_loss: 0.7716 - val_accuracy: 0.7246\n",
            "Epoch 444/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7385 - accuracy: 0.6995 - val_loss: 0.7883 - val_accuracy: 0.7150\n",
            "Epoch 445/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7103 - accuracy: 0.7225 - val_loss: 0.7752 - val_accuracy: 0.6957\n",
            "Epoch 446/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7223 - accuracy: 0.7037 - val_loss: 0.7908 - val_accuracy: 0.7101\n",
            "Epoch 447/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7252 - accuracy: 0.7231 - val_loss: 0.7880 - val_accuracy: 0.7150\n",
            "Epoch 448/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7148 - accuracy: 0.7273 - val_loss: 0.7957 - val_accuracy: 0.7005\n",
            "Epoch 449/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7141 - accuracy: 0.7352 - val_loss: 0.7901 - val_accuracy: 0.6860\n",
            "Epoch 450/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7108 - accuracy: 0.7189 - val_loss: 0.7901 - val_accuracy: 0.7198\n",
            "Epoch 451/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7133 - accuracy: 0.7273 - val_loss: 0.8054 - val_accuracy: 0.6812\n",
            "Epoch 452/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6988 - accuracy: 0.7213 - val_loss: 0.7924 - val_accuracy: 0.7198\n",
            "Epoch 453/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7241 - accuracy: 0.7050 - val_loss: 0.7796 - val_accuracy: 0.7101\n",
            "Epoch 454/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7109 - accuracy: 0.7164 - val_loss: 0.7877 - val_accuracy: 0.6957\n",
            "Epoch 455/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7140 - accuracy: 0.7074 - val_loss: 0.7919 - val_accuracy: 0.6957\n",
            "Epoch 456/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7091 - accuracy: 0.7249 - val_loss: 0.7786 - val_accuracy: 0.7295\n",
            "Epoch 457/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7045 - accuracy: 0.7291 - val_loss: 0.7958 - val_accuracy: 0.7005\n",
            "Epoch 458/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6914 - accuracy: 0.7334 - val_loss: 0.7911 - val_accuracy: 0.7053\n",
            "Epoch 459/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7078 - accuracy: 0.7285 - val_loss: 0.7686 - val_accuracy: 0.7246\n",
            "Epoch 460/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7162 - accuracy: 0.7128 - val_loss: 0.7859 - val_accuracy: 0.7198\n",
            "Epoch 461/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7196 - accuracy: 0.7177 - val_loss: 0.7666 - val_accuracy: 0.7101\n",
            "Epoch 462/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7158 - accuracy: 0.7092 - val_loss: 0.7622 - val_accuracy: 0.7440\n",
            "Epoch 463/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6921 - accuracy: 0.7388 - val_loss: 0.7839 - val_accuracy: 0.7005\n",
            "Epoch 464/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7138 - accuracy: 0.7110 - val_loss: 0.7727 - val_accuracy: 0.6860\n",
            "Epoch 465/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7169 - accuracy: 0.7080 - val_loss: 0.7713 - val_accuracy: 0.7391\n",
            "Epoch 466/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6993 - accuracy: 0.7164 - val_loss: 0.7781 - val_accuracy: 0.7198\n",
            "Epoch 467/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6884 - accuracy: 0.7249 - val_loss: 0.7920 - val_accuracy: 0.6812\n",
            "Epoch 468/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6829 - accuracy: 0.7285 - val_loss: 0.7982 - val_accuracy: 0.6570\n",
            "Epoch 469/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6926 - accuracy: 0.7255 - val_loss: 0.7576 - val_accuracy: 0.7295\n",
            "Epoch 470/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6920 - accuracy: 0.7328 - val_loss: 0.7691 - val_accuracy: 0.7005\n",
            "Epoch 471/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7073 - accuracy: 0.7201 - val_loss: 0.7674 - val_accuracy: 0.7053\n",
            "Epoch 472/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6897 - accuracy: 0.7291 - val_loss: 0.7728 - val_accuracy: 0.6957\n",
            "Epoch 473/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6918 - accuracy: 0.7328 - val_loss: 0.7647 - val_accuracy: 0.7053\n",
            "Epoch 474/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6821 - accuracy: 0.7219 - val_loss: 0.7813 - val_accuracy: 0.6957\n",
            "Epoch 475/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6873 - accuracy: 0.7382 - val_loss: 0.7719 - val_accuracy: 0.6667\n",
            "Epoch 476/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6982 - accuracy: 0.7219 - val_loss: 0.7823 - val_accuracy: 0.6908\n",
            "Epoch 477/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6924 - accuracy: 0.7243 - val_loss: 0.7753 - val_accuracy: 0.6908\n",
            "Epoch 478/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7259 - accuracy: 0.7116 - val_loss: 0.7778 - val_accuracy: 0.7053\n",
            "Epoch 479/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6939 - accuracy: 0.7231 - val_loss: 0.7832 - val_accuracy: 0.7150\n",
            "Epoch 480/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6943 - accuracy: 0.7334 - val_loss: 0.7938 - val_accuracy: 0.7053\n",
            "Epoch 481/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6762 - accuracy: 0.7291 - val_loss: 0.7938 - val_accuracy: 0.7150\n",
            "Epoch 482/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6560 - accuracy: 0.7382 - val_loss: 0.7747 - val_accuracy: 0.7101\n",
            "Epoch 483/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6752 - accuracy: 0.7304 - val_loss: 0.7739 - val_accuracy: 0.6957\n",
            "Epoch 484/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6908 - accuracy: 0.7116 - val_loss: 0.7671 - val_accuracy: 0.7295\n",
            "Epoch 485/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6645 - accuracy: 0.7316 - val_loss: 0.7757 - val_accuracy: 0.7053\n",
            "Epoch 486/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6939 - accuracy: 0.7322 - val_loss: 0.7734 - val_accuracy: 0.7150\n",
            "Epoch 487/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6669 - accuracy: 0.7358 - val_loss: 0.7834 - val_accuracy: 0.7150\n",
            "Epoch 488/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6865 - accuracy: 0.7249 - val_loss: 0.7888 - val_accuracy: 0.7198\n",
            "Epoch 489/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6681 - accuracy: 0.7340 - val_loss: 0.7709 - val_accuracy: 0.7053\n",
            "Epoch 490/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6889 - accuracy: 0.7273 - val_loss: 0.7772 - val_accuracy: 0.6957\n",
            "Epoch 491/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6617 - accuracy: 0.7352 - val_loss: 0.7659 - val_accuracy: 0.7101\n",
            "Epoch 492/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6742 - accuracy: 0.7406 - val_loss: 0.7616 - val_accuracy: 0.7150\n",
            "Epoch 493/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6869 - accuracy: 0.7201 - val_loss: 0.7779 - val_accuracy: 0.6957\n",
            "Epoch 494/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6663 - accuracy: 0.7358 - val_loss: 0.7528 - val_accuracy: 0.7343\n",
            "Epoch 495/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6911 - accuracy: 0.7231 - val_loss: 0.7661 - val_accuracy: 0.7150\n",
            "Epoch 496/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6677 - accuracy: 0.7418 - val_loss: 0.7549 - val_accuracy: 0.7343\n",
            "Epoch 497/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6744 - accuracy: 0.7322 - val_loss: 0.7474 - val_accuracy: 0.7246\n",
            "Epoch 498/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6982 - accuracy: 0.7225 - val_loss: 0.7589 - val_accuracy: 0.7246\n",
            "Epoch 499/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6639 - accuracy: 0.7394 - val_loss: 0.7837 - val_accuracy: 0.6957\n",
            "Epoch 500/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6689 - accuracy: 0.7279 - val_loss: 0.7707 - val_accuracy: 0.6957\n",
            "Epoch 501/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6652 - accuracy: 0.7352 - val_loss: 0.7759 - val_accuracy: 0.7198\n",
            "Epoch 502/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6502 - accuracy: 0.7388 - val_loss: 0.7830 - val_accuracy: 0.7053\n",
            "Epoch 503/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6677 - accuracy: 0.7370 - val_loss: 0.7615 - val_accuracy: 0.6812\n",
            "Epoch 504/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6791 - accuracy: 0.7285 - val_loss: 0.7648 - val_accuracy: 0.7053\n",
            "Epoch 505/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6607 - accuracy: 0.7370 - val_loss: 0.7564 - val_accuracy: 0.7295\n",
            "Epoch 506/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6678 - accuracy: 0.7412 - val_loss: 0.7844 - val_accuracy: 0.7246\n",
            "Epoch 507/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6517 - accuracy: 0.7455 - val_loss: 0.7546 - val_accuracy: 0.7005\n",
            "Epoch 508/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6679 - accuracy: 0.7352 - val_loss: 0.7731 - val_accuracy: 0.7101\n",
            "Epoch 509/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6573 - accuracy: 0.7382 - val_loss: 0.7482 - val_accuracy: 0.7246\n",
            "Epoch 510/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6545 - accuracy: 0.7424 - val_loss: 0.7715 - val_accuracy: 0.7101\n",
            "Epoch 511/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6662 - accuracy: 0.7388 - val_loss: 0.7645 - val_accuracy: 0.7101\n",
            "Epoch 512/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6227 - accuracy: 0.7503 - val_loss: 0.7575 - val_accuracy: 0.7246\n",
            "Epoch 513/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6386 - accuracy: 0.7545 - val_loss: 0.7503 - val_accuracy: 0.7246\n",
            "Epoch 514/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6649 - accuracy: 0.7485 - val_loss: 0.7417 - val_accuracy: 0.7295\n",
            "Epoch 515/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6755 - accuracy: 0.7430 - val_loss: 0.7556 - val_accuracy: 0.7391\n",
            "Epoch 516/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6375 - accuracy: 0.7503 - val_loss: 0.7559 - val_accuracy: 0.7101\n",
            "Epoch 517/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6403 - accuracy: 0.7443 - val_loss: 0.7456 - val_accuracy: 0.7150\n",
            "Epoch 518/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6515 - accuracy: 0.7443 - val_loss: 0.7493 - val_accuracy: 0.7343\n",
            "Epoch 519/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6572 - accuracy: 0.7376 - val_loss: 0.7491 - val_accuracy: 0.7246\n",
            "Epoch 520/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6532 - accuracy: 0.7563 - val_loss: 0.7499 - val_accuracy: 0.7295\n",
            "Epoch 521/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6436 - accuracy: 0.7430 - val_loss: 0.7571 - val_accuracy: 0.7246\n",
            "Epoch 522/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6572 - accuracy: 0.7394 - val_loss: 0.7541 - val_accuracy: 0.7005\n",
            "Epoch 523/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6483 - accuracy: 0.7382 - val_loss: 0.7714 - val_accuracy: 0.6908\n",
            "Epoch 524/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6511 - accuracy: 0.7497 - val_loss: 0.7617 - val_accuracy: 0.7005\n",
            "Epoch 525/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6245 - accuracy: 0.7539 - val_loss: 0.7493 - val_accuracy: 0.7246\n",
            "Epoch 526/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6573 - accuracy: 0.7297 - val_loss: 0.7670 - val_accuracy: 0.7053\n",
            "Epoch 527/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6397 - accuracy: 0.7503 - val_loss: 0.7489 - val_accuracy: 0.7295\n",
            "Epoch 528/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6562 - accuracy: 0.7364 - val_loss: 0.7490 - val_accuracy: 0.7150\n",
            "Epoch 529/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6374 - accuracy: 0.7479 - val_loss: 0.7461 - val_accuracy: 0.7246\n",
            "Epoch 530/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6444 - accuracy: 0.7346 - val_loss: 0.7451 - val_accuracy: 0.7198\n",
            "Epoch 531/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6453 - accuracy: 0.7485 - val_loss: 0.7477 - val_accuracy: 0.7198\n",
            "Epoch 532/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6588 - accuracy: 0.7394 - val_loss: 0.7646 - val_accuracy: 0.7101\n",
            "Epoch 533/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6177 - accuracy: 0.7624 - val_loss: 0.7544 - val_accuracy: 0.7053\n",
            "Epoch 534/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6366 - accuracy: 0.7491 - val_loss: 0.7551 - val_accuracy: 0.7150\n",
            "Epoch 535/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6479 - accuracy: 0.7388 - val_loss: 0.7509 - val_accuracy: 0.7005\n",
            "Epoch 536/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6297 - accuracy: 0.7497 - val_loss: 0.7672 - val_accuracy: 0.7150\n",
            "Epoch 537/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6318 - accuracy: 0.7503 - val_loss: 0.7514 - val_accuracy: 0.7343\n",
            "Epoch 538/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6314 - accuracy: 0.7570 - val_loss: 0.7412 - val_accuracy: 0.7343\n",
            "Epoch 539/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6383 - accuracy: 0.7358 - val_loss: 0.7495 - val_accuracy: 0.7198\n",
            "Epoch 540/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6435 - accuracy: 0.7412 - val_loss: 0.7488 - val_accuracy: 0.7343\n",
            "Epoch 541/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6362 - accuracy: 0.7557 - val_loss: 0.7540 - val_accuracy: 0.7198\n",
            "Epoch 542/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6366 - accuracy: 0.7521 - val_loss: 0.7700 - val_accuracy: 0.7005\n",
            "Epoch 543/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6400 - accuracy: 0.7467 - val_loss: 0.7485 - val_accuracy: 0.7391\n",
            "Epoch 544/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6401 - accuracy: 0.7437 - val_loss: 0.7686 - val_accuracy: 0.7101\n",
            "Epoch 545/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6158 - accuracy: 0.7594 - val_loss: 0.7438 - val_accuracy: 0.7295\n",
            "Epoch 546/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6426 - accuracy: 0.7437 - val_loss: 0.7452 - val_accuracy: 0.7391\n",
            "Epoch 547/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6270 - accuracy: 0.7539 - val_loss: 0.7371 - val_accuracy: 0.7198\n",
            "Epoch 548/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6121 - accuracy: 0.7666 - val_loss: 0.7375 - val_accuracy: 0.7246\n",
            "Epoch 549/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6180 - accuracy: 0.7515 - val_loss: 0.7448 - val_accuracy: 0.7440\n",
            "Epoch 550/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6083 - accuracy: 0.7624 - val_loss: 0.7439 - val_accuracy: 0.7246\n",
            "Epoch 551/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6201 - accuracy: 0.7642 - val_loss: 0.7430 - val_accuracy: 0.7295\n",
            "Epoch 552/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6148 - accuracy: 0.7509 - val_loss: 0.7526 - val_accuracy: 0.7295\n",
            "Epoch 553/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6207 - accuracy: 0.7497 - val_loss: 0.7298 - val_accuracy: 0.7391\n",
            "Epoch 554/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6191 - accuracy: 0.7503 - val_loss: 0.7255 - val_accuracy: 0.7343\n",
            "Epoch 555/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6247 - accuracy: 0.7563 - val_loss: 0.7303 - val_accuracy: 0.7246\n",
            "Epoch 556/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6295 - accuracy: 0.7521 - val_loss: 0.7228 - val_accuracy: 0.7343\n",
            "Epoch 557/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6246 - accuracy: 0.7660 - val_loss: 0.7332 - val_accuracy: 0.7391\n",
            "Epoch 558/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6162 - accuracy: 0.7576 - val_loss: 0.7419 - val_accuracy: 0.7343\n",
            "Epoch 559/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6019 - accuracy: 0.7600 - val_loss: 0.7262 - val_accuracy: 0.7198\n",
            "Epoch 560/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6073 - accuracy: 0.7672 - val_loss: 0.7391 - val_accuracy: 0.7391\n",
            "Epoch 561/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6136 - accuracy: 0.7509 - val_loss: 0.7460 - val_accuracy: 0.7053\n",
            "Epoch 562/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6148 - accuracy: 0.7721 - val_loss: 0.7401 - val_accuracy: 0.7198\n",
            "Epoch 563/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6311 - accuracy: 0.7551 - val_loss: 0.7437 - val_accuracy: 0.7295\n",
            "Epoch 564/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6114 - accuracy: 0.7557 - val_loss: 0.7448 - val_accuracy: 0.7198\n",
            "Epoch 565/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6296 - accuracy: 0.7570 - val_loss: 0.7373 - val_accuracy: 0.7295\n",
            "Epoch 566/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6108 - accuracy: 0.7672 - val_loss: 0.7292 - val_accuracy: 0.7391\n",
            "Epoch 567/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5983 - accuracy: 0.7642 - val_loss: 0.7373 - val_accuracy: 0.7246\n",
            "Epoch 568/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6256 - accuracy: 0.7576 - val_loss: 0.7375 - val_accuracy: 0.7488\n",
            "Epoch 569/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5857 - accuracy: 0.7763 - val_loss: 0.7551 - val_accuracy: 0.7053\n",
            "Epoch 570/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5926 - accuracy: 0.7642 - val_loss: 0.7351 - val_accuracy: 0.7053\n",
            "Epoch 571/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5868 - accuracy: 0.7830 - val_loss: 0.7386 - val_accuracy: 0.7198\n",
            "Epoch 572/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5934 - accuracy: 0.7805 - val_loss: 0.7333 - val_accuracy: 0.7198\n",
            "Epoch 573/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5794 - accuracy: 0.7666 - val_loss: 0.7306 - val_accuracy: 0.7198\n",
            "Epoch 574/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5997 - accuracy: 0.7678 - val_loss: 0.7413 - val_accuracy: 0.7101\n",
            "Epoch 575/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6166 - accuracy: 0.7594 - val_loss: 0.7417 - val_accuracy: 0.7295\n",
            "Epoch 576/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5821 - accuracy: 0.7836 - val_loss: 0.7401 - val_accuracy: 0.7295\n",
            "Epoch 577/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5921 - accuracy: 0.7660 - val_loss: 0.7567 - val_accuracy: 0.7053\n",
            "Epoch 578/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5760 - accuracy: 0.7787 - val_loss: 0.7439 - val_accuracy: 0.7005\n",
            "Epoch 579/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6242 - accuracy: 0.7563 - val_loss: 0.7418 - val_accuracy: 0.6908\n",
            "Epoch 580/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5931 - accuracy: 0.7781 - val_loss: 0.7293 - val_accuracy: 0.7343\n",
            "Epoch 581/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5728 - accuracy: 0.7757 - val_loss: 0.7504 - val_accuracy: 0.6860\n",
            "Epoch 582/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6022 - accuracy: 0.7612 - val_loss: 0.7285 - val_accuracy: 0.7005\n",
            "Epoch 583/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6047 - accuracy: 0.7521 - val_loss: 0.7396 - val_accuracy: 0.7246\n",
            "Epoch 584/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5806 - accuracy: 0.7715 - val_loss: 0.7303 - val_accuracy: 0.7198\n",
            "Epoch 585/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6032 - accuracy: 0.7630 - val_loss: 0.7322 - val_accuracy: 0.7246\n",
            "Epoch 586/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5774 - accuracy: 0.7727 - val_loss: 0.7331 - val_accuracy: 0.7198\n",
            "Epoch 587/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5856 - accuracy: 0.7624 - val_loss: 0.7248 - val_accuracy: 0.7198\n",
            "Epoch 588/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5902 - accuracy: 0.7823 - val_loss: 0.7263 - val_accuracy: 0.7246\n",
            "Epoch 589/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5890 - accuracy: 0.7715 - val_loss: 0.7268 - val_accuracy: 0.7391\n",
            "Epoch 590/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5987 - accuracy: 0.7672 - val_loss: 0.7373 - val_accuracy: 0.7295\n",
            "Epoch 591/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5831 - accuracy: 0.7799 - val_loss: 0.7334 - val_accuracy: 0.7246\n",
            "Epoch 592/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5776 - accuracy: 0.7848 - val_loss: 0.7186 - val_accuracy: 0.7198\n",
            "Epoch 593/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5840 - accuracy: 0.7642 - val_loss: 0.7143 - val_accuracy: 0.7391\n",
            "Epoch 594/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6146 - accuracy: 0.7527 - val_loss: 0.7386 - val_accuracy: 0.7246\n",
            "Epoch 595/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5825 - accuracy: 0.7624 - val_loss: 0.7368 - val_accuracy: 0.7391\n",
            "Epoch 596/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5966 - accuracy: 0.7672 - val_loss: 0.7286 - val_accuracy: 0.7295\n",
            "Epoch 597/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5731 - accuracy: 0.7727 - val_loss: 0.7272 - val_accuracy: 0.7440\n",
            "Epoch 598/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5646 - accuracy: 0.7751 - val_loss: 0.7251 - val_accuracy: 0.7198\n",
            "Epoch 599/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5810 - accuracy: 0.7793 - val_loss: 0.7307 - val_accuracy: 0.7150\n",
            "Epoch 600/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6039 - accuracy: 0.7648 - val_loss: 0.7513 - val_accuracy: 0.7150\n",
            "Epoch 601/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5852 - accuracy: 0.7739 - val_loss: 0.7422 - val_accuracy: 0.7198\n",
            "Epoch 602/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5844 - accuracy: 0.7636 - val_loss: 0.7149 - val_accuracy: 0.7295\n",
            "Epoch 603/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5853 - accuracy: 0.7745 - val_loss: 0.7090 - val_accuracy: 0.7343\n",
            "Epoch 604/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6027 - accuracy: 0.7781 - val_loss: 0.7318 - val_accuracy: 0.7101\n",
            "Epoch 605/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5749 - accuracy: 0.7787 - val_loss: 0.7239 - val_accuracy: 0.7391\n",
            "Epoch 606/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5873 - accuracy: 0.7884 - val_loss: 0.7308 - val_accuracy: 0.7391\n",
            "Epoch 607/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5484 - accuracy: 0.7902 - val_loss: 0.7338 - val_accuracy: 0.7005\n",
            "Epoch 608/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5808 - accuracy: 0.7745 - val_loss: 0.7090 - val_accuracy: 0.7391\n",
            "Epoch 609/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5679 - accuracy: 0.7848 - val_loss: 0.7051 - val_accuracy: 0.7343\n",
            "Epoch 610/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5715 - accuracy: 0.7799 - val_loss: 0.7096 - val_accuracy: 0.7343\n",
            "Epoch 611/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5884 - accuracy: 0.7666 - val_loss: 0.7294 - val_accuracy: 0.6908\n",
            "Epoch 612/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5745 - accuracy: 0.7733 - val_loss: 0.7056 - val_accuracy: 0.7440\n",
            "Epoch 613/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5537 - accuracy: 0.7830 - val_loss: 0.7049 - val_accuracy: 0.7343\n",
            "Epoch 614/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5751 - accuracy: 0.7817 - val_loss: 0.7143 - val_accuracy: 0.7198\n",
            "Epoch 615/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5624 - accuracy: 0.7793 - val_loss: 0.7218 - val_accuracy: 0.7005\n",
            "Epoch 616/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5892 - accuracy: 0.7624 - val_loss: 0.7333 - val_accuracy: 0.7198\n",
            "Epoch 617/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5779 - accuracy: 0.7703 - val_loss: 0.7233 - val_accuracy: 0.7440\n",
            "Epoch 618/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5604 - accuracy: 0.7751 - val_loss: 0.7289 - val_accuracy: 0.7198\n",
            "Epoch 619/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5892 - accuracy: 0.7648 - val_loss: 0.7213 - val_accuracy: 0.7295\n",
            "Epoch 620/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5545 - accuracy: 0.7848 - val_loss: 0.7275 - val_accuracy: 0.7198\n",
            "Epoch 621/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5611 - accuracy: 0.7721 - val_loss: 0.7215 - val_accuracy: 0.7391\n",
            "Epoch 622/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5550 - accuracy: 0.7848 - val_loss: 0.7074 - val_accuracy: 0.7585\n",
            "Epoch 623/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5625 - accuracy: 0.7793 - val_loss: 0.7098 - val_accuracy: 0.7343\n",
            "Epoch 624/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5534 - accuracy: 0.7860 - val_loss: 0.7285 - val_accuracy: 0.7295\n",
            "Epoch 625/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5584 - accuracy: 0.7854 - val_loss: 0.7092 - val_accuracy: 0.7198\n",
            "Epoch 626/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5650 - accuracy: 0.7836 - val_loss: 0.7069 - val_accuracy: 0.7391\n",
            "Epoch 627/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5744 - accuracy: 0.7823 - val_loss: 0.7111 - val_accuracy: 0.7440\n",
            "Epoch 628/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5735 - accuracy: 0.7763 - val_loss: 0.7043 - val_accuracy: 0.7488\n",
            "Epoch 629/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5652 - accuracy: 0.7769 - val_loss: 0.7307 - val_accuracy: 0.7198\n",
            "Epoch 630/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5580 - accuracy: 0.7914 - val_loss: 0.7289 - val_accuracy: 0.7101\n",
            "Epoch 631/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5587 - accuracy: 0.7751 - val_loss: 0.7116 - val_accuracy: 0.7295\n",
            "Epoch 632/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5584 - accuracy: 0.7848 - val_loss: 0.7101 - val_accuracy: 0.7246\n",
            "Epoch 633/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5434 - accuracy: 0.7902 - val_loss: 0.7069 - val_accuracy: 0.7053\n",
            "Epoch 634/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5657 - accuracy: 0.7902 - val_loss: 0.7194 - val_accuracy: 0.7198\n",
            "Epoch 635/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5348 - accuracy: 0.7944 - val_loss: 0.7198 - val_accuracy: 0.7343\n",
            "Epoch 636/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5592 - accuracy: 0.7860 - val_loss: 0.6907 - val_accuracy: 0.7681\n",
            "Epoch 637/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5413 - accuracy: 0.8029 - val_loss: 0.7095 - val_accuracy: 0.7536\n",
            "Epoch 638/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5513 - accuracy: 0.7902 - val_loss: 0.7149 - val_accuracy: 0.7246\n",
            "Epoch 639/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5428 - accuracy: 0.7969 - val_loss: 0.7306 - val_accuracy: 0.7198\n",
            "Epoch 640/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5671 - accuracy: 0.7799 - val_loss: 0.7073 - val_accuracy: 0.7440\n",
            "Epoch 641/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5417 - accuracy: 0.7975 - val_loss: 0.7040 - val_accuracy: 0.7246\n",
            "Epoch 642/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5680 - accuracy: 0.7775 - val_loss: 0.7104 - val_accuracy: 0.7488\n",
            "Epoch 643/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5586 - accuracy: 0.7799 - val_loss: 0.7191 - val_accuracy: 0.7440\n",
            "Epoch 644/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5385 - accuracy: 0.7902 - val_loss: 0.7204 - val_accuracy: 0.7343\n",
            "Epoch 645/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5489 - accuracy: 0.7842 - val_loss: 0.6937 - val_accuracy: 0.7440\n",
            "Epoch 646/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5347 - accuracy: 0.7908 - val_loss: 0.6999 - val_accuracy: 0.7440\n",
            "Epoch 647/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5397 - accuracy: 0.7944 - val_loss: 0.7102 - val_accuracy: 0.7198\n",
            "Epoch 648/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5479 - accuracy: 0.7823 - val_loss: 0.7146 - val_accuracy: 0.7343\n",
            "Epoch 649/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5704 - accuracy: 0.7715 - val_loss: 0.7187 - val_accuracy: 0.7391\n",
            "Epoch 650/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5373 - accuracy: 0.7902 - val_loss: 0.7152 - val_accuracy: 0.7343\n",
            "Epoch 651/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5457 - accuracy: 0.7817 - val_loss: 0.7174 - val_accuracy: 0.7391\n",
            "Epoch 652/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5492 - accuracy: 0.7932 - val_loss: 0.6993 - val_accuracy: 0.7488\n",
            "Epoch 653/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5445 - accuracy: 0.7944 - val_loss: 0.6806 - val_accuracy: 0.7536\n",
            "Epoch 654/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5430 - accuracy: 0.7884 - val_loss: 0.7105 - val_accuracy: 0.7101\n",
            "Epoch 655/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5383 - accuracy: 0.7823 - val_loss: 0.7038 - val_accuracy: 0.7198\n",
            "Epoch 656/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5163 - accuracy: 0.7932 - val_loss: 0.7064 - val_accuracy: 0.7391\n",
            "Epoch 657/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5464 - accuracy: 0.7969 - val_loss: 0.6984 - val_accuracy: 0.7536\n",
            "Epoch 658/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5299 - accuracy: 0.7914 - val_loss: 0.7083 - val_accuracy: 0.7053\n",
            "Epoch 659/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5368 - accuracy: 0.7836 - val_loss: 0.7026 - val_accuracy: 0.7391\n",
            "Epoch 660/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5333 - accuracy: 0.7999 - val_loss: 0.6981 - val_accuracy: 0.7488\n",
            "Epoch 661/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5283 - accuracy: 0.7956 - val_loss: 0.6894 - val_accuracy: 0.7536\n",
            "Epoch 662/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5418 - accuracy: 0.7950 - val_loss: 0.6946 - val_accuracy: 0.7295\n",
            "Epoch 663/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5073 - accuracy: 0.7950 - val_loss: 0.6903 - val_accuracy: 0.7536\n",
            "Epoch 664/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5232 - accuracy: 0.7926 - val_loss: 0.6686 - val_accuracy: 0.7778\n",
            "Epoch 665/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5262 - accuracy: 0.7999 - val_loss: 0.6847 - val_accuracy: 0.7681\n",
            "Epoch 666/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5196 - accuracy: 0.8005 - val_loss: 0.7318 - val_accuracy: 0.7198\n",
            "Epoch 667/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5344 - accuracy: 0.7908 - val_loss: 0.7038 - val_accuracy: 0.7391\n",
            "Epoch 668/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5311 - accuracy: 0.7969 - val_loss: 0.6946 - val_accuracy: 0.7391\n",
            "Epoch 669/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5332 - accuracy: 0.7836 - val_loss: 0.6795 - val_accuracy: 0.7633\n",
            "Epoch 670/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5428 - accuracy: 0.7872 - val_loss: 0.6920 - val_accuracy: 0.7488\n",
            "Epoch 671/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5254 - accuracy: 0.8077 - val_loss: 0.7197 - val_accuracy: 0.7391\n",
            "Epoch 672/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5409 - accuracy: 0.7896 - val_loss: 0.7005 - val_accuracy: 0.7391\n",
            "Epoch 673/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 0.4992 - accuracy: 0.8132 - val_loss: 0.6940 - val_accuracy: 0.7585\n",
            "Epoch 674/1000\n",
            "104/104 [==============================] - 1s 8ms/step - loss: 0.5379 - accuracy: 0.7896 - val_loss: 0.6883 - val_accuracy: 0.7536\n",
            "Epoch 675/1000\n",
            "104/104 [==============================] - 1s 8ms/step - loss: 0.5134 - accuracy: 0.8041 - val_loss: 0.7062 - val_accuracy: 0.7295\n",
            "Epoch 676/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5069 - accuracy: 0.8071 - val_loss: 0.6861 - val_accuracy: 0.7633\n",
            "Epoch 677/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 0.5055 - accuracy: 0.7999 - val_loss: 0.6941 - val_accuracy: 0.7295\n",
            "Epoch 678/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 0.5220 - accuracy: 0.7999 - val_loss: 0.6797 - val_accuracy: 0.7488\n",
            "Epoch 679/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5382 - accuracy: 0.7902 - val_loss: 0.6938 - val_accuracy: 0.7440\n",
            "Epoch 680/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5206 - accuracy: 0.8005 - val_loss: 0.6867 - val_accuracy: 0.7778\n",
            "Epoch 681/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4923 - accuracy: 0.8077 - val_loss: 0.6792 - val_accuracy: 0.7585\n",
            "Epoch 682/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5275 - accuracy: 0.7848 - val_loss: 0.6871 - val_accuracy: 0.7391\n",
            "Epoch 683/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5178 - accuracy: 0.8011 - val_loss: 0.6889 - val_accuracy: 0.7343\n",
            "Epoch 684/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5262 - accuracy: 0.7975 - val_loss: 0.6865 - val_accuracy: 0.7391\n",
            "Epoch 685/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5011 - accuracy: 0.8096 - val_loss: 0.6923 - val_accuracy: 0.7343\n",
            "Epoch 686/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5291 - accuracy: 0.7932 - val_loss: 0.6750 - val_accuracy: 0.7826\n",
            "Epoch 687/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5170 - accuracy: 0.7969 - val_loss: 0.6861 - val_accuracy: 0.7585\n",
            "Epoch 688/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4986 - accuracy: 0.8029 - val_loss: 0.6993 - val_accuracy: 0.7585\n",
            "Epoch 689/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5146 - accuracy: 0.8029 - val_loss: 0.6857 - val_accuracy: 0.7440\n",
            "Epoch 690/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5398 - accuracy: 0.7878 - val_loss: 0.6870 - val_accuracy: 0.7536\n",
            "Epoch 691/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5060 - accuracy: 0.7981 - val_loss: 0.6978 - val_accuracy: 0.7536\n",
            "Epoch 692/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5432 - accuracy: 0.7854 - val_loss: 0.6857 - val_accuracy: 0.7536\n",
            "Epoch 693/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5038 - accuracy: 0.8047 - val_loss: 0.6783 - val_accuracy: 0.7440\n",
            "Epoch 694/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5049 - accuracy: 0.8077 - val_loss: 0.6903 - val_accuracy: 0.7391\n",
            "Epoch 695/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5011 - accuracy: 0.8053 - val_loss: 0.6849 - val_accuracy: 0.7295\n",
            "Epoch 696/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4934 - accuracy: 0.8017 - val_loss: 0.6751 - val_accuracy: 0.7585\n",
            "Epoch 697/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5084 - accuracy: 0.7956 - val_loss: 0.6816 - val_accuracy: 0.7488\n",
            "Epoch 698/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5066 - accuracy: 0.8053 - val_loss: 0.6904 - val_accuracy: 0.7391\n",
            "Epoch 699/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5041 - accuracy: 0.8011 - val_loss: 0.6873 - val_accuracy: 0.7391\n",
            "Epoch 700/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4856 - accuracy: 0.8096 - val_loss: 0.6895 - val_accuracy: 0.7440\n",
            "Epoch 701/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4911 - accuracy: 0.8029 - val_loss: 0.6933 - val_accuracy: 0.7246\n",
            "Epoch 702/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5026 - accuracy: 0.8041 - val_loss: 0.6917 - val_accuracy: 0.7391\n",
            "Epoch 703/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5114 - accuracy: 0.7956 - val_loss: 0.6802 - val_accuracy: 0.7536\n",
            "Epoch 704/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4964 - accuracy: 0.8047 - val_loss: 0.6852 - val_accuracy: 0.7488\n",
            "Epoch 705/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4956 - accuracy: 0.8174 - val_loss: 0.7031 - val_accuracy: 0.7295\n",
            "Epoch 706/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5129 - accuracy: 0.8005 - val_loss: 0.6910 - val_accuracy: 0.7391\n",
            "Epoch 707/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4964 - accuracy: 0.8083 - val_loss: 0.6867 - val_accuracy: 0.7391\n",
            "Epoch 708/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5122 - accuracy: 0.8114 - val_loss: 0.6802 - val_accuracy: 0.7488\n",
            "Epoch 709/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4929 - accuracy: 0.8126 - val_loss: 0.6935 - val_accuracy: 0.7585\n",
            "Epoch 710/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5140 - accuracy: 0.7932 - val_loss: 0.7259 - val_accuracy: 0.7150\n",
            "Epoch 711/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5043 - accuracy: 0.8120 - val_loss: 0.6938 - val_accuracy: 0.7391\n",
            "Epoch 712/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5095 - accuracy: 0.8023 - val_loss: 0.6764 - val_accuracy: 0.7440\n",
            "Epoch 713/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4992 - accuracy: 0.8077 - val_loss: 0.6917 - val_accuracy: 0.7246\n",
            "Epoch 714/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4646 - accuracy: 0.8180 - val_loss: 0.6678 - val_accuracy: 0.7633\n",
            "Epoch 715/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4972 - accuracy: 0.8077 - val_loss: 0.6939 - val_accuracy: 0.7391\n",
            "Epoch 716/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4958 - accuracy: 0.8162 - val_loss: 0.7170 - val_accuracy: 0.7295\n",
            "Epoch 717/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4877 - accuracy: 0.8102 - val_loss: 0.7025 - val_accuracy: 0.7391\n",
            "Epoch 718/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4800 - accuracy: 0.8156 - val_loss: 0.6856 - val_accuracy: 0.7488\n",
            "Epoch 719/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4955 - accuracy: 0.8071 - val_loss: 0.6839 - val_accuracy: 0.7391\n",
            "Epoch 720/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4860 - accuracy: 0.8168 - val_loss: 0.6782 - val_accuracy: 0.7343\n",
            "Epoch 721/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5065 - accuracy: 0.8077 - val_loss: 0.6874 - val_accuracy: 0.7343\n",
            "Epoch 722/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4797 - accuracy: 0.8168 - val_loss: 0.6748 - val_accuracy: 0.7295\n",
            "Epoch 723/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4878 - accuracy: 0.8041 - val_loss: 0.6806 - val_accuracy: 0.7681\n",
            "Epoch 724/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5112 - accuracy: 0.7999 - val_loss: 0.6737 - val_accuracy: 0.7440\n",
            "Epoch 725/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4937 - accuracy: 0.7975 - val_loss: 0.7080 - val_accuracy: 0.7246\n",
            "Epoch 726/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4754 - accuracy: 0.8174 - val_loss: 0.6967 - val_accuracy: 0.7198\n",
            "Epoch 727/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4775 - accuracy: 0.8241 - val_loss: 0.6795 - val_accuracy: 0.7488\n",
            "Epoch 728/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4769 - accuracy: 0.8277 - val_loss: 0.6793 - val_accuracy: 0.7440\n",
            "Epoch 729/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5157 - accuracy: 0.7987 - val_loss: 0.6864 - val_accuracy: 0.7440\n",
            "Epoch 730/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4544 - accuracy: 0.8325 - val_loss: 0.6896 - val_accuracy: 0.7536\n",
            "Epoch 731/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4719 - accuracy: 0.8210 - val_loss: 0.6701 - val_accuracy: 0.7585\n",
            "Epoch 732/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4740 - accuracy: 0.8071 - val_loss: 0.6694 - val_accuracy: 0.7681\n",
            "Epoch 733/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5026 - accuracy: 0.8096 - val_loss: 0.6778 - val_accuracy: 0.7488\n",
            "Epoch 734/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4909 - accuracy: 0.8096 - val_loss: 0.6804 - val_accuracy: 0.7440\n",
            "Epoch 735/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4641 - accuracy: 0.8222 - val_loss: 0.7058 - val_accuracy: 0.7343\n",
            "Epoch 736/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4829 - accuracy: 0.8150 - val_loss: 0.6726 - val_accuracy: 0.7488\n",
            "Epoch 737/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4940 - accuracy: 0.8005 - val_loss: 0.6957 - val_accuracy: 0.7391\n",
            "Epoch 738/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4894 - accuracy: 0.8023 - val_loss: 0.6686 - val_accuracy: 0.7488\n",
            "Epoch 739/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4891 - accuracy: 0.8059 - val_loss: 0.6833 - val_accuracy: 0.7295\n",
            "Epoch 740/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4952 - accuracy: 0.8089 - val_loss: 0.6696 - val_accuracy: 0.7633\n",
            "Epoch 741/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4882 - accuracy: 0.8174 - val_loss: 0.6708 - val_accuracy: 0.7633\n",
            "Epoch 742/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4847 - accuracy: 0.8102 - val_loss: 0.6713 - val_accuracy: 0.7681\n",
            "Epoch 743/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4912 - accuracy: 0.8192 - val_loss: 0.6701 - val_accuracy: 0.7633\n",
            "Epoch 744/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4749 - accuracy: 0.8210 - val_loss: 0.6883 - val_accuracy: 0.7343\n",
            "Epoch 745/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4794 - accuracy: 0.8065 - val_loss: 0.6947 - val_accuracy: 0.7536\n",
            "Epoch 746/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4598 - accuracy: 0.8289 - val_loss: 0.6832 - val_accuracy: 0.7343\n",
            "Epoch 747/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4883 - accuracy: 0.8114 - val_loss: 0.6767 - val_accuracy: 0.7488\n",
            "Epoch 748/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4844 - accuracy: 0.8174 - val_loss: 0.6808 - val_accuracy: 0.7198\n",
            "Epoch 749/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4645 - accuracy: 0.8289 - val_loss: 0.6803 - val_accuracy: 0.7391\n",
            "Epoch 750/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4750 - accuracy: 0.8180 - val_loss: 0.6644 - val_accuracy: 0.7681\n",
            "Epoch 751/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4720 - accuracy: 0.8247 - val_loss: 0.6715 - val_accuracy: 0.7246\n",
            "Epoch 752/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4697 - accuracy: 0.8265 - val_loss: 0.6664 - val_accuracy: 0.7585\n",
            "Epoch 753/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4788 - accuracy: 0.8150 - val_loss: 0.6610 - val_accuracy: 0.7633\n",
            "Epoch 754/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4680 - accuracy: 0.8150 - val_loss: 0.6554 - val_accuracy: 0.7585\n",
            "Epoch 755/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4586 - accuracy: 0.8319 - val_loss: 0.6657 - val_accuracy: 0.7536\n",
            "Epoch 756/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4709 - accuracy: 0.8210 - val_loss: 0.6786 - val_accuracy: 0.7295\n",
            "Epoch 757/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4557 - accuracy: 0.8210 - val_loss: 0.6961 - val_accuracy: 0.7295\n",
            "Epoch 758/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4759 - accuracy: 0.8102 - val_loss: 0.6883 - val_accuracy: 0.7343\n",
            "Epoch 759/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4734 - accuracy: 0.8259 - val_loss: 0.6853 - val_accuracy: 0.7440\n",
            "Epoch 760/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4555 - accuracy: 0.8162 - val_loss: 0.6751 - val_accuracy: 0.7440\n",
            "Epoch 761/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4507 - accuracy: 0.8192 - val_loss: 0.6986 - val_accuracy: 0.7440\n",
            "Epoch 762/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4712 - accuracy: 0.8229 - val_loss: 0.6591 - val_accuracy: 0.7778\n",
            "Epoch 763/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4517 - accuracy: 0.8325 - val_loss: 0.6855 - val_accuracy: 0.7343\n",
            "Epoch 764/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4664 - accuracy: 0.8277 - val_loss: 0.6642 - val_accuracy: 0.7488\n",
            "Epoch 765/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4732 - accuracy: 0.8235 - val_loss: 0.6653 - val_accuracy: 0.7440\n",
            "Epoch 766/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4799 - accuracy: 0.8108 - val_loss: 0.6551 - val_accuracy: 0.7440\n",
            "Epoch 767/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4635 - accuracy: 0.8216 - val_loss: 0.6674 - val_accuracy: 0.7440\n",
            "Epoch 768/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4412 - accuracy: 0.8331 - val_loss: 0.6658 - val_accuracy: 0.7633\n",
            "Epoch 769/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4570 - accuracy: 0.8180 - val_loss: 0.6744 - val_accuracy: 0.7391\n",
            "Epoch 770/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4685 - accuracy: 0.8162 - val_loss: 0.6732 - val_accuracy: 0.7633\n",
            "Epoch 771/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4695 - accuracy: 0.8144 - val_loss: 0.6684 - val_accuracy: 0.7536\n",
            "Epoch 772/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4654 - accuracy: 0.8216 - val_loss: 0.6730 - val_accuracy: 0.7391\n",
            "Epoch 773/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4688 - accuracy: 0.8162 - val_loss: 0.6595 - val_accuracy: 0.7585\n",
            "Epoch 774/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4434 - accuracy: 0.8343 - val_loss: 0.6580 - val_accuracy: 0.7585\n",
            "Epoch 775/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4547 - accuracy: 0.8265 - val_loss: 0.6740 - val_accuracy: 0.7729\n",
            "Epoch 776/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4689 - accuracy: 0.8138 - val_loss: 0.6805 - val_accuracy: 0.7536\n",
            "Epoch 777/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4469 - accuracy: 0.8295 - val_loss: 0.6867 - val_accuracy: 0.7391\n",
            "Epoch 778/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4382 - accuracy: 0.8283 - val_loss: 0.6856 - val_accuracy: 0.7295\n",
            "Epoch 779/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4344 - accuracy: 0.8368 - val_loss: 0.6741 - val_accuracy: 0.7343\n",
            "Epoch 780/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4687 - accuracy: 0.8114 - val_loss: 0.6698 - val_accuracy: 0.7295\n",
            "Epoch 781/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4548 - accuracy: 0.8325 - val_loss: 0.6711 - val_accuracy: 0.7536\n",
            "Epoch 782/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4522 - accuracy: 0.8222 - val_loss: 0.6717 - val_accuracy: 0.7391\n",
            "Epoch 783/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4696 - accuracy: 0.8222 - val_loss: 0.6907 - val_accuracy: 0.7488\n",
            "Epoch 784/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4823 - accuracy: 0.8283 - val_loss: 0.6531 - val_accuracy: 0.7729\n",
            "Epoch 785/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4469 - accuracy: 0.8247 - val_loss: 0.6802 - val_accuracy: 0.7440\n",
            "Epoch 786/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4616 - accuracy: 0.8271 - val_loss: 0.6588 - val_accuracy: 0.7633\n",
            "Epoch 787/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4376 - accuracy: 0.8319 - val_loss: 0.6898 - val_accuracy: 0.7295\n",
            "Epoch 788/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4513 - accuracy: 0.8216 - val_loss: 0.6723 - val_accuracy: 0.7391\n",
            "Epoch 789/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4437 - accuracy: 0.8325 - val_loss: 0.6790 - val_accuracy: 0.7488\n",
            "Epoch 790/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4503 - accuracy: 0.8356 - val_loss: 0.6683 - val_accuracy: 0.7826\n",
            "Epoch 791/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4308 - accuracy: 0.8374 - val_loss: 0.6683 - val_accuracy: 0.7681\n",
            "Epoch 792/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4311 - accuracy: 0.8386 - val_loss: 0.6764 - val_accuracy: 0.7585\n",
            "Epoch 793/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4399 - accuracy: 0.8301 - val_loss: 0.6545 - val_accuracy: 0.7681\n",
            "Epoch 794/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4645 - accuracy: 0.8168 - val_loss: 0.6743 - val_accuracy: 0.7391\n",
            "Epoch 795/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4484 - accuracy: 0.8271 - val_loss: 0.6749 - val_accuracy: 0.7391\n",
            "Epoch 796/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 0.4401 - accuracy: 0.8362 - val_loss: 0.6784 - val_accuracy: 0.7488\n",
            "Epoch 797/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4434 - accuracy: 0.8247 - val_loss: 0.6480 - val_accuracy: 0.7874\n",
            "Epoch 798/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4470 - accuracy: 0.8289 - val_loss: 0.6888 - val_accuracy: 0.7246\n",
            "Epoch 799/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4452 - accuracy: 0.8313 - val_loss: 0.6655 - val_accuracy: 0.7633\n",
            "Epoch 800/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4722 - accuracy: 0.8198 - val_loss: 0.6728 - val_accuracy: 0.7343\n",
            "Epoch 801/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4595 - accuracy: 0.8235 - val_loss: 0.6679 - val_accuracy: 0.7440\n",
            "Epoch 802/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4368 - accuracy: 0.8319 - val_loss: 0.6466 - val_accuracy: 0.7826\n",
            "Epoch 803/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4570 - accuracy: 0.8198 - val_loss: 0.6584 - val_accuracy: 0.7440\n",
            "Epoch 804/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4211 - accuracy: 0.8289 - val_loss: 0.6440 - val_accuracy: 0.7536\n",
            "Epoch 805/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4248 - accuracy: 0.8428 - val_loss: 0.6512 - val_accuracy: 0.7536\n",
            "Epoch 806/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4340 - accuracy: 0.8349 - val_loss: 0.6535 - val_accuracy: 0.7585\n",
            "Epoch 807/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4523 - accuracy: 0.8331 - val_loss: 0.6547 - val_accuracy: 0.7488\n",
            "Epoch 808/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4287 - accuracy: 0.8295 - val_loss: 0.6814 - val_accuracy: 0.7343\n",
            "Epoch 809/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4531 - accuracy: 0.8162 - val_loss: 0.6633 - val_accuracy: 0.7391\n",
            "Epoch 810/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4407 - accuracy: 0.8349 - val_loss: 0.6629 - val_accuracy: 0.7440\n",
            "Epoch 811/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4392 - accuracy: 0.8259 - val_loss: 0.6621 - val_accuracy: 0.7536\n",
            "Epoch 812/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4539 - accuracy: 0.8216 - val_loss: 0.6578 - val_accuracy: 0.7536\n",
            "Epoch 813/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4330 - accuracy: 0.8368 - val_loss: 0.6582 - val_accuracy: 0.7440\n",
            "Epoch 814/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4400 - accuracy: 0.8301 - val_loss: 0.6405 - val_accuracy: 0.7826\n",
            "Epoch 815/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4296 - accuracy: 0.8313 - val_loss: 0.6629 - val_accuracy: 0.7488\n",
            "Epoch 816/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4247 - accuracy: 0.8416 - val_loss: 0.6539 - val_accuracy: 0.7440\n",
            "Epoch 817/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4268 - accuracy: 0.8404 - val_loss: 0.6808 - val_accuracy: 0.7343\n",
            "Epoch 818/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4161 - accuracy: 0.8501 - val_loss: 0.6487 - val_accuracy: 0.7778\n",
            "Epoch 819/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4220 - accuracy: 0.8440 - val_loss: 0.6776 - val_accuracy: 0.7488\n",
            "Epoch 820/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4255 - accuracy: 0.8374 - val_loss: 0.6598 - val_accuracy: 0.7488\n",
            "Epoch 821/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4349 - accuracy: 0.8404 - val_loss: 0.6567 - val_accuracy: 0.7778\n",
            "Epoch 822/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4371 - accuracy: 0.8313 - val_loss: 0.6441 - val_accuracy: 0.7778\n",
            "Epoch 823/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4404 - accuracy: 0.8229 - val_loss: 0.6628 - val_accuracy: 0.7488\n",
            "Epoch 824/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4201 - accuracy: 0.8392 - val_loss: 0.6759 - val_accuracy: 0.7488\n",
            "Epoch 825/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4152 - accuracy: 0.8434 - val_loss: 0.6757 - val_accuracy: 0.7585\n",
            "Epoch 826/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4626 - accuracy: 0.8102 - val_loss: 0.6473 - val_accuracy: 0.7729\n",
            "Epoch 827/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4231 - accuracy: 0.8464 - val_loss: 0.6762 - val_accuracy: 0.7440\n",
            "Epoch 828/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4280 - accuracy: 0.8319 - val_loss: 0.6570 - val_accuracy: 0.7633\n",
            "Epoch 829/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4343 - accuracy: 0.8422 - val_loss: 0.6489 - val_accuracy: 0.7681\n",
            "Epoch 830/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4464 - accuracy: 0.8337 - val_loss: 0.6537 - val_accuracy: 0.7391\n",
            "Epoch 831/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4301 - accuracy: 0.8452 - val_loss: 0.6800 - val_accuracy: 0.7440\n",
            "Epoch 832/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4235 - accuracy: 0.8446 - val_loss: 0.6709 - val_accuracy: 0.7633\n",
            "Epoch 833/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4323 - accuracy: 0.8428 - val_loss: 0.6601 - val_accuracy: 0.7633\n",
            "Epoch 834/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4565 - accuracy: 0.8313 - val_loss: 0.6821 - val_accuracy: 0.7488\n",
            "Epoch 835/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4166 - accuracy: 0.8416 - val_loss: 0.6451 - val_accuracy: 0.7874\n",
            "Epoch 836/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4105 - accuracy: 0.8513 - val_loss: 0.6569 - val_accuracy: 0.7729\n",
            "Epoch 837/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4492 - accuracy: 0.8259 - val_loss: 0.6717 - val_accuracy: 0.7391\n",
            "Epoch 838/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4399 - accuracy: 0.8386 - val_loss: 0.6796 - val_accuracy: 0.7488\n",
            "Epoch 839/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4269 - accuracy: 0.8416 - val_loss: 0.6550 - val_accuracy: 0.7391\n",
            "Epoch 840/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4258 - accuracy: 0.8325 - val_loss: 0.6621 - val_accuracy: 0.7343\n",
            "Epoch 841/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 0.4020 - accuracy: 0.8446 - val_loss: 0.6458 - val_accuracy: 0.7440\n",
            "Epoch 842/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4118 - accuracy: 0.8446 - val_loss: 0.6597 - val_accuracy: 0.7440\n",
            "Epoch 843/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4145 - accuracy: 0.8452 - val_loss: 0.6652 - val_accuracy: 0.7633\n",
            "Epoch 844/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4098 - accuracy: 0.8507 - val_loss: 0.6454 - val_accuracy: 0.7585\n",
            "Epoch 845/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4131 - accuracy: 0.8525 - val_loss: 0.6485 - val_accuracy: 0.7343\n",
            "Epoch 846/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4172 - accuracy: 0.8464 - val_loss: 0.6498 - val_accuracy: 0.7488\n",
            "Epoch 847/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4157 - accuracy: 0.8392 - val_loss: 0.6407 - val_accuracy: 0.7536\n",
            "Epoch 848/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4227 - accuracy: 0.8434 - val_loss: 0.6669 - val_accuracy: 0.7488\n",
            "Epoch 849/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4186 - accuracy: 0.8476 - val_loss: 0.6541 - val_accuracy: 0.7633\n",
            "Epoch 850/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3933 - accuracy: 0.8482 - val_loss: 0.6384 - val_accuracy: 0.7729\n",
            "Epoch 851/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4115 - accuracy: 0.8404 - val_loss: 0.6399 - val_accuracy: 0.7633\n",
            "Epoch 852/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4032 - accuracy: 0.8452 - val_loss: 0.6441 - val_accuracy: 0.7778\n",
            "Epoch 853/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3997 - accuracy: 0.8482 - val_loss: 0.6477 - val_accuracy: 0.7633\n",
            "Epoch 854/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4086 - accuracy: 0.8519 - val_loss: 0.6526 - val_accuracy: 0.7440\n",
            "Epoch 855/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3982 - accuracy: 0.8507 - val_loss: 0.6439 - val_accuracy: 0.7681\n",
            "Epoch 856/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3934 - accuracy: 0.8489 - val_loss: 0.6419 - val_accuracy: 0.7729\n",
            "Epoch 857/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4286 - accuracy: 0.8356 - val_loss: 0.6570 - val_accuracy: 0.7488\n",
            "Epoch 858/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3959 - accuracy: 0.8579 - val_loss: 0.6849 - val_accuracy: 0.7343\n",
            "Epoch 859/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4043 - accuracy: 0.8513 - val_loss: 0.6529 - val_accuracy: 0.7681\n",
            "Epoch 860/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3985 - accuracy: 0.8410 - val_loss: 0.6720 - val_accuracy: 0.7585\n",
            "Epoch 861/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4119 - accuracy: 0.8410 - val_loss: 0.6619 - val_accuracy: 0.7633\n",
            "Epoch 862/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4209 - accuracy: 0.8452 - val_loss: 0.6523 - val_accuracy: 0.7536\n",
            "Epoch 863/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4097 - accuracy: 0.8507 - val_loss: 0.6833 - val_accuracy: 0.7440\n",
            "Epoch 864/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 0.4196 - accuracy: 0.8416 - val_loss: 0.6687 - val_accuracy: 0.7295\n",
            "Epoch 865/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4042 - accuracy: 0.8507 - val_loss: 0.6588 - val_accuracy: 0.7585\n",
            "Epoch 866/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4019 - accuracy: 0.8507 - val_loss: 0.6878 - val_accuracy: 0.7343\n",
            "Epoch 867/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4157 - accuracy: 0.8476 - val_loss: 0.6647 - val_accuracy: 0.7440\n",
            "Epoch 868/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3965 - accuracy: 0.8519 - val_loss: 0.6487 - val_accuracy: 0.7778\n",
            "Epoch 869/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4036 - accuracy: 0.8380 - val_loss: 0.6601 - val_accuracy: 0.7585\n",
            "Epoch 870/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3831 - accuracy: 0.8555 - val_loss: 0.6531 - val_accuracy: 0.7729\n",
            "Epoch 871/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4040 - accuracy: 0.8416 - val_loss: 0.6408 - val_accuracy: 0.7633\n",
            "Epoch 872/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3741 - accuracy: 0.8543 - val_loss: 0.6372 - val_accuracy: 0.7681\n",
            "Epoch 873/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3968 - accuracy: 0.8452 - val_loss: 0.6524 - val_accuracy: 0.7729\n",
            "Epoch 874/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4007 - accuracy: 0.8501 - val_loss: 0.6554 - val_accuracy: 0.7488\n",
            "Epoch 875/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4145 - accuracy: 0.8337 - val_loss: 0.6498 - val_accuracy: 0.7440\n",
            "Epoch 876/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3958 - accuracy: 0.8464 - val_loss: 0.6563 - val_accuracy: 0.7536\n",
            "Epoch 877/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3927 - accuracy: 0.8567 - val_loss: 0.6473 - val_accuracy: 0.7391\n",
            "Epoch 878/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3868 - accuracy: 0.8452 - val_loss: 0.6327 - val_accuracy: 0.7681\n",
            "Epoch 879/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3784 - accuracy: 0.8597 - val_loss: 0.6437 - val_accuracy: 0.7633\n",
            "Epoch 880/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4038 - accuracy: 0.8398 - val_loss: 0.6483 - val_accuracy: 0.7440\n",
            "Epoch 881/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3835 - accuracy: 0.8489 - val_loss: 0.6393 - val_accuracy: 0.7585\n",
            "Epoch 882/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 0.3978 - accuracy: 0.8501 - val_loss: 0.6400 - val_accuracy: 0.7440\n",
            "Epoch 883/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3869 - accuracy: 0.8549 - val_loss: 0.6390 - val_accuracy: 0.7729\n",
            "Epoch 884/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3803 - accuracy: 0.8615 - val_loss: 0.6492 - val_accuracy: 0.7681\n",
            "Epoch 885/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3758 - accuracy: 0.8591 - val_loss: 0.6430 - val_accuracy: 0.7681\n",
            "Epoch 886/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3882 - accuracy: 0.8549 - val_loss: 0.6363 - val_accuracy: 0.7729\n",
            "Epoch 887/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4152 - accuracy: 0.8362 - val_loss: 0.6745 - val_accuracy: 0.7585\n",
            "Epoch 888/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3901 - accuracy: 0.8482 - val_loss: 0.6679 - val_accuracy: 0.7536\n",
            "Epoch 889/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4081 - accuracy: 0.8392 - val_loss: 0.6481 - val_accuracy: 0.7778\n",
            "Epoch 890/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3857 - accuracy: 0.8482 - val_loss: 0.6708 - val_accuracy: 0.7440\n",
            "Epoch 891/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4015 - accuracy: 0.8446 - val_loss: 0.6520 - val_accuracy: 0.7729\n",
            "Epoch 892/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3930 - accuracy: 0.8507 - val_loss: 0.6702 - val_accuracy: 0.7440\n",
            "Epoch 893/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 0.3814 - accuracy: 0.8555 - val_loss: 0.6259 - val_accuracy: 0.7536\n",
            "Epoch 894/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3877 - accuracy: 0.8597 - val_loss: 0.6456 - val_accuracy: 0.7633\n",
            "Epoch 895/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3850 - accuracy: 0.8567 - val_loss: 0.6418 - val_accuracy: 0.7440\n",
            "Epoch 896/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3906 - accuracy: 0.8464 - val_loss: 0.6534 - val_accuracy: 0.7585\n",
            "Epoch 897/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3718 - accuracy: 0.8603 - val_loss: 0.6451 - val_accuracy: 0.7681\n",
            "Epoch 898/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4106 - accuracy: 0.8380 - val_loss: 0.6700 - val_accuracy: 0.7585\n",
            "Epoch 899/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4025 - accuracy: 0.8464 - val_loss: 0.6544 - val_accuracy: 0.7488\n",
            "Epoch 900/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3740 - accuracy: 0.8597 - val_loss: 0.6680 - val_accuracy: 0.7585\n",
            "Epoch 901/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4125 - accuracy: 0.8482 - val_loss: 0.6582 - val_accuracy: 0.7440\n",
            "Epoch 902/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3741 - accuracy: 0.8603 - val_loss: 0.6582 - val_accuracy: 0.7343\n",
            "Epoch 903/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3840 - accuracy: 0.8579 - val_loss: 0.6520 - val_accuracy: 0.7633\n",
            "Epoch 904/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4108 - accuracy: 0.8380 - val_loss: 0.6308 - val_accuracy: 0.7778\n",
            "Epoch 905/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3931 - accuracy: 0.8525 - val_loss: 0.6449 - val_accuracy: 0.7729\n",
            "Epoch 906/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3901 - accuracy: 0.8458 - val_loss: 0.6493 - val_accuracy: 0.7729\n",
            "Epoch 907/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3616 - accuracy: 0.8688 - val_loss: 0.6301 - val_accuracy: 0.7585\n",
            "Epoch 908/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3746 - accuracy: 0.8609 - val_loss: 0.6429 - val_accuracy: 0.7681\n",
            "Epoch 909/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3800 - accuracy: 0.8549 - val_loss: 0.6393 - val_accuracy: 0.7681\n",
            "Epoch 910/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3783 - accuracy: 0.8664 - val_loss: 0.6505 - val_accuracy: 0.7729\n",
            "Epoch 911/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4068 - accuracy: 0.8452 - val_loss: 0.6344 - val_accuracy: 0.7729\n",
            "Epoch 912/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3637 - accuracy: 0.8567 - val_loss: 0.6395 - val_accuracy: 0.7778\n",
            "Epoch 913/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3848 - accuracy: 0.8525 - val_loss: 0.6374 - val_accuracy: 0.7729\n",
            "Epoch 914/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3797 - accuracy: 0.8585 - val_loss: 0.6707 - val_accuracy: 0.7488\n",
            "Epoch 915/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3803 - accuracy: 0.8519 - val_loss: 0.6581 - val_accuracy: 0.7391\n",
            "Epoch 916/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3904 - accuracy: 0.8507 - val_loss: 0.6493 - val_accuracy: 0.7536\n",
            "Epoch 917/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3680 - accuracy: 0.8543 - val_loss: 0.6554 - val_accuracy: 0.7681\n",
            "Epoch 918/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3917 - accuracy: 0.8561 - val_loss: 0.6468 - val_accuracy: 0.7536\n",
            "Epoch 919/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3657 - accuracy: 0.8652 - val_loss: 0.6170 - val_accuracy: 0.7971\n",
            "Epoch 920/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3754 - accuracy: 0.8591 - val_loss: 0.6514 - val_accuracy: 0.7729\n",
            "Epoch 921/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3708 - accuracy: 0.8634 - val_loss: 0.6521 - val_accuracy: 0.7681\n",
            "Epoch 922/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3773 - accuracy: 0.8567 - val_loss: 0.6511 - val_accuracy: 0.7488\n",
            "Epoch 923/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3543 - accuracy: 0.8634 - val_loss: 0.6464 - val_accuracy: 0.7681\n",
            "Epoch 924/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3683 - accuracy: 0.8622 - val_loss: 0.6564 - val_accuracy: 0.7536\n",
            "Epoch 925/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3435 - accuracy: 0.8748 - val_loss: 0.6463 - val_accuracy: 0.7681\n",
            "Epoch 926/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 0.3595 - accuracy: 0.8730 - val_loss: 0.6288 - val_accuracy: 0.7633\n",
            "Epoch 927/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 0.3542 - accuracy: 0.8646 - val_loss: 0.6674 - val_accuracy: 0.7536\n",
            "Epoch 928/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 0.3622 - accuracy: 0.8700 - val_loss: 0.6561 - val_accuracy: 0.7440\n",
            "Epoch 929/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3519 - accuracy: 0.8742 - val_loss: 0.6511 - val_accuracy: 0.7585\n",
            "Epoch 930/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3790 - accuracy: 0.8525 - val_loss: 0.6553 - val_accuracy: 0.7681\n",
            "Epoch 931/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 0.3699 - accuracy: 0.8549 - val_loss: 0.6520 - val_accuracy: 0.7633\n",
            "Epoch 932/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3354 - accuracy: 0.8724 - val_loss: 0.6421 - val_accuracy: 0.7633\n",
            "Epoch 933/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3672 - accuracy: 0.8591 - val_loss: 0.6336 - val_accuracy: 0.7778\n",
            "Epoch 934/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3745 - accuracy: 0.8585 - val_loss: 0.6348 - val_accuracy: 0.7729\n",
            "Epoch 935/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 0.3783 - accuracy: 0.8567 - val_loss: 0.6490 - val_accuracy: 0.7778\n",
            "Epoch 936/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3750 - accuracy: 0.8622 - val_loss: 0.6447 - val_accuracy: 0.7874\n",
            "Epoch 937/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3742 - accuracy: 0.8597 - val_loss: 0.6303 - val_accuracy: 0.7826\n",
            "Epoch 938/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3844 - accuracy: 0.8495 - val_loss: 0.6500 - val_accuracy: 0.7778\n",
            "Epoch 939/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3647 - accuracy: 0.8555 - val_loss: 0.6647 - val_accuracy: 0.7633\n",
            "Epoch 940/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3673 - accuracy: 0.8640 - val_loss: 0.6604 - val_accuracy: 0.7440\n",
            "Epoch 941/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3795 - accuracy: 0.8622 - val_loss: 0.6512 - val_accuracy: 0.7681\n",
            "Epoch 942/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3623 - accuracy: 0.8561 - val_loss: 0.6409 - val_accuracy: 0.7874\n",
            "Epoch 943/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3696 - accuracy: 0.8579 - val_loss: 0.6664 - val_accuracy: 0.7633\n",
            "Epoch 944/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3849 - accuracy: 0.8507 - val_loss: 0.6586 - val_accuracy: 0.7633\n",
            "Epoch 945/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3692 - accuracy: 0.8597 - val_loss: 0.6371 - val_accuracy: 0.7633\n",
            "Epoch 946/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3812 - accuracy: 0.8525 - val_loss: 0.6403 - val_accuracy: 0.7826\n",
            "Epoch 947/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3844 - accuracy: 0.8464 - val_loss: 0.6382 - val_accuracy: 0.7826\n",
            "Epoch 948/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3703 - accuracy: 0.8603 - val_loss: 0.6453 - val_accuracy: 0.7778\n",
            "Epoch 949/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3523 - accuracy: 0.8628 - val_loss: 0.6399 - val_accuracy: 0.7874\n",
            "Epoch 950/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3651 - accuracy: 0.8561 - val_loss: 0.6597 - val_accuracy: 0.7633\n",
            "Epoch 951/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3479 - accuracy: 0.8712 - val_loss: 0.6405 - val_accuracy: 0.7778\n",
            "Epoch 952/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3416 - accuracy: 0.8761 - val_loss: 0.6486 - val_accuracy: 0.7826\n",
            "Epoch 953/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3706 - accuracy: 0.8646 - val_loss: 0.6415 - val_accuracy: 0.7633\n",
            "Epoch 954/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 0.3660 - accuracy: 0.8597 - val_loss: 0.6955 - val_accuracy: 0.7488\n",
            "Epoch 955/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3753 - accuracy: 0.8567 - val_loss: 0.6557 - val_accuracy: 0.7681\n",
            "Epoch 956/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3487 - accuracy: 0.8658 - val_loss: 0.6408 - val_accuracy: 0.7633\n",
            "Epoch 957/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3625 - accuracy: 0.8609 - val_loss: 0.6464 - val_accuracy: 0.7778\n",
            "Epoch 958/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3567 - accuracy: 0.8718 - val_loss: 0.6519 - val_accuracy: 0.7681\n",
            "Epoch 959/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 0.3614 - accuracy: 0.8688 - val_loss: 0.6499 - val_accuracy: 0.7729\n",
            "Epoch 960/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3670 - accuracy: 0.8615 - val_loss: 0.6404 - val_accuracy: 0.7681\n",
            "Epoch 961/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3529 - accuracy: 0.8652 - val_loss: 0.6465 - val_accuracy: 0.7778\n",
            "Epoch 962/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3659 - accuracy: 0.8706 - val_loss: 0.6241 - val_accuracy: 0.7633\n",
            "Epoch 963/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3627 - accuracy: 0.8634 - val_loss: 0.6175 - val_accuracy: 0.7971\n",
            "Epoch 964/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3496 - accuracy: 0.8658 - val_loss: 0.6420 - val_accuracy: 0.7681\n",
            "Epoch 965/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3649 - accuracy: 0.8585 - val_loss: 0.6297 - val_accuracy: 0.7633\n",
            "Epoch 966/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3667 - accuracy: 0.8543 - val_loss: 0.6309 - val_accuracy: 0.7633\n",
            "Epoch 967/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3467 - accuracy: 0.8706 - val_loss: 0.6265 - val_accuracy: 0.8068\n",
            "Epoch 968/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 0.3517 - accuracy: 0.8658 - val_loss: 0.6191 - val_accuracy: 0.7971\n",
            "Epoch 969/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3607 - accuracy: 0.8609 - val_loss: 0.6242 - val_accuracy: 0.7778\n",
            "Epoch 970/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3245 - accuracy: 0.8761 - val_loss: 0.6451 - val_accuracy: 0.7633\n",
            "Epoch 971/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3440 - accuracy: 0.8615 - val_loss: 0.6542 - val_accuracy: 0.7633\n",
            "Epoch 972/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3607 - accuracy: 0.8670 - val_loss: 0.6490 - val_accuracy: 0.7681\n",
            "Epoch 973/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3569 - accuracy: 0.8652 - val_loss: 0.6399 - val_accuracy: 0.7778\n",
            "Epoch 974/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3442 - accuracy: 0.8724 - val_loss: 0.6280 - val_accuracy: 0.8019\n",
            "Epoch 975/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3621 - accuracy: 0.8712 - val_loss: 0.6689 - val_accuracy: 0.7585\n",
            "Epoch 976/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3485 - accuracy: 0.8628 - val_loss: 0.6323 - val_accuracy: 0.7681\n",
            "Epoch 977/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3407 - accuracy: 0.8658 - val_loss: 0.6685 - val_accuracy: 0.7536\n",
            "Epoch 978/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3888 - accuracy: 0.8476 - val_loss: 0.6349 - val_accuracy: 0.7874\n",
            "Epoch 979/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3492 - accuracy: 0.8670 - val_loss: 0.6562 - val_accuracy: 0.7488\n",
            "Epoch 980/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3373 - accuracy: 0.8688 - val_loss: 0.6342 - val_accuracy: 0.7778\n",
            "Epoch 981/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3383 - accuracy: 0.8706 - val_loss: 0.6416 - val_accuracy: 0.7778\n",
            "Epoch 982/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3430 - accuracy: 0.8646 - val_loss: 0.6601 - val_accuracy: 0.7585\n",
            "Epoch 983/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3322 - accuracy: 0.8748 - val_loss: 0.6509 - val_accuracy: 0.7681\n",
            "Epoch 984/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3435 - accuracy: 0.8748 - val_loss: 0.6304 - val_accuracy: 0.7729\n",
            "Epoch 985/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3553 - accuracy: 0.8676 - val_loss: 0.6421 - val_accuracy: 0.7585\n",
            "Epoch 986/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3492 - accuracy: 0.8658 - val_loss: 0.6188 - val_accuracy: 0.7778\n",
            "Epoch 987/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3439 - accuracy: 0.8676 - val_loss: 0.6467 - val_accuracy: 0.7778\n",
            "Epoch 988/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3456 - accuracy: 0.8755 - val_loss: 0.6214 - val_accuracy: 0.7778\n",
            "Epoch 989/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 0.3364 - accuracy: 0.8724 - val_loss: 0.6258 - val_accuracy: 0.7923\n",
            "Epoch 990/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3296 - accuracy: 0.8718 - val_loss: 0.6163 - val_accuracy: 0.8068\n",
            "Epoch 991/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3435 - accuracy: 0.8664 - val_loss: 0.6377 - val_accuracy: 0.7681\n",
            "Epoch 992/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3309 - accuracy: 0.8730 - val_loss: 0.6690 - val_accuracy: 0.7536\n",
            "Epoch 993/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3410 - accuracy: 0.8718 - val_loss: 0.6675 - val_accuracy: 0.7536\n",
            "Epoch 994/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3222 - accuracy: 0.8791 - val_loss: 0.6417 - val_accuracy: 0.7826\n",
            "Epoch 995/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3474 - accuracy: 0.8682 - val_loss: 0.6542 - val_accuracy: 0.7585\n",
            "Epoch 996/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3437 - accuracy: 0.8779 - val_loss: 0.6519 - val_accuracy: 0.7585\n",
            "Epoch 997/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 0.3263 - accuracy: 0.8821 - val_loss: 0.6194 - val_accuracy: 0.7826\n",
            "Epoch 998/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3268 - accuracy: 0.8833 - val_loss: 0.6160 - val_accuracy: 0.7971\n",
            "Epoch 999/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3584 - accuracy: 0.8628 - val_loss: 0.6319 - val_accuracy: 0.7874\n",
            "Epoch 1000/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3472 - accuracy: 0.8609 - val_loss: 0.6425 - val_accuracy: 0.7681\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(cnnhistory.history['loss'])\n",
        "plt.plot(cnnhistory.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train','test'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "oQYnuaCrDH_A",
        "outputId": "6b481006-d08c-4776-deff-499290af23bd"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEWCAYAAABsY4yMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXwV9b3/8dfnJCd7CCEJEIhsLiyCgiCKWxWrgrYuxdqqWK/a4u+2t2pvr7W22mrb2/a2t2rt4tKrXdRqXat1qYoCalFWESOggCwJawhkJfv5/v6YSUhIgCTk5CST9/PxyCPnzHLmOxl4z/d8vzPfMeccIiISPKFYF0BERKJDAS8iElAKeBGRgFLAi4gElAJeRCSgFPAiIgGlgBcBzOxPZvaTdi670cw+e7ifIxJtCngRkYBSwIuIBJQCXnoNv2nkZjNbaWaVZvaQmQ0ys1fMrNzM5ppZZrPlLzSzj8ysxMzmm9nYZvMmmdlyf72/AUn7betzZrbCX3ehmR3XyTJ/zczWmdluM3vBzIb4083M7jaznWZWZmYfmtl4f975ZrbKL9sWM/uvTv3BpM9TwEtvMws4BzgG+DzwCvA9IAfv3/MNAGZ2DPA4cJM/72XgH2aWYGYJwN+BR4ABwFP+5+KvOwl4GLgeyAIeAF4ws8SOFNTMpgM/Ay4DcoFNwBP+7HOBM/z9yPCXKfbnPQRc75xLB8YDb3ZkuyKNFPDS2/zGObfDObcFeBtY5Jx73zlXDTwHTPKX+xLwknPudedcHfC/QDJwCnAyEAbucc7VOeeeBpY028Yc4AHn3CLnXINz7s9Ajb9eR1wJPOycW+6cqwFuBaaZ2QigDkgHxgDmnFvtnNvmr1cHjDOzfs65Pc655R3crgiggJfeZ0ez11VtvE/zXw/BqzED4JyLAAXAUH/eFtdypL1NzV4PB77tN8+UmFkJcIS/XkfsX4YKvFr6UOfcm8Bvgd8BO83sQTPr5y86Czgf2GRmC8xsWge3KwIo4CW4tuIFNeC1eeOF9BZgGzDUn9ZoWLPXBcB/O+f6N/tJcc49fphlSMVr8tkC4Jy71zk3GRiH11Rzsz99iXPuImAgXlPSkx3crgiggJfgehK4wMzONrMw8G28ZpaFwLtAPXCDmYXN7AvA1Gbr/gH4f2Z2kt8ZmmpmF5hZegfL8DhwjZlN9Nvvf4rXpLTRzE70Pz8MVALVQMTvI7jSzDL8pqUyIHIYfwfpwxTwEkjOuY+B2cBvgF14HbKfd87VOudqgS8A/wbsxmuvf7bZukuBr+E1oewB1vnLdrQMc4HbgWfwvjUcCXzZn90P70SyB68Zpxj4pT/vKmCjmZUB/w+vLV+kw0wP/BARCSbV4EVEAkoBLyISUAp4EZGAUsCLiARUfKwL0Fx2drYbMWJErIshItJrLFu2bJdzLqeteT0q4EeMGMHSpUtjXQwRkV7DzDYdaJ6aaEREAkoBLyISUAp4EZGA6lFt8G2pq6ujsLCQ6urqWBclqpKSksjLyyMcDse6KCISED0+4AsLC0lPT2fEiBG0HPwvOJxzFBcXU1hYyMiRI2NdHBEJiB7fRFNdXU1WVlZgwx3AzMjKygr8txQR6V49PuCBQId7o76wjyLSvXpFwB/KjrJqyqvrYl0MEZEeJRABX1ReQ0VNfVQ+u6SkhN///vcdXu/888+npKQkCiUSEWmfQAQ8QLSGtT9QwNfXH/yE8vLLL9O/f//oFEpEpB16/FU07RHN5uvvfve7rF+/nokTJxIOh0lKSiIzM5M1a9bwySefcPHFF1NQUEB1dTU33ngjc+bMAfYNu1BRUcHMmTM57bTTWLhwIUOHDuX5558nOTk5eoUWEaGXBfyd//iIVVvLWk3fW1tPfChEQnzHv5CMG9KPH37+2APO//nPf05+fj4rVqxg/vz5XHDBBeTn5zddzvjwww8zYMAAqqqqOPHEE5k1axZZWVktPmPt2rU8/vjj/OEPf+Cyyy7jmWeeYfbs2R0uq4hIR/SqgD+w7rsCZerUqS2uVb/33nt57rnnACgoKGDt2rWtAn7kyJFMnDgRgMmTJ7Nx48ZuK6+I9F29KuAPVNNetbWMfsnx5GWmRL0MqampTa/nz5/P3Llzeffdd0lJSeHMM89s81r2xMTEptdxcXFUVVVFvZwiIoHoZI1mG3x6ejrl5eVtzistLSUzM5OUlBTWrFnDe++9F72CiIh0UK+qwR9MtK6iycrK4tRTT2X8+PEkJyczaNCgpnkzZszg/vvvZ+zYsYwePZqTTz45OoUQEekEc9FKxk6YMmWK2/+BH6tXr2bs2LEHXW/NtjJSE+M5YkD0m2iiqT37KiLSnJktc85NaWteIJpourGPVUSk1whEwBvRa6IREemtAhHwfsTHuhAiIj1KQAJe8S4isr9ABLxG2hURaS0QAQ9qgxcR2V9UA97MNprZh2a2wsyWHnqNTm4nWh9M54cLBrjnnnvYu3dvF5dIRKR9uqMGf5ZzbuKBrtPsEha9NngFvIj0VoG4k9WiWIdvPlzwOeecw8CBA3nyySepqanhkksu4c4776SyspLLLruMwsJCGhoauP3229mxYwdbt27lrLPOIjs7m3nz5kWtjCIibYl2wDvgNTNzwAPOuQf3X8DM5gBzAIYNG3bwT3vlu7D9w1aTh9Y1eC/CcR0v4eAJMPPnB5zdfLjg1157jaeffprFixfjnOPCCy/krbfeoqioiCFDhvDSSy8B3hg1GRkZ3HXXXcybN4/s7OyOl0tE5DBFu4nmNOfcCcBM4Btmdsb+CzjnHnTOTXHOTcnJyYlycQ7Pa6+9xmuvvcakSZM44YQTWLNmDWvXrmXChAm8/vrr3HLLLbz99ttkZGTEuqgiItGtwTvntvi/d5rZc8BU4K1Of+ABatpbiyoAODInrdMf3R7OOW699Vauv/76VvOWL1/Oyy+/zG233cbZZ5/ND37wg6iWRUTkUKJWgzezVDNLb3wNnAvkR2VbELVe1ubDBZ933nk8/PDDVFR4J5QtW7awc+dOtm7dSkpKCrNnz+bmm29m+fLlrdYVEelu0azBDwKeM+8upHjgr865f0ZrY9G6iqb5cMEzZ87kiiuuYNq0aQCkpaXx6KOPsm7dOm6++WZCoRDhcJj77rsPgDlz5jBjxgyGDBmiTlYR6XaBGC7406IKIg6OGhjdJppo03DBItJRgR8u2PuW0HNOVCIiPUEwAh4NVSAisr9eEfA9qRkpWvrCPopI9+rxAZ+UlERxcfEhA7A3x6NzjuLiYpKSkmJdFBEJkB4/VEFeXh6FhYUUFRUdcJniilrqIxEadvfegExKSiIvLy/WxRCRAOnxAR8Ohxk5cuRBl/nGY8tZs72MN749qZtKJSLS8/X4Jpr2CIWMSG9uoxERiYJABHycQUSdlCIiLQQi4ENmNKgKLyLSQjACPmS6Dl5EZD/BCHhDNXgRkf0EIuDjQkaDqvAiIi0EIuBDZroTVERkP4EJeDXRiIi0FIiAjwsp4EVE9heIgDfTaJIiIvsLRMDHmTpZRUT2F4yAD5nuZBUR2U8gAt7MiERiXQoRkZ4lEAEfF0JNNCIi+wlGwJuaaERE9heIgDfzxqLRzU4iIvsEIuDjQgagMeFFRJoJRMD7+a6bnUREmglGwDfV4BXwIiKNAhHwcaaAFxHZXyACPuQHvJpoRET2CUbAq5NVRKSVYAS838kaUcKLiDSJesCbWZyZvW9mL0ZrG42XSepuVhGRfbqjBn8jsDqaGwipk1VEpJWoBryZ5QEXAP8Xze00BbwGHBMRaRLtGvw9wHeAA0avmc0xs6VmtrSoqKhTG4nz90I1eBGRfaIW8Gb2OWCnc27ZwZZzzj3onJvinJuSk5PT2W0BukxSRKS5aNbgTwUuNLONwBPAdDN7NBob0o1OIiKtRS3gnXO3OufynHMjgC8DbzrnZkdjWxpsTESktWBcB994maR6WUVEmsR3x0acc/OB+dH6/LimNvhobUFEpPcJRA2+8SoadbKKiOwTkID3dkOdrCIi+wQk4L3f9arBi4g0CUTAa7hgEZHWAhHw8X4TjQJeRGSfQAR8SJ2sIiKtBCLg49XJKiLSSiACXp2sIiKtBSLg9w0XrIAXEWkUiIBXJ6uISGuBCPiQmmhERFoJRMCrk1VEpLVABLw6WUVEWgtEwKuTVUSktUAEvDpZRURaC0TA605WEZHWAhHwTTV4dbKKiDQJRMDrMkkRkdYCEfBx6mQVEWklEAGvTlYRkdYCEfDqZBURaS0QAR/273Sqi0RiXBIRkZ4jEAEfH/La4OsbVIMXEWkUiICPCxlmUNegGryISKNABLyZEY4LUacavIhIk0AEPEBCXEg1eBGRZgIT8OE4U8CLiDQToIBXDV5EpLlABXxtvdrgRUQaBSjg1UQjItJc1ALezJLMbLGZfWBmH5nZndHaFqiJRkRkf+0KeDO70cz6mechM1tuZuceYrUaYLpz7nhgIjDDzE4+3AIfiC6TFBFpqb01+Gudc2XAuUAmcBXw84Ot4DwV/tuw/xO1BA7HqwYvItJcewPe/N/nA4845z5qNu3AK5nFmdkKYCfwunNuURvLzDGzpWa2tKioqL3lbiVBbfAiIi20N+CXmdlreAH/qpmlA4dMU+dcg3NuIpAHTDWz8W0s86BzbopzbkpOTk5Hyt5CQnyI6rqGTq8vIhI08e1c7jq8dvRPnXN7zWwAcE17N+KcKzGzecAMIL/jxTy0/skJbCsti8ZHi4j0Su2twU8DPvaDejZwG1B6sBXMLMfM+vuvk4FzgDWHU9iDyUwNs6eyNlofLyLS67Q34O8D9prZ8cC3gfXAXw6xTi4wz8xWAkvw2uBf7HRJD2FASgIlVXV66IeIiK+9TTT1zjlnZhcBv3XOPWRm1x1sBefcSmDSYZewnTJSEnAOyqrqyExN6K7Nioj0WO2twZeb2a14l0e+ZGYhvMsee4z0RO9cVVFTH+OSiIj0DO0N+C/h3bh0rXNuO95VMb+MWqk6IdUP+MpaBbyICLQz4P1QfwzIMLPPAdXOuUO1wXertCS/Bl+tgBcRgfYPVXAZsBj4InAZsMjMLo1mwToqLTEOUBONiEij9nayfh840Tm3E7xLIIG5wNPRKlhHNTXR1OhmJxERaH8bfKgx3H3FHVi3W6QmNAa8avAiItD+Gvw/zexV4HH//ZeAl6NTpM5J99vgyxXwIiJAOwPeOXezmc0CTvUnPeicey56xeq4fU00CngREWh/DR7n3DPAM1Esy2EJx4VIiA8p4EVEfAcNeDMrp+0x3A1vyPd+USlVJ6UnxquJRkTEd9CAd86ld1dBukJqYrxq8CIivh51JczhUsCLiOwTqIBPS4zTjU4iIr6ABXy8Al5ExBeogPeaaHQnq4gIBCzgVYMXEdknUAGfmhiv0SRFRHyBCviM5DBVdQ3U1kdiXRQRkZgLVMBnpngPmSqp0sO3RUSCFfD+s1j3VNbFuCQiIrEXqIAfkOIF/O5K1eBFRAIV8AP7JQKwo6w6xiUREYm9QAV8XmYKAHNX74hxSUREYi9QAZ8UjiMcZ8z/uCjWRRERiblABTzAv595FBU19ZRVq6NVRPq2wAX8uFxvhOPNxXtjXBIRkdgKXMAPG5AKwCYFvIj0cYEL+OFZXkfrmu1lMS6JiEhsBS7gUxPjOXFEJm+s3hnrooiIxFTUAt7MjjCzeWa2ysw+MrMbo7Wt/R01MJ2d5TXdtTkRkR7poM9kPUz1wLedc8vNLB1YZmavO+dWRXGbAGSnJbC7soZIxBEKWbQ3JyLSI0WtBu+c2+acW+6/LgdWA0Ojtb3mslITiDh4v2BPd2xORKRH6pY2eDMbAUwCFrUxb46ZLTWzpUVFXXOD0lljBpIQH+JnL6/BOdclnyki0ttEPeDNLA14BrjJOdfq0hbn3IPOuSnOuSk5OTldss3hWal848yjWLppD0UVaosXkb4pqgFvZmG8cH/MOfdsNLe1vwl5/QC47k9Lu3OzIiI9RjSvojHgIWC1c+6uaG0HgL27oWRzi0njh2YA8OGWUjbsqozq5kVEeqJo1uBPBa4CppvZCv/n/Khs6TcnwD0ToK4a/Db3gelJvPjN0wD475dW0xBRW7yI9C3RvIrmHeecOeeOc85N9H9ejsrGqvyrZf57EOQ/0zR5/NAMzh03iLmrd/DrN9ZGZdMiIj1V77+TtX6/pze9c3eLt6cfnQ3AvW+sZdmmPVTXNXRXyUREYqr3B3x8AtzWbFiCHflQtg02vgPAlScNZ9gAb3yaWfctZMzt/1SbvIj0Cb0/4AHiE1u+v2sM/OkCKPqEUMh46YbTuGTSvnusZtzzVjcXUESk+wUj4A9k2wewPZ/0pDA/nzWB1IQ4AGrqI/zy1TUxLpyISHQFJ+Av+wvMeqjltGe/CvefCiufJDE+jpV3nOcPJ+wofet+fvrcEqpqG6hviMSkyCIi0RTNwca617iLvN8fPg2fvNJy3rNfg+QBxIWT+XvuX3i2tIrr4l/hkWUFjF10LWMGp/PSDacTp4HJRCRAghPwja54ArathNJCeOLyfdMfmwVAJnCdv9eDbTcAa7aX88d/beCrp4/q5sKKiERPcJpomss9DsacD19/D27Kh7jENhebnl3KC8Of5vhBYX7y0iquf2Qpq7fpSVAiEgzWk0ZbnDJlilu6NApjxzgHC34BFTtg0mz4w1ltLjay+lEcIdIT4/nr105mQl5G15dFRKQLmdky59yUtuYFswa/PzM48xb43F0w9AT43N1tLnbraV6gl9fUM+u+hTy5pIB1O8u7s6QiIl2mb9Tg21K2FYrXwcgz4NnrYeUTAPz62KdYsD1MaOtyVrijqCee+2dPZsb4wd1TLhGRDjhYDb7vBnxz9bXwk9Zj0f+x/jzurL8agJvPG83XzzwSb5BMEZGeQQHfHts/hPfugxWPtZi8rd9E7iqeyprIMD50o8hOS+AXlx7H9DGDYlNOEZFmFPAdsWcjrHoBQnGw7M+w6+OmWbfWXcc/G06kMq4/3zrnGJLCIa45dWTsyioifZ4CvrMiDfCbybBnQ4vJJ1b/niL6A17/7fz/OpPhWamxKKGI9HG6iqazQnHwtTfhK8/DF//UNHlJ0te5fpw3vIFz8Jlfzqe4ooadZdUxKqiISGuqwXeEc7D0YXjpPwGIjLuEeSnn8c13EthLEgArfnAO/VMSYllKEelD1ETT1ebe0erBImOq/8ik0DrWRYZyzknHMeuEoWwpqebC44fEpowi0iccLOCDNxZNdzj7h5A+BF65uWnSmqRrml5ftvh2rl10BKWkUbq3litPGk5IA5mJSDdTDf5wlW+HX41uNXlhZBxX1N4GQEZymIsmDuFHF43v7tKJSMCpiSbaqvZ4TTabF0HBe02T1w84g0u2XkUZ+66wuemzR9MvKcw1p47QTVMictgU8N1p9wa4d2LT29KxV3DfpiH8c08uG11ui0XPOCaHuy87nqy0tke7FBE5FAV8d3MOPp0Pj1zcYvJdI//A01uz2Fra8nLKy6bk8f0LxrF2RzlH5qSRmaqrcESkfRTwsVK8HhY9ADvyYdO/miZXD/sMn2zbw0dVWSRaHd+pm0O93999+tHZPHLdSbEqsYj0Mgr4nmDbB/DAGW3Ourz+Dt6tP6bpfXpSPOOHZHDHhccyenB6d5VQRHohBXxPUVUCmxbCM9dBUgaUb2uatTfvdP40+PssWf0p84r7N00/79hBjMvN4PrPjCIpHBeLUotID6aA76kqd8Hjl8OWpeAiTZNLzr2XR15+k+WRo3k/chQleLX4pHCI604byTnjBjPxiP4H+lQR6UMU8L3BB3+D5+a0OevVwV/jz5tzONK2stkNYkHkeCYe0Z+rTxnOJZPyurmgItKTKOB7k/cfg+e/ftBFzqz5FVtdNrXEA8bFE4ewvayafztlBDPG5x50XREJFgV8b+Mc1JRDONm7Eue178O6uW0uujmSw0W1P2YP/Zqm/f7KE5g5frBupBLpA2IS8Gb2MPA5YKdzrl336CvgD8I5WPgbWPc6bP0AakpbLfLX1Nl8v3gGZ4Q+pGbQJMaMHMaUEZl8vL2cb5x1FAlxIcxQ8IsESKwC/gygAviLAj4Kqku9J0+98B9tzl4WOZq5DZN5KzKBj9wIwAv1s4eF+OYFJ9IvJYlROWndV14RiYqYNdGY2QjgRQV8FG1937vGfs8m2Lkaaitg49stFtkYGcSCyHFcHf86AG82TOS2umvZSjb9U8LMPmk4s08eTlF5DeOH9lMNX6QX6dEBb2ZzgDkAw4YNm7xp06aolafPqC6Fhb+FSD2seAwXisPKth5w8c2RHDKsktvrrmVxwlRumDGRCycNZeG6XdQ1OCYPz2RneTXH5enSTJGepkcHfHOqwUfRtpWQ/zSkDYb4RFa8+TcmVi064OJ/rD+Pc+KWsTkykFvcf/AF9wazR0fIufDH0P+Ibiy4iByMAl7atuFtKNtK9aBJRP5yMba3mGQO/VzZ/8u9g1PG5jEuNxMyhuJyxgBg69+EtEEweL/DHWnwnm8rIl1OT3SSto08HcB7mux3VkGkgdr6CEs2lTBw1cMMK19BVcEKFtUfxfC43YypzQfgq9vugH2jLNBWi/3G/ieTffRU0pbc60341keQ4d+UVbAEco+HeI2aKRJN0byK5nHgTCAb2AH80Dn30MHWUQ2+Z1u4fhfPv/YGkysWcMHev5PqKju0fn04nfi68n0TTr0J4pPgpOshZUAXl1akb9CNThIVDRHHE0s289Dbn7JlVwk1JHCUFTIr7m02u4FsddlMC63CiHBV3FxSrOaAn+XGfxGrKYO6vVBaCCWbYPBxsG0FXPR774lZp94AR5wMhUtg4hWgq31EFPDSPSIRx/sFJeRlJnPNH5ewaltZ07wQEeJpIIVqSkgnkzKG205OD63k3LilHGOFJFp9xzaYPRp2fQzHfdkL/FAcfPg0DJ0Moz4D/Yc1Fgy2r4TUbOg3dN+JIRKBhloIJ3XRX0Ck+yngJSacc6zeVs6YwemEQkZlTT13vPARr+Rvp6KmZZgnUstA20MIxyY3iETqODu0nB+H/0i9hdmWN5OJhY8BsCttDNkVaw5dgOGntnjQSgvjLobVL8CwaTDpKu/1jnzvBDBsGhQshtNugiGT4I07IT3Xu5t49ExvWuNJor627b4E5/QNQ7qFAl56nN2VtWwqrmTSsEz+tW4XA1ITWLi+mFfzt7N44+421xnIHorIwBEiTD1HWyHjhvbnptTXCRcsZNPgc5kU9ynhggOEeldJz4UJX/T6Debe4U074WrY/al3k9mxX4CPnvWmf28bFC72ThrxiVBdBgmpsP5NyBwJCSne64HjWp44AOproGiN1yEtcgAKeOk1IhHHyi2llFXVccYxORTs3sucR5axullzz6GMH9qPWccPpmzrGiI71hAZcRpfnzmV9TtKOTa9gsg/voU11BIaPcMb3ydtIOVjv8zOkRdzZPECrx9g+0pY9qd9HzpwHOxc1fU7vL/jvuTdmVy71wv7kk3eCWXUWZA3BVJz4K1fwrEXQ/oQrxlqx0cw4VIoLYDidXD0uZCQ5n2LCIX2ffbe3bD8zzD1eogLez/tsW2ldwWUOsJ7JAW89HqRiOPvK7bQPyVMVmoiuf2TuPv1tXy8vYzlm0s69ZnHDErj6lNGkJ4U5obH3wdg/U/Pp6a+gZSEeGioZ+/O9RTVpzD8CP/mrh0feSN9Fi6FQeMgbyqUbIY9G7ybyBJS4JNXYfgpkP8MVOyAj57bt9HUgd4yezZCYj8vvGvKWjzdq8vFJcDZP4Cda2DFo960UBhm/AwmzfZGLa3c5ZWp/3BIy/GWqSiCDQu8J5D1G+rdGT3zF943jUX3w/TbvG8Z7/4Wpv2HdwKo2gPJmd76ddXe36p8G+QeF7396ynqa6CuCpK7945vBbwEWnVdAw0RhxlU1jSweMNu5q7ewXPvbwFgRFYKpVV17Nlb1+ltPHjVZOojjuljBlLXEGHh+mLOHTeofeP2RBq88EzO9EKwrfb5xv+HpQVeEFftgZFneKOHrn3d+1axbi4MPBY+8x348ClY8yJYnPdZkQ52UB9KKAyRDv69hk6GLcu8JqWzboP3H/H6NsA7ARz1WSjb6v0tNizwTiq713vzT/p3r5P83d/CjJ/D1Dnw5k/ghKu8byPPXQ+n3OAtX13qfcPZkQ+5E2Hs572b6xrqwULet5Z5P4XSLV6TWUIqXPcaJO73fOMty7x1MvK8R2jWlEG/IfvmV+6CUPy+wG6oAwzimt0+FIns+5b0l4vg0/lwR+uRXqko8k7sCaktPz8x3Wu6OwwKeOnz6hoiVNbUs7O8htr6CJuK9/LR1lKWbNxNRU0DCXHGB4Vt/Mc8iOy0RCLOsbuyln5J8UwalsmsyXmcNTqHhojDOUhLiiccFzr0hx2usq3e5aPjLoLdG7zAb6iFF78Fn/81bHwHXv4vb9m8qTBkIix+0HufkOYNUtdc3oleAJVv94K+q08g0TLqLO9E8/b/tp437mJwDTD2IsDBs19rvczg4+CEr3h/k1du8YblnvBF2Fvs9ZXEJ8OMn0JNBbz9K6gu8fpcaith7aveZ0y5zmsyyz3eO8m8+j3vRA2QMQxO/0/vG81D53p/1yufhpwxnR4CRAEv0g7LNu3hB8/nM3l4JpU1DWSnJTCkfzIrC0t5Znlhpz5zVE4q/3nOMXy0tYwjc9Koqmvgpy+t5trTRnDD2UdTXRdh3c5yslITGZG9r3YXiThCoS6+CmfbSq/Nvq0mhIb6fTXThrrW7fO7N3jPDc460mt6Ae8EsiMfMo6Aze95l6EOmeh9A3nnbu/qoksegKf+DXasghGneeuZwfTbYcVfYd5P4OSve01fGxa0LlfyAKhqu9M9UFKy4KZ8r5bfQQp4kcPUGLgfFJRw0e/+xQXH5XLVycNZtbWMH724ilOPyuJf64oBOP3obNZsL6eo/MA3dgGMGZzOmu377uzNSA7zP7Mm8P7mEh5461P+/cwjqaypJzUxnq+feSTJ4TjiQkZNfYT4kBHfHd8MupNzXlNUfY3Xr5F7vNe89fHLXsdxTZnXZGIhr6ljb7H3u7rUa+vfttI7GWQf43M/LB0AAAtGSURBVDV7NJ5QGuq8ppq9u73P2PCW16eQ3B/GXgi/nwbTvuH1U8z/mVczt5BXE0/Ngfk/hbO+D+vnwbk/8ZqdVj4JU78KA0Z5J6+KHV5TT85oKCnwnsLWaMgkb1hvgCOne98EGuVN9a6yOuv7XtNbJyjgRbpQdV0DSeHWg6cV7N5Lv6QwGSlhnHO8vmoHTy4tpD4S4dvnjOYXr3rX7i9cX0xD5PD+36UlxvOv707nySUFnHFMDpmpYbJTE/lwSym5/ZMIh0Js2r2XiUdoiOfD4pzXoZ43pWP3NezZCAnpsHeXF/rNlW/3Op4T+3nffjYs8JqW4jo3NJgCXqSH+Wf+Nm5+eiVVtQ3UNwv7U47MYndlbYua/eG4+bzRZKUmcMqR2TgcwwaksLKwlLSkeEZkpVJZW0+/JK85ZktJFbvKazheJ4VeRQEv0gt9WFjKmNx0Vm0t4/I/vMd3zhvN44sLOHnUAJ5aVkicGbfMHMMTSzaTv6X99wkcyqLvnc2gfknU1kcoqaqltj5CbkYycX6fQEPEEXGOTcWVjMpO6/q+AukQBbxIgDnneHPNTgBG5aSxcVclW0urGJmVyltrd3H/gvVdsp0zjslh7Y5ytpXue2aAGXxz+tFkpoSJDxnTxw4iMT5EdtrhXfon7aeAF+mjnHMUVdTQLynMo+9t4vKpw6iqa+CyB95lzOB0Xv5we9OyjaG8q+LgncPt0dghPDgjiWEDUthRVs2U4Zn0T0kgJSGO4VkpLPikiFfyt/PqTWeQlZrA6m3lxMcZ764v5ppTR7R5j0FVbQOVtfU6gTSjgBeRNtU1RDBocUVOQ8SxsrCEoZnJDExPoqa+gbN/tYDCPVV8YdJQ+iWHWbuzvOmqIYDvzBjNO2t3sXB9cRtbOTxnjxnIg1+ZwpY9VUz/1XzqI46rTh7OI+9tYvzQfvz28hNaXGLa1yjgReSw7K2txzlITdx3pccrH25jYL9EhvRPJjcjGYAVBSVkpSYwOCOJ++avZ3tZNU8tLWDWCXk8vayQmRNy+ccHB34AfGfdMmMM64sqGDYghfMn5PLNx9/n4olDeO79LVw6OY/Lpw6juKKWytp6xub2A7wH2AwbkEJeZgrl1XVs3r0XwzhiQDKpCfGY0b47lWNMAS8iMVdbHyEh3vumsKKghOEDUkhNjGdrSRVvrNnJZ47JJi8zheWb9/Cjf6wiNyOJr5wygr8u2szrq3YwYWgGH27x7jaeNiqLdz/t/LeF7LQEdlXUAm3fjzAyO5VPdpRz/RlHMnpwGjPG5wJek5eZEYk4du+tJRwXYltpFbn9kslIOfDgbQ0Rx9tri/jMMTldftJQwItIr9Z4o1ljXpl5zxcoq65j7uqdDEhJYHBGIrPue7dpnVHZqcycMJjfzeuaTuZLJ3vfQg5k+piBLN6wmxnjB7O7spYjMpOZNCyT3Iwk7nr9ExZt2M39sydz5uicFvdR7Cyvpl9SuM17K9pDAS8ifdr9C9YTHzJmTsjl5ZXbSEmMIxwKMe3ILPK3lPLEkgLe37yHqSMH8M66XVTXRaJanpHZqRSV11DlD5SXl5nMgpvParoUtSMU8CIi7VRRU09SfIiQWdM1/j97eTUPvPUpqQlxVNY28OI3T2P80Ayq6xoIx4W4/fl83l5bxPghGYzITiUcMlZuKWX+x0Xt3u6aH8/oVC1eAS8i0kVKq+rISG7fw1L+tW4XtfURzhozkIaIY0dZNSkJcVTVNfDE4gLyt5QyZcQArj5luPcMgk5QwIuIBNTBAj5gw9GJiEgjBbyISEAp4EVEAkoBLyISUAp4EZGAUsCLiASUAl5EJKAU8CIiAdWjbnQysyJgUydXzwZ2dWFxegPtc9+gfQ6+w9nf4c65nLZm9KiAPxxmtvRAd3MFlfa5b9A+B1+09ldNNCIiAaWAFxEJqCAF/IOxLkAMaJ/7Bu1z8EVlfwPTBi8iIi0FqQYvIiLNKOBFRAKq1we8mc0ws4/NbJ2ZfTfW5ekqZnaEmc0zs1Vm9pGZ3ehPH2Bmr5vZWv93pj/dzOxe/++w0sxOiO0edJ6ZxZnZ+2b2ov9+pJkt8vftb2aW4E9P9N+v8+ePiGW5O8vM+pvZ02a2xsxWm9m0oB9nM/uW/+8638weN7OkoB1nM3vYzHaaWX6zaR0+rmZ2tb/8WjO7uiNl6NUBb2ZxwO+AmcA44HIzGxfbUnWZeuDbzrlxwMnAN/x9+y7whnPuaOAN/z14f4Oj/Z85wH3dX+QucyOwutn7/wHuds4dBewBrvOnXwfs8aff7S/XG/0a+KdzbgxwPN6+B/Y4m9lQ4AZginNuPBAHfJngHec/ATP2m9ah42pmA4AfAicBU4EfNp4U2sU512t/gGnAq83e3wrcGutyRWlfnwfOAT4Gcv1pucDH/usHgMubLd+0XG/6AfL8f/jTgRcBw7vDL37/Yw68CkzzX8f7y1ms96GD+5sBbNi/3EE+zsBQoAAY4B+3F4HzgnicgRFAfmePK3A58ECz6S2WO9RPr67Bs+8fSqNCf1qg+F9JJwGLgEHOuW3+rO3AIP91UP4W9wDfASL++yygxDlX779vvl9N++zPL/WX701GAkXAH/1mqf8zs1QCfJydc1uA/wU2A9vwjtsygn2cG3X0uB7W8e7tAR94ZpYGPAPc5Jwraz7Peaf0wFznamafA3Y655bFuizdKB44AbjPOTcJqGTf13YgkMc5E7gI7+Q2BEildVNG4HXHce3tAb8FOKLZ+zx/WiCYWRgv3B9zzj3rT95hZrn+/Fxgpz89CH+LU4ELzWwj8AReM82vgf5mFu8v03y/mvbZn58BFHdngbtAIVDonFvkv38aL/CDfJw/C2xwzhU55+qAZ/GOfZCPc6OOHtfDOt69PeCXAEf7ve8JeB01L8S4TF3CzAx4CFjtnLur2awXgMae9Kvx2uYbp3/F740/GSht9lWwV3DO3eqcy3POjcA7lm86564E5gGX+ovtv8+Nf4tL/eV7VU3XObcdKDCz0f6ks4FVBPg44zXNnGxmKf6/88Z9Duxxbqajx/VV4Fwzy/S/+ZzrT2ufWHdCdEEnxvnAJ8B64PuxLk8X7tdpeF/fVgIr/J/z8doe3wDWAnOBAf7yhndF0XrgQ7wrFGK+H4ex/2cCL/qvRwGLgXXAU0CiPz3Jf7/Onz8q1uXu5L5OBJb6x/rvQGbQjzNwJ7AGyAceARKDdpyBx/H6GOrwvqld15njClzr7/s64JqOlEFDFYiIBFRvb6IREZEDUMCLiASUAl5EJKAU8CIiAaWAFxEJKAW8SBcwszMbR78U6SkU8CIiAaWAlz7FzGab2WIzW2FmD/hjz1eY2d3++ORvmFmOv+xEM3vPH5/7uWZjdx9lZnPN7AMzW25mR/ofn9ZsXPfH/Ls0RWJGAS99hpmNBb4EnOqcmwg0AFfiDXa11Dl3LLAAb/xtgL8AtzjnjsO7u7Bx+mPA75xzxwOn4N2tCN6InzfhPZtgFN74KiIxE3/oRUQC42xgMrDEr1wn4w32FAH+5i/zKPCsmWUA/Z1zC/zpfwaeMrN0YKhz7jkA51w1gP95i51zhf77FXhjgb8T/d0SaZsCXvoSA/7snLu1xUSz2/dbrrPjd9Q0e92A/n9JjKmJRvqSN4BLzWwgND0fczje/4PGUQyvAN5xzpUCe8zsdH/6VcAC51w5UGhmF/ufkWhmKd26FyLtpBqG9BnOuVVmdhvwmpmF8Eb5+wbeQzam+vN24rXTgzec6/1+gH8KXONPvwp4wMx+5H/GF7txN0TaTaNJSp9nZhXOubRYl0Okq6mJRkQkoFSDFxEJKNXgRUQCSgEvIhJQCngRkYBSwIuIBJQCXkQkoP4/KeuwZPAn89cAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(cnnhistory.history['accuracy'])\n",
        "plt.plot(cnnhistory.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train','test'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "IFkTuO8nDNdq",
        "outputId": "a85d2c20-87af-476d-f3c5-7d01e5fa90e7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOydd3wV1fLAv5NOgCSQ0Huv0kUUVBBUFMGCDWz4fGJDeXZsWJ/ybD+fioK9oig2lI4PBBUUkCK9l9AJEEpIP78/dm/u3pbcQC4hufP9fPK5Z885uzs3gZ09M3NmxBiDoiiKEr5ElLYAiqIoSumiikBRFCXMUUWgKIoS5qgiUBRFCXNUESiKooQ5qggURVHCHFUESlghIh+JyHNBzt0sIn1CLZOilDaqCBRFUcIcVQSKUgYRkajSlkEpP6giUE45bJPMgyKyTESOisj7IlJDRKaIyGERmSkiVRzzB4jIChE5KCKzRaSVY6yjiPxlnzceiPO61yUissQ+93cRaRekjP1EZLGIHBKRbSLylNd4D/t6B+3xIXZ/BRF5RUS2iEi6iPxq9/UUkVQ/v4c+dvspEZkgIp+JyCFgiIh0FZF59j12isibIhLjOL+NiMwQkf0isltEHhWRmiKSISLJjnmdRGSviEQH892V8ocqAuVUZSBwPtAc6A9MAR4FqmH9u70HQESaA18A/7LHJgM/ikiM/VD8HvgUqAp8bV8X+9yOwAfAbUAyMBaYKCKxQch3FLgRSAL6AXeIyGX2dRvY8r5hy9QBWGKf9zLQGTjLlukhID/I38mlwAT7np8DecC9QApwJtAbuNOWoTIwE5gK1AaaAj8bY3YBs4GrHde9AfjSGJMTpBxKOUMVgXKq8oYxZrcxZjswF/jDGLPYGJMJfAd0tOddA0wyxsywH2QvAxWwHrTdgGjgNWNMjjFmArDAcY+hwFhjzB/GmDxjzMdAln1eoRhjZhtj/jbG5BtjlmEpo3Pt4cHATGPMF/Z904wxS0QkAvgHMNwYs92+5+/GmKwgfyfzjDHf2/c8ZoxZZIyZb4zJNcZsxlJkLhkuAXYZY14xxmQaYw4bY/6wxz4GrgcQkUhgEJayVMIUVQTKqcpuR/uYn+NKdrs2sMU1YIzJB7YBdeyx7cYzs+IWR7sBcL9tWjkoIgeBevZ5hSIiZ4jILNukkg7cjvVmjn2NDX5OS8EyTfkbC4ZtXjI0F5GfRGSXbS56PggZAH4AWotII6xVV7ox5s/jlEkpB6giUMo6O7Ae6ACIiGA9BLcDO4E6dp+L+o72NuDfxpgkx0+8MeaLIO47DpgI1DPGJAJjANd9tgFN/JyzD8gMMHYUiHd8j0gss5IT71TBbwOrgWbGmAQs05lThsb+BLdXVV9hrQpuQFcDYY8qAqWs8xXQT0R6287O+7HMO78D84Bc4B4RiRaRK4CujnPfBW633+5FRCraTuDKQdy3MrDfGJMpIl2xzEEuPgf6iMjVIhIlIski0sFerXwAvCoitUUkUkTOtH0Sa4E4+/7RwONAUb6KysAh4IiItATucIz9BNQSkX+JSKyIVBaRMxzjnwBDgAGoIgh7VBEoZRpjzBqsN9s3sN64+wP9jTHZxphs4AqsB95+LH/Ct45zFwK3Am8CB4D19txguBN4RkQOAyOxFJLruluBi7GU0n4sR3F7e/gB4G8sX8V+4D9AhDEm3b7me1irmaOARxSRHx7AUkCHsZTaeIcMh7HMPv2BXcA6oJdj/DcsJ/VfxhinuUwJQ0QL0yhKeCIi/wPGGWPeK21ZlNJFFYGihCEicjowA8vHcbi05VFKFzUNKUqYISIfY+0x+JcqAQV0RaAoihL26IpAURQlzClziatSUlJMw4YNS1sMRVGUMsWiRYv2GWO896YAZVARNGzYkIULF5a2GIqiKGUKEQkYJqymIUVRlDAnpIpARPqKyBoRWS8iI/yMNxCRn8VKNzxbROqGUh5FURTFl5ApAjtXymjgIqA1MEhEWntNexn4xBjTDngGeCFU8iiKoij+CaWPoCuw3hizEUBEvsTKp77SMac1cJ/dnoWVO77Y5OTkkJqaSmZm5gmIe+oTFxdH3bp1iY7W+iGKopQcoVQEdfBMm5sKnOE1ZylWLpj/ApcDlUUk2RiT5pwkIkOxcsdTv359vElNTaVy5co0bNgQz0ST5QdjDGlpaaSmptKoUaPSFkdRlHJEaTuLHwDOFZHFWAU1tmNVXfLAGPOOMaaLMaZLtWq+0U+ZmZkkJyeXWyUAICIkJyeX+1WPoignn1CuCLZj5YV3UdfuK8AYswNrRYCIVAIGGmMOHs/NyrMScBEO31FRlJNPKFcEC4BmItLIrh17LVYhjwJEJMUu3wfwCFaudkVRFMXBoi37mbchreiJx0nIFIExJhcYBkwDVgFfGWNWiMgzIjLAntYTWCMia4EawL9DJU8oOXjwIG+99Vaxz7v44os5ePC4FkCKooQJi7YcYODb8xj07vyQ3SOkPgJjzGRjTHNjTBNjzL/tvpHGmIl2e4Ixppk955/FKOJ9ShFIEeTm5hZ63uTJk0lKSgqVWIqinGR2ph8jM8fHzVnAn5v28+u6fR59OXn5vDd3I1m51nnGGJZvT2fljkOs33OYgW//HlKZoQymmDgVGTFiBBs2bKBDhw5ER0cTFxdHlSpVWL16NWvXruWyyy5j27ZtZGZmMnz4cIYOHQq402UcOXKEiy66iB49evD7779Tp04dfvjhBypUqFDK30xRlOJw5gv/K2h/ffuZnN6wqsf41WPnAfDNHWdRvXIstZMq8Pn8LTw3aRVj52zkk390Zd+RLG54/8+TKne5UwRP/7iClTsOleg1W9dO4Mn+bQKOjxo1iuXLl7NkyRJmz55Nv379WL58eUGY5wcffEDVqlU5duwYp59+OgMHDiQ5OdnjGuvWreOLL77g3Xff5eqrr+abb77h+uuvL9HvoShKybDj4DF+XLqDoec0LgjiOJbtuRK4asw8No/qV3DseuMH/L7l7z2cxUX/nRvwnpERgjEmJEEjpR0+Wi7p2rWrR6z/66+/Tvv27enWrRvbtm1j3bp1Puc0atSIDh06ANC5c2c2b958ssRVFKWYjPj2b16YsppGj0wGYMrfO3l79nqfedm5+SzdZvkBDxzNOaF75uUbsnLzT+gagSh3K4LC3txPFhUrVixoz549m5kzZzJv3jzi4+Pp2bOn370AsbGxBe3IyEiOHTt2UmRVFMWX9GM57Ew/xneLt/PgBS2IjBC27s9g/9FsjuXkcTjT/VDPys3jjs//8nud5o9PAeDlq9rz6bzNxZJh6DmN6dygCnWrVOCvrQd54vvlbN2fQfMalY/3awWk3CmC0qBy5cocPuy/4l96ejpVqlQhPj6e1atXM39+6Dz/iqIEz870Y4z9ZSOPXtyKmCi3cSTtSBadn5tZcNyhbhK7D2Xy1I8r/V2GM57/2eP46i51+WphqkffA18v9TkvuWIMaUezC45HXXEaI779G8DDpARQtWIML06J4s9N+1URnKokJyfTvXt32rZtS4UKFahRo0bBWN++fRkzZgytWrWiRYsWdOvWrRQlVZTwxRjDpn1HaVytEsYYXpq6hm8Xb6dzgyr0b1+bq8b8zoLNB3zOm7NuH6t3BfY7Hsxwrw7Oa1mdyIiiLe69W1bn/SGn03DEJAAWPNaHapVjCxSBN7USKzD7wZ4kV4r1O36iqCIoIcaNG+e3PzY2lilTpvgdc/kBUlJSWL58eUH/Aw88UOLyKUp5Z82uwyzZdoBrTnfnI8vMyaP3K7/wzKVtWLTlAG/N3sDke87mh6Xb+Xaxlejg7i8Wc/cXiwNed/O+o2zbX7SpdvET55NQIZqPft8MwPs3deGWjz2LaA05qyGP9WtFpO3w/enuHoz7cyvJFWOKvH6olACoIlAUpZxw1ZjfOZSZS//2tZmzdh9gyM03bD94jH9PWsWOdOthfvHrgSNz/DFvY9E7ep+5tA1V7If5TWc24MI2NaiTZIV/Dz2nMe/M2QhAtcqxREe6Vwxt6yTy/OWnFUueUKCKQFGUMsmaXYeZtWYPt5/bBIBDmdYGzgmLUhn5wwqPuRv3HQ2ZHKMHd6Jfu1oFx1GREdStEg+4bf03d2/IsHGLubJz4bW3vri1G/ExkSGTNRCqCBRFKRNs25/BvI1pXN3FymV5w/t/sOdwFoO61iftiDspgbcSKCk+uvl0hny4oOD4zcEdOatJClWDMOvUSqzAN3ecVeS8M5skFzknFOg+AkVRTjmWpR7kzf+to+GISWRk55KTl8/ZL87ioQnL6Pb8z+w46E7lMGrKas575ZegrvvcZW0ZclZD3r2xS0Hf17efybKnLgCgcpz1bpwUH83/XdOexApWEajRgzvRMNkKC+/dsjoAbWonBqUEygK6IlAUJWQs3XaQ2z5dxNR/nU1SvO9D85tFqeTm59O1UTK3f7qIzWlHaV07gcVb3ckYH/32b/7enl5wvOtQJmeNcqdy+OLPrQXtfqfVYtLfOwPKc323BgXtNwZ1ZPvBYwVpINY815dVOw9z2ejfuLdPcy7vWJfT6iTy56YDBaaf1c/2JS46ksycPOKiT74JJ1SoIlAUJWRcOvo3AH5bn1bwMP1+8Xb+NX4J9apWKIjGGXJWQ9bstvbiOJUAwPdLdgR9v4Gd63gogtvOacxY21H73Z2eppn+7Wt7HMdGRdKhXhKzHuhJw2TLxt+0emWaVnfH7bse/uVJCYCahkqE401DDfDaa6+RkZFRwhIpSumTesD973rfkSyy7fQIj35nxco7QzJdIZcnwgdDuhSYbwA61U/ikYtbFRx3rF8lqOs0SqkYdkWgVBGUAKoIFMWXRVvcm7OenLiCAW/+SnpGDhnZgdM0nwj1qsRTO8mdsdfYn3Mf6sXch3qF5J7lBTUNlQDONNTnn38+1atX56uvviIrK4vLL7+cp59+mqNHj3L11VeTmppKXl4eTzzxBLt372bHjh306tWLlJQUZs2aVdpfRVGKzT8/XsiVnevQuFolLvi/OTSpVpEtaRnUrxrvMW/1rsO0f2b6cd3jp7t7cCwnj6vGWGmcK8dGcTgrlxoJsew+lEWHekk0rV4JEeHDm0/n5g8XYGxNUM9LDsWX8qcIpoyAXf63aR83NU+Di0YFHHamoZ4+fToTJkzgzz//xBjDgAEDmDNnDnv37qV27dpMmmRtKU9PTycxMZFXX32VWbNmkZKSUrIyK8oJ8uPSHRzLySsI1/RHZk4eM1ftZuaq3QV9G/ZaMfvFjd2PiYwgO883u2bnBlVoWycRgPvOb07DlIoMsO37K3ak0/+NX3ljUMcCc05CXHSx7quE2DQkIn1FZI2IrBeREX7G64vILBFZLCLLROTiUMpzMpg+fTrTp0+nY8eOdOrUidWrV7Nu3TpOO+00ZsyYwcMPP8zcuXNJTEwsbVEVJSDGGO7+YjEPTVhGXr5h3e7DPPbd36zedQhjDGt2HWbRlgOFpmYoikcvbslXt51ZcHxlF2uz1XOXtWXisO5cYyugs5u5X5Lu6d2sQAmAFcK58YV+Hm/9sXYCufIS2nkyCNmKQEQigdHA+UAqsEBEJhpjnCn8HseqZfy2iLQGJgMNT+jGhby5nwyMMTzyyCPcdtttPmN//fUXkydP5vHHH6d3796MHDmyFCRUlKL58LfNBe0mj04uaM/fmMZNZzUskU1bt55tFXX5cMjpJMZH07pWAhe1rcnZzaoB0O7KJIad17QgVUOwtKmdwNMD2vhEBZ2SHNkLLzeFQeOhRV/PscO74JUWMPhraH5BSMUI5YqgK7DeGLPRGJMNfAlc6jXHAAl2OxEIPk7sFMKZhvrCCy/kgw8+4MiRIwBs376dPXv2sGPHDuLj47n++ut58MEH+euvv3zOVZRQ8/CEZVxjl0v0Jis3j4zsXNbtPsyMlbv9ztmw92iRSmDmfed6vMWD9fYPEBUhfHFrN76986wCU06vltXpVL8KcdGRBUrARb2q8UREFC+CR0S46ayGZWNFkGYXs5n7Mvz8rKdZe9sf1udfH4dcjFD6COoA2xzHqcAZXnOeAqaLyN1ARaBPCOUJGc401BdddBGDBw/mzDOtJW+lSpX47LPPWL9+PQ8++CARERFER0fz9ttvAzB06FD69u1L7dq11VmshJzxC63/khMWpXrkvUk7ksW5L83mSFbuCV3/sYtb0bR6JYb3bsZcR5H2Xi2q8/zk1TzWr1WppVEIKXtWQ0QkpDQL/pycTNjyq9XevxFSF8CC92DEFqsvy3qZJKYSpG2A3EyoEZrCW6XtLB4EfGSMeUVEzgQ+FZG2xhgPj5GIDAWGAtSvX9/PZUof7zTUw4cP9zhu0qQJF154oc95d999N3fffXdIZVMUsGriunjg66VERwr7j2aTkZ3HS9PWnNC1XVE8rk1jnepXoXWtBGonVaB/+1o0q1GZJSPP97u7uFzwlv2O+1R64fOcTH/MevADZNgZTl2hTgDZtrM9piK80an41y8GoVQE2wFnuEFdu8/JLUBfAGPMPBGJA1KAPc5Jxph3gHcAunTpYlAUxYeflu2gZkIcXeyUCU7uG7+kIP++i+FfLjmh+429oTNt6ySybX8GCzfv5+Xpa0mKtyJ2IiKEycPP9phfbpXA8bLPt3Y5kY5HcrZtMo6tFHJRQukjWAA0E5FGIhIDXAtM9JqzFegNICKtgDhgbwhlUpRyQ1aue2PWnsOZDBu3mBve/7OgLycvn9W7DjFn7V4fJRCISrFRPHhhC4++tnUSCtoxjlz6F7apSZ2kCnRrnMxdvZqy4fmLiY85CUaGr2+G5d8GN3fPKviwn9vMcjJZ/i1Muh9m/wfmvgp/fQLTHnOP+9u9nJEGv71utV0yR8W5xzPL2IrAGJMrIsOAaUAk8IExZoWIPAMsNMZMBO4H3hWRe7Ecx0OMMcf1xm+MKffbwo/zV6OUYSYt28nRrFxmrNrNS1e2Iyk+hi1pR3n2p1XMXLW7wNyyyY7dtzZd/c4jF7fiird+L/b9oiKFO85tUmAq2jyqH8YYGj0ymaurb2N4nxZ0H5fBOc09nboiQmSo//st/gya94UV31o/ORnQ8XprbMs8y0Zfr6vnOTOetOzwm+dCi4tCLKAXE27239+8LzQ6GyTAe/iMJ6D7PXBsv3W84nv3WEYaxJV86HlI1bcxZjJWSKizb6SjvRLofqL3iYuLIy0tjeTk5HKrDIwxpKWlERcXV/Rkpdxw17i/Ctp9Xv2FXx8+j3Nfml3Qt2nfUX5atp7kSm6zy4LNBwIqgZ/u7sElb/zqd+y8ltW5r1dDIg5v56G+Lehs5+aRnAym3tKclp8Phm9h8RN7iY8t4aRr2Uetn0rV/Y+nb4cf7oJ6jniTH+6C+mdCchP40A69HLbQcqrWtKt+xdi5h7xXBPs3QZWG7rdyY+DAZqjayPfeh3db5hnXtfLz4eBmqNoYDu2EWD/F5HMyA3/Xjy+BexZb4aGBOHYQNtjBI/sc/pvMwLWTT4TSdhaXCHXr1iU1NZW9e8u3VSkuLo66dQuvcKSUH5Zs88zCue9INte+M9+j77HvlrNyZ+EPh4oxkRy18/vUSarAgxe24KVpa2hbJ4Hl2w9RKTaK5U/bgQw/3QsLP+DOEdsgzjYJfdSPljvcG8eqhCIs851e1gMvkDM0345m2u0VupqX43n8pl1n4J4l1kPdZV/PdiiCbX/C++dD//9C5yFW3+JPYeLd8I/pUN8ruPGV5lCrPdw2x577Cfw43Jr7wQWWMvJm0n2Ffl1e71j4+H8a+O/PUkUQkOjoaBo18qPJFaUMsmjLAX5dt88jbYMLb+VQlBIAmDL8HC55Yy7GWAVX7urVlLObpZBcKZbuo/5XsBMXgA12nv/DO92KYMfx7x4Omn1eUUvTHoPti+AfU2F0N6hu7UPweKAXxusd4LoJVuil93nvn299/jjcMs90uhFS7SLzH1wA3e6Evi9Y9n1XVM/OpXBwGyTVczt5P7A3eW117MvIyYToOFjp7Q4tIXRFoCjlk31HsoiOiCAxPprcvHwGvl182z7A64M6co+flA/1k+NZ9pT9xv/b69C0N+3qtiE3L586SRV4xN7sBUCFKpaJJH0bVGvhc62g2bsG1kyBHv+CVT9afa36W32Z6VZ79gvQ6zGI9rNzeN6bjmutsn78kV/IvofPr4SON1ht75WDi4l3Q4PuEBXr7pv/lqUgXErA2R9fFaILSWL37xow8H13xE+lmnCkEBNQUcQlejqIdUWgKGUfYwxZufkehU26PDcTgM9uOYPr3//juK9dMSaSLg2qsNCR/nlQV0cEd36e5Yj8+RkYuY+oyAh+G3Ge50Wi7IfypjlQuxMcO0DQGAObf4WGPeCjfnB0L3QdCuNth+7je+GLa632/o3w+xvW2/Xg8e5r7N8IW//wvGZh5GUVPr74U+szN8vatVvZT9qJNzrBmcM8+5yKyMV8O9V868sKv+c3t1ifEVGW/+BEFEFUHOBQBJGhCcHVegSKchIZv2AbLZ+YWlC0Zc8ht1Pxxg+CVwIpDudwoxTLidmmdiKf3nIGUxzx+/ee39x9Uq59r/wAb8fOsd/+C++c697I5MT1cHY+pI2BVRMtR+iij9zO0mxHBtKNs93ttA3W59qpsNXh93i9I3x/u/u4KEVUmFPWSW4mjOlh5e7xR7AmJ4Ddy93tFoXkyWx7paUMXFSq4TnucmgXhnO1NGQynHZlcDIWE1UEihJCjmXnMX7B1oLQ3ynLrbfDp39cyXtzN9L1+Z8L5uYHePn1ztvTuFpFpt97bsHxrAd6snlUP2omxlEhJpJWtRL49s6zWDM0ieqv1LBs7RDcQzMv290+uNX/nA3/g/cvgFfd1b94vSN8daPV3rcWIu1U0Icd6cOcZg1n2yWfPw4Vsf/ho4utVUhR5NjFnwIpwUUfFX0NF678QADJTf3PiU2ES0e7FcEF/4b7VsHV9gqlTmcY+gs8uhNGbIUHN/q/jtMMFYKwURdqGlKUEPDpvM08O2kV13Spx6fzt/D27A3c3L0RFe2wyxkrdwdM7ObkzcEdubBNTX5etZvbP7NCSVvWrFxkQrVO9avAjGnWwYZZ1oMn95j/yXNftZyh4P4sjM+ucLfXTrNi9A9scvfl5bhNGBt/cffnOhTROkeBmq2ekVAeTH2kaHm+HlL0nKP7ip4D0O4aWDa+6HnJzaDfy5Ce6n88LtHaJRxhmwCrtbDa8XaepQh7LMbxoL98LHxnZy3u94oVKuv6/qddHbI8Q6CKQFFCwhN2hs5P51sJxDanZfDkxOKlbl53d12izU6IrE3ftrWY+1Avzn5xFhe2qQlAv9NqkRhfSBGWfHvncdZhWP4N1OrgHlsyDtpcDhHR8PPTxZLLg3FX+/at+tFt0pnxhLt/4j3+r1HYW//muUXLcDSIsPEVQe5ETvQKzy7M2du4J6z4zv+Ya2XlWhG49iC4jo1vAR5Ou8qtCDpcZ5mFznsc/hhrRTGFcI+UKgJFCUTGfohN8Mz/UgSHM3M8cvkHQyR53H9OLZbsE+7p3QxjILFCNNFvWAncXLH19arGs/a5i4ixwz1HX9fJiihxhSy6OLLXeoi47N6/vWZ93vSje873d8DCD6D/68WSNSgCPThNgFrFxw767y8NvJ2xUQFWXll2VJDLXBObCJWqQXwKbJvvUAReG++q2T6bs/wkmoyIhAufh2mPQqQdxVS/m/UTYlQRKIo/8vPhxUbQfjBc/nbAaakHMli18zAZ2blc2qEOz/20qiDVc7CMinqXq/6cAyMPQEThbruYKK/xUfWt6J6h9i7UFd8FNpVk7PcSfgFM+EexZA0J+zeU7PWePAhPJwUev+A5a0W0Z6XvmPfmsAY9wPwK6ba/pHYn2PGXWxEk2CuI3GNw9yJrB/MLdaCpnVG/YQ+rrkBFe8d0hSqFZxA98y7r5ySjikBR/OGyZy8dB+2uwjTuxehZ6+nXrja/rt9H61qV6dygKj3+464h4Z3N852YV1mY14x38vr7vUUb2cSkWEcSsldaWOGQmemFv6nvWQUfD4CjdpLeHX9ZDtxPLy/8Ox3b79sXKD4/EKddbaV0mP1C8OdcOw6+HFy8+wRLVAXrIXz2A1ZxF7BMKPethldbBjgnDv4xzdp05kr/7KKx2wnPjRMtO31upuU7iUu0fl7vADl2NFRiHeuzop17KbYSDFvkNjH1eswy+aQEcCqfIqgiUMKPg9tg9STodrtH913j/mJgpzr0alGdBeu240pflj/5IVZe8TMvT1/LB79tZv9Rd2RNLNmMif4/fsjrzvf5PTyud0HEQi6IWMic/PasNu46GpHk8XjUZ9wcNc1TrqOO7Os/+rGnzx8DLfvBvNGec6FoJQC+K4KiSGluRQA5ObAJOt1QvOvU9hOCGohud1n289od4buhVl98CmTYzt4r3oNv/+me73KAd7vDrQgAEmrBlR9a+YSO7IEvrnGPRcVau6artXIXhnFyxzwrlYVLKUTHudu52Z5zYyrCgDesTWkunA/9iEio3opTHQ0fVcKPcVfD1Ie58qXv2LXuL8g+SnpGDpOW7eQfHy1k5vxFPPWZO6olIm0d170xFcBWAoY2YkXJ3Bg5nV6RS3ktxtps1EB2kcARpt7evuD8qbEjAGgrG2lXJ4Eld9T3VQKFcewALB0PUx+GcdcUvrO1MIIJs3TS/7/udo97rc/zn4XEev7nByK6gpXGoTDqnm4lces+3Ko73t7x4HaZWQDaXeVuXzsOLn8HWl8KFVOs1cqFjpVK2yugTierFnDnm+H0W61wT1fs/wXPQopjX0G/V6zPGq097+MkKsZyEl/xrruv043WKqkMoysCJfywI1oO7d9Nzc+HkNHkYjb1dPsBzp/Wm/NjPU+ZE/sv2mdZKQduiJzBs9EfMTj7UaJxpziIIYdfYu8jIyaZ+OivPc7/sPEv9NoxFtP1JSS/mG+IP93njnrJPBjYgVkUGwOUQm10Lmz6xbc/yZH47LyR0Ocpq51r7+aNT/Y1rfgjOt56a96zGlL/tGzkT3nFxHe5BToM8n9+gzNh2ZdWaKeTlv2sT5fSGPguAen/mm9fnU4w7E+rvsGKb+H0f/rO8WVy9BYAACAASURBVMeNPwQ3rwyhikAp3zyVaJkb+j4PWMVcYu088J0jLLNH/IbJZG+6FHgo4GUSJaOg3VIsZ3Bj2UkW7vDNKyKtUMf47DR41zN1Q68dYwGQKQ/CNZ8V7ztsX+huH9pupWYoSZyRLd2HW7uKwbJ/P7jRjod3GA+iYq1NUJt/tWz/Z9xu7QvYb2+KeiTVytXzvJ3OwbW5bMgkzw1rYNnWb5kOVfwkjXxsl2XOSqhthbpGVyyZ7+vNFe96rn7CEFUESrlh8dYDpOydT72MlXD2/e6B+aN5cG4O8V1v4uN5W5gXl0UtoJm449e75i+hT8QiRkcXHU6ZZ1tUn4v+kI9yLyjoHxX9XqBTPFn6pf/+23+DMX7KcwTa4Que9vPjJcex0axCFc+xigEKzcclWiaWge9DqwHWxjKwzCve+fld8e9RMb6rmdvmWA96f0RXcDtjnbtq7/4r+A1iwRAZBZEJRc8rx6giUMoHG2Yx5N09LI2zHYzd7/XYgPNS9Ds0nNcTgNx8gQhoJDs9LvFezCuF3uKBC5ozdcZUboyaUdA3JGp6IWcEYPVPvn13LbBizG+bC2PtXEF1u1qmFH/0edpy5CY1gNnPB75X3//A+pmw3i0znW6EM+6w3uDT1sPSL9xjEmn5AfwVaPFGxJ37xpUFtOn57vHrvrFi6v3K/5Rlnw+kBAojuUmZt8mfaqgiUMoe2UfdOzXBiun+9DLGxzicmIe2u0P6bBrJTvaaxII3+tYRW4p12471qzAs9vHjFjsgEdHujUa12rn7W/T1rwicxVN2LLFDOf0kKrr2C2h5MZxxm2dc/QDbtFSjtfX5m8N+HhF5fHHsZ94FU0d4/s6b9bF+/OFyPiunBCGNGhKRviKyRkTWi8gIP+P/JyJL7J+1InIKbTFUTklSF1q257XTSc/I4dyXZpH11jkAtIxwbOR6ra1Pbp1ZsffzQ8wT5GOtFGrIQbJN8CUX29Q+TvOBM8omMtZ33Bm77iQiwHuaUwnW7mDZ0v3hMtEUlZqgtqNaVqA6ukXR7Q7LCRxznBFNSqkSMkUgIpHAaOAioDUwSERaO+cYY+41xnQwxnQA3gCCTAiinFJ8dWNgu3ewrPje/47Y9TPhk0utnb5zXob3elv9W35j3sZ9ND8wh9h0/5kbJ/zhu2O1ScROGke4H5x5Ndv7zAlE0o+3BD2XlOZw70orJj3Kkf6hTmffuVd97P8arlxBba7w7I/xcppGx8G9fvIYOR/K9xRSZeyqj90hmk5ZlbAhlCuCrsB6Y8xGY0w28CVwaSHzBwFfFDKunKqs/MGdLCsQB7fCEq8/766/YZVtL//6Jis9woL3rbm/vwnZGfDl9VYe++wj8L9n3edWSKJCxnbejXk14C0PzXypSNEr1G5d5JwCVhWj/GCH6yxHZw3H9as2gSs/8J3rqqvr4or3rAibtldYNXX7POU5HuM1H3yTpYGntahqY8sk9M//+c6LS4CrPrIc7B2u8/t1lPJNKH0EdQBn0pVU4Ax/E0WkAdAI8POvFERkKDAUoH79+v6mKCWBMZYDMRSOuHHXwp4V1oaeGm2st9Ux9k5cZ+4VZ9HvjbPd5p1cz0pUWVGV2TJxVKH/gv8RNdXjOMdEEi1eic+qnsB3jYgOnN++7UB325Vpsucj1o7XdtdaIZgVU6x8P944NzO5whpjE9w5/L0Vh4smvWGDXd+gQhW338FFYZu6YitD75GBx5Vyzamys/haYIIx/tMTGmPeMcZ0McZ0qVatmr8pSkkwb7RVkSqYnPQuiiol6MKV5+b9PvC5n12bFar69jkjXX4Z5TE0YeE2kqQYVaWA3VTx7azS0P/kqz6GmMr+x1w4d7/W6eJu3znfKnJegP07cr21XzEWBrxuPXidGUELw1ndyl+NX4AbHJbVhzf7hnEqSgBCuSLYDjj/N9S1+/xxLXDyU+4pnmydZ30e2Ay1grSd5wdILexNpCNvvnd+lwXvFe3Q/OsTj8Pr9r0Gwft5AahdrxGkesWfpzSzPivXgsN2OGnvJ63Y+GmPuouQJ9V3x/MPX2opibgEOOdBy1QTEQX/sXfieptuqja2VlqVvUoVFoda7SBt3fGfryiFEEpFsABoJiKNsBTAtYBPCkIRaQlUAeaFUBYlGAre7oMsgLF3Lfzlx9G5fqa16zQiCrrdaTl5vTdF5blTMzDpft+NTN5470i12W8qUTXIlUGEl3mJaz636sZeNsZylr5sJwureZq1k9b5Rl21ifs7OFcR/lYU3s7cy9+xVjdVGwclp1/6v24VlymKuxZAevHSYCtKyBSBMSZXRIYB07De3T4wxqwQkWeAhcYYl+ftWuBLY4K1MSihw/4TBBtCOOk+/xWkPnPYx/eu8e9k3fCz53FRRcoDMD6vF3dEBWle6XKz5Zx23bvVJdZnoBw3V7zr3tzlssv7c9R64z2nYjK0vzY4GQMRW8lKqFZY4Xmw/ALevgFFKYKQ+giMMZONMc2NMU2MMf+2+0Y6lADGmKeMMT57DJRSwKWLA5lp8vOtOa558V7pBzbO9vUZBHrA+ytxWASv5Fzp07fbVOHObDtlc1vfcQ+qNLLs6BWrQ812vuOXjbE+q9kZKWu1s/IUgTussjAl2dWOnDrepHBFceadVi4gRSlhThVnsXJKYD/EXQ/zxZ9bSdsO2bbzt7pZO1SfToJ1M31jztNTfStD5WRQUqQa30CBwyaeyfndaJz5mRVuCXD+M56TXLH7LifrfatgqJ9smx0Gwcj9lj/ARZS9AUxsh0RhvoyL/mOdryhlDE0xobhxKYA825a+2M6SmbbOCnvct8Y9d81k37fjuX5y9WSXnCJYZhpzXfYjfB7jzjl/lDjWPNeX7Nx8iI2CwV9ZYZQtLrYyV0ZEWakPwK0ICqtB7F1j1nslUNiKQMStMBSlDKErgnAkYz8s88yXz6Y5sHe11XY5VV1vvxt/scojOklb75tQbL+fHb7ZRwsV5du8HoWOO8k0MfyWfxqZ57jz/WQQS2xUJJXjoi15m19oPehTmln5eOp2tvLUdx8ONU4L+l4FuFYErr0Ax5uCQVFOYXRFEI58epm1V6BxT6hkm1s+dtTV3b3CfoDbimDuy55lAMF/IRN/5LgVQZaJIlZyPYZn57XnisjgKmc9f/UZtG/ZhLj4frDmB9j9Nzf1bFP0iVUa+pqLgsW1Ioi2P/s8dXzXUZRTGH29CUdcG8ZcO1W9mfcmvNi46Nj+wnBll8xyh3YexdOn8FruFfxlgo9wObdtfZLibUesbd7p3eo40hgXB+eK4Kn0oksuKkoZRBVBeWTbnzCqgf9i5amOalezR1kJ3fyRm+k/NDRYEuyCInnu2P3dxnP38H9zr2C7CVD4BBiQ9Sw9sxx+hyjHjlrX9SNC/E/Ytb/heOsEK0oZQE1D5ZG5r1i1bbfOc9d1TdtglROc6ojU/fsr63NO0cnZio3XBrF8Ixjb1LS++S08sbwmpoj3kJWmAbnOf6LOh/6AN6Dh2VC7U4mJ7JdWA+CCf1vJ3xSlnKIrgvKIK4+9q2rU4d1WDqGpAbZr/O+5Ehfhj135Hse35dxLurF23GbV6sq8fLdtP6/rHXCur2y5rhwSl42B5hd5DsZXhW63n5j5KhgiIuCsYYETvSlKOUAVQXnEFQLpUgSvnJydptmPuzePPfe/3QXtflnPMyO/Cx/nWfV98yvV9Dgv8uJRcO7D7o52rl249kO+wyAYfIL1DhRFCYgqgvKIa0Uw4R+wenLxz3dm0iwGT050F0fZ6bD9H8Ky7U/N78ornWbStsu5jLiopefJTrPPZW9hHrHyEw495wTy8yiKEhSqCMoyu1fClBGeaR02zfFMTvZlgDw6hRHvJyV0APLFbcP/YYk7uWwa7oRth43b0XqYeESE288tpA5ARCQSW4nNo/rx6MWtgpZFUZTjQ53FZZnPBsLhHVbytLQN0PEGz/0Ax0vOsaLnAONze/JTxcvoXSuH7esWk5GXhytC1OkIrl29OnWiolmx4xDZeW7fwbOXtWXBJkdk07kPQ+NeJy6/oijFQhVBWcblA/juDkjfCnWCjKDp96pnJTBvDmwO6jIP594K6cLcdLAK0sGnuX24IWomAPPzW9El6SiT7zuPrxZs46FvlpGT61YEN3RrwA3dGrgv2OvR4ORXFKVEUdNQWcaV9iDdzpMfbF6fqFio183/WPU2kOeb6vhohGXqaZg5jm35ruRvvhE7T+T+g4aZ46gcF8Wg7MfIvtPat9CjWQoAg87QUqOKcqqhK4Kyys6lkOFVbauwt3wnkTFw/QQrW+hbtkK45nNIaQ6VqsMbnX1OGVn/I168pDG8vIpLsv9NVTlc6C0+/+cZNK9RmbhoK4KpdlIFNo/qF5x8iqKcVFQRlFUmPeDbt3t5cOdGRlvVt6o7HLHNznenU7BNTguTB9AlzSod8c3qLL5ZbSWeS6cS6cZ/XH3VijEseKwPkREhju9XFKXEUEVQlsjLhQObrHQHxSlHWK8b7FjsTvfgLNl43uOwb71bCUBBHeKvYy5lS94Bsoyj3nBRt6oar0pAUcoYqgjKEtMfgz/GFP+8s4ZZOfqfr2UdZznMOuc86Dvf5YSOqsD9OXcEfZt/9mjETWc1LL58iqKUKiF1FotIXxFZIyLrRcRvfgMRuVpEVorIChEZF0p5yhQzRsLLLTz7Vv5Q/Ovctxpa9XcXZQHfymI26cdy+Pj3zRhbEUTEVPA7D+Cc5tXo185SLPExkdzbpzmPX9KaelU1OZuilDVCtiIQkUhgNHA+kAosEJGJxpiVjjnNgEeA7saYAyJSPVTylDl++6/n8cZf4PDO4l8nwV4FOHPydBjsd+qtnyzkz037uSE2FxH4cUUa4Plgv7l7Qz78bTMXtqnBwE51+VfvZjSrUdnv9RRFKRuE0jTUFVhvjNkIICJfApcCKx1zbgVGG2MOABhj9oRQnrLLkT3wyYCSu553OUabJVsPWsNi7VTOxLMI+5ThZ9OqVgKDutanWfVKiIgqAUUpB4TSNFQHcHo0U3HtOnLTHGguIr+JyHwR6evvQiIyVEQWisjCvXv3hkjcU5ifi6iuVf9M//0JdX37mvbxOFy7+zAX/N8vNBwxqWDX73u5VqbPguyfNq1qJQDQvEZlJNRZPxVFOWmU9oayKKAZ0BMYBLwrIknek4wx7xhjuhhjulSrVs17uHxjDGSmFz4nMtqqntXKkV6i8xC4b4XnvCcPwnUTCg4PZebwy5q9rN19xGPac7k30DDzc85rWYPaif79CYqilB9CaRraDtRzHNe1+5ykAn8YY3KATSKyFksxLAihXGWLvJyik8BFW3n+PZLPia/553BWLtGREURHRvDi1NWMneOn2LxNjYQ4PhhyOgC/r99HcqXYgHMVRSnbBLUiEJFvRaSfiBRnBbEAaCYijUQkBrgWmOg153us1QAikoJlKgr8dApH8rIL4vr9ctbd0N92LO9Y4u6P8NXxpz01navGzGNp6sFClQDAmOvdu4vPappCi5rqC1CU8kqwD/a3gMHAOhEZJSItijrBGJMLDAOmAauAr4wxK0TkGRFxeT6nAWkishKYBTxojEkr9rcoL+xdC0e9vn5etjuu3x8XPAeVa1htZ07/AA7hv7enE+nHvt++XhKvXNWextWs1UWFGP/nK4pS/gjKNGSMmQnMFJFELFv+TBHZBrwLfGabdvydNxmY7NU30tE2wH32T3iz5Xf40C7HeMsMd39ejqUMXFRpZO0u9ocz6ZzX4i0zx72qyMr1LCMJ8MNd3QEYPWs9AFG6O1hRwoagTT0ikgwMAf4JLAb+C3QCZhRymhIsRxyRs7uWudt52Z7ZQBucBcMWWe24RM9rVExxtx0rgpy8fHalZxYcXz12XkAxaiVZzuGYSF0RKEq4ENSKQES+A1oAnwL9jTGunU3jRWRhqIQLK5xv8HkOU9DK72GVw7WSWA9SmsLA933rD1w3AT69DNLWe/gI+r/xK6t3+WYLHXtDZ277dJFH3+vXdmTWmr3UT9YdwooSLgQbNfS6MWaWvwFjzPEVuFU8cVYFc5qCpj9ufVaoamUL7XqrdXzalb7XSKoHp10Fs1/wiBrypwQA6iT5ppBIrhTLlZ397D9QFKXcEqxpqLUzvl9EqojInSGSKTzJcdj3vesMACTUgZsne5p//OGKMArgLHZSvbIVElqvauCcQoqilH+CVQS3GmMOug7slBC3hkakMMW5IvDOMwTuFNJFYSxFkEcExrmvwIthvZpSrXIsn91yBt/d2b04kiqKUs4I1jQUKSJiR/m4EsrFFHGOEiw7l8K0RwqfkxukIrB5ecYG4nLXc9NZDXzGXrmqPQNt84+rhKSiKOFLsIpgKpZjeKx9fJvdp5QEY88peo6fOsL+GJN7CVVyF/FJ3vkcnbmWFTus9BSXdajNsZw8pq3YzZGsQvYlKIoSdgSrCB7Gevi7qpTMAN4LiUThQnaGZcffudTPoEBMJch2OHmLMA3l5uXz8vS1jPllJzC0oH/6yt0AjBrYjolLdzBtxW5qJGj+IEVR3AS7oSwfeNv+UUoCV7Uwf7S+FK7+GJ5y7BMoZHfx7xv2MfjdPwq9XVx0JFd1rkvT6pXoWM8nr5+iKGFMsPsImgEvAK2BgtdJY0zjEMkVvtzxu7V7GODGie46BLU7BTzl1elrC73kIxe1BEBE6FS/SomIqShK+SFY09CHwJPA/wG9gJsp/RTW5ZMabdztBEf5hoHv+50+Z+1eFm454NN/Z88mdG1UlZ4ttOiboiiFE+zDvIIx5mdAjDFbjDFPAf1CJ5YCWBvEqjSCwV9BxWS/U/4zdbXH8QWtrQR0leKiVAkoihIUwa4IsuwU1OtEZBhWXYFKoRNLASAqFoYvCTicn29YseOQR9/I/q3ZkpbBwE66O1hRlOAIVhEMx6pifg/wLJZ56KZQCVXuWfb1CV/ihSmr+GzeFo++pPho6laJZ9q9QYSjKoqi2BSpCOzNY9cYYx4AjmD5B5TjYcH7UKUh/PbaCV9q7C++hWUm3B6gdrGiKEohFKkIjDF5ItLjZAhT7plkl13wFwF03uNQtUmRl8jPN4hAZISQl++ZQqJpda0ipihK8QnWNLRYRCYCXwNHXZ3GmG9DIlV5Z8dfvn3nPBjUqR2fncGx7DwfJVC1omb8UBTl+AhWEcQBacB5jj4DqCI4STz94wp6NE0h/Zhvqokfh/WgZqLuFlYU5fgIdmfxcfkFRKQvViWzSOA9Y8wor/EhwEtYUUgAbxpjNHWFFwczsvnwt818+Ntmn7H+7WtzWt1E35MURVGCJNidxR9irQA8MMb8o5BzIoHRwPlAKrBARCYaY1Z6TR1vjBkWvMhllPy8wGNXFK771u054rd/eO9m3Ht+8xORSlEUJWjT0E+OdhxwObCjiHO6AuuNMRsBRORL4FLAWxGEB7mZgcfaXRVwyBjDG/9b79PfpFpFbjtXM3woinLiBGsa+sZ5LCJfAL8WcVodYJvjOBU4w8+8gSJyDrAWuNcYs817gogMxU6pWb9+/WBEPvXIKUQRFMIfm/YzZ+1en/5Xru5AfEywelxRFCUwx5svqBlQEvkLfgQaGmPaYaW2/tjfJGPMO8aYLsaYLtWqVSuB25YCha0IAjB+wVZGz/JdDYC7zKSiKMqJEqyP4DCePoJdWDUKCmM7UM9xXBe3UxgAY0ya4/A94MVg5CmT7Frmv/+yMX67jTE8/M3fAS+XXEnDRRVFKRmCNQ0dz06lBUAzEWmEpQCuBQY7J4hILWPMTvtwALDqOO5TNti13H9/q0s8DvPzDS9OW8Ml7XzrFfxwV3dqJMSx70gWsVFFF6dXFEUJhmBXBJcD/zPGpNvHSUBPY8z3gc4xxuTaCeqmYYWPfmCMWSEizwALjTETgXtEZACQC+wHhpzQtzmVCWQaioj2OFy58xBjftnAmF82+ExtbxeU0T0DiqKUJMF6G580xnznOjDGHBSRJ4GAisCeNxmY7NU30tF+BCiiansZZ8ItsHwCnBkgQjbC80/w4zL/wVjf39W9pCVTFEUBgncW+5unISuFkZcD391uKQGAeW/6nxfhaeLxl0wOoJo6hxVFCRHBPswXisirWBvEAO4CFoVGpHLC1vmw9IvC55w7AkQAGPLhn2RkBd50FhelBeEURQkNwT5d7gaygfHAl0AmljJQToRellVs2/4MZq/Zy5+b93sMd6zvLjJfIUadw4qihIZgo4aOAiNCLEs5wycjR0BSDxzz6ftXn2ZUqxzL4q0H+enuHrp5TFGUkBFs1NAM4CpjzEH7uArwpTHmwlAKFy7sOewbUTSsV1MiI4RLO9ShUqwqAUVRQkewpqEUlxIAMMYcoGR2Fpdf8nODnror3VMRVK0YQ1RkBCKiSkBRlJATrCLIF5GCJD8i0pDi2D7Ckdxs377u//I4/G5xKs/8uJIXpqwmxuEMvqB1jVBLpyiKUkCwr5uPAb+KyC+AAGdjJ4FTAuC9gey8x60qZI56xfeOX1rQ7lA3iScHtObv1HQGdq57sqRUFEUJ2lk8VUS6YD38F2NtJPP1cCoWuVnw9U1enVLoKZm5ebSpnUib2lpkRlGUk0uwzuJ/AsOxEsctAboB8/AsXam4OLLbt088rXD9sv7tcZydmx9KiRRFUQISrGloOHA6MN8Y00tEWgLPh06sMk5mum+fRHDXuL9om9ufX/PbssI08hjOzlNFoChK6RCsIsg0xmSKCCISa4xZLSItQipZWSI/z4oSioq1CtBkpPnOEWHSsp1MYpDfS/RsrkFYiqKUDsEqglQ74+j3wAwROQBsCZ1YZYwvr4O1U+CuBTD6dGh2gXus623w51gf05CTXx/uRY0EzSiqKErpEKyz+HK7+ZSIzAISgakhk6qssXaK9blzifW5brp7zFYA+V6RuvWrxjNxWHeMgSoVtciMoiilR7F3KxljfgmFIOUCf05iY9n+s/I8t110b5pCUrwqAEVRSh9NaVkSRNopoves9h2zFcEfmw54dLeseTxF3xRFUUoeVQQlQfWW1ufOpX4GrZXAz2v2efTe0K1BiIVSFEUJjpAqAhHpKyJrRGS9iATMXioiA0XE2JvWyi5H9/r2VagKwCET79EdEVH4BjNFUZSTRcgUgYhEYhWyuQhoDQwSkdZ+5lXG2qfwR6hkCTk59iZrfz6Ccx7gtbg7mJh/luYQUhTllCSUK4KuwHpjzEZjTDZWQZtL/cx7FvgPVrGbssHRNNi13GrvXQtp6+0B3zx8M9Ye5LWDZ2OIIP1YDn3b1GT04E4nT1ZFUZQiCKUiqANscxyn2n0FiEgnoJ4xZlII5Sh53u0FY+xi8qNPL3AIe3PQVOTWTxYWHB/OzGXMDZ3p167WyZBSURQlKErNWSwiEcCrwP1BzB0qIgtFZOHevX7s8Cebg/ZeuqcCJ4hb0PEFOmWN9ej7V59moZRKURTluAilItgO1HMc17X7XFQG2gKzRWQzViK7if4cxsaYd4wxXYwxXapVqxZCkY+TyBg4626Prn3Z0R6byJY9dQEXtKl5siVTFEUpklCWv1oANBORRlgK4FpgsGvQGJMOpLiORWQ28IAxZiGnKpnpsG6Gb3/nm6FOZ4+ug7nRHscJcZ7HiqIopwohWxEYY3KBYcA0YBXwlTFmhYg8IyIDQnXfEyIn03/mUBeT7odvbvHtj46DBM9iMhOW7S9h4RRFUUJDSAviGmMmA5O9+kYGmNszlLIExXu9YfdyeCqAMjiyx39/VAVI9PCDk0lsCQunKIoSGrQyupPdywsfjw2QFiIqFhJq89PZP3DJXCtC9hgxdKqfxJWd69GiZqUSFlRRFKXkUEXgj7QNsHEWnP5Pz/5AiiC6AgBr89xhocdMLM9e0ILuTVP8n6MoinKKoIrAHx9caKWLaNAdEupAXILVbz/wfYiyagkcyswt6DpGDJVi9derKMqpjyad84erwthb3eDLwe7+CMeDvUrDguaU1Qe5b/wSZqx0p5g4Rizt6yWFWFBFUZQTR19Z/WEcqSK2L3K3nVXGYtxmovErM5idb2+RsAuNzX20bwgFVBRFKTl0ReAXhyJIcEQD5WW72zEVC5r7ja/voHpCADOSoijKKYauCIoizpFGwqEI9mRH4yo3n45bKfzQfiznxKyiykkST1EU5URRRVAUTnNQrlsRLN2RwfmRVnuvcfsCLrn0GiK11oCiKGUINQ0VhVMR5GUVNPOxHvZZJpoMl2MAVAkoilLmUEVQFOJ4sOfluJv2ry6HyIK+Yb2anjSxFEVRSgo1DRWJQF4u/DkWsg4X9EZi1SDIdSiCBsnxPmcriqKc6qgiKAoRWPwJTHvUo/sIFdhrEnkm54aCvta1E062dIqiKCeMKoKikAjI8M0kmm+E07PeLjhe8FgfqlXWRHOKopQ91EdQFCKQn+vTHSGe5SmrxGu9AUVRyiaqCIpi0xzYu9qnOwJD+7qJuIKEoiL1V6koStlETUPBsOK7gubPeR2JIo9Xcq/i12E92LY/g2WphRSzURRFOcVRRVBMWkZspXvWGwXH9arGU6+qRgspilJ2UXtGMYnF2kvwy4M9S1cQRVGUEiKkikBE+orIGhFZLyIj/IzfLiJ/i8gSEflVRFqHUp6SIMdeRNVK1KRyiqKUD0KmCEQkEhgNXAS0Bgb5edCPM8acZozpALwIvBoqeYpk829BTYvFyjcUE6WLKUVRygehfJp1BdYbYzYaY7KBL4FLnROMMYcchxXxyP98kvno4qCmxZJD42oVi56oKIpSRgils7gOsM1xnAqc4T1JRO4C7gNigPP8XUhEhgJDAerXr1/ighaHOfntiNLEcoqilCNK3b5hjBltjGkCPAw8HmDOO8aYLsaYLtWqVTu5Anoxvv5IXr6qfanKoCiKUpKEckWwHajnOK5r9wXiS+DtQsZPCT669ZzSFkFRFKVECeWKYAHQTEQaiUgMEGCorwAACvlJREFUcC0w0TlBRJo5DvsB60IoT2Byjrnb5/oENymKopRrQqYIjDG5wDBgGrAK+MoYs0JEnhGRAfa0YSKyQkSWYPkJbgqVPIXy3W3udlwC3PBd4LmKoijljJDuLDbGTAYme/WNdLSHh/L+RbJjCbxzrmdfVJxnVTJFUZRyTng/8bb94dunikBRlDAjvJ94Far69kUHUAS1OsCNE337FUVRyjjhnXQuJ8O3L6oCGM9aA2k3ziK5caeTJJSiKMrJJbxXBM5oIZtsieb+Lxd69FWtpNlFFUUpv4TniiA7Az68CBJq+wylZUVy8PARa5+zjURq9TFFUcov4aUIcrPhj7ehTmfYucT68Z4iMUST59mpikBRlHJMeCmCBe/BjJHQoHvAKb9sPEyMXXOggAhVBIqilF/CSxEc2299bgmccvqd+TtJoJZnp64IFEUpx4SXszgvp8gpWSaa5aYxp2eOdndGhJe+VBQlvAgvRZCfW+SUTNtL/Og1Pd2dqggURSnHhJciCLQiaNCjoOlSBC1rJrjH1TSkKEo5JrwUgXFEA1VvU9Dc1eaWgnaleGvPgEcpSnUWK4pSjgkvReAkxl1u8s1ZGwraSRVjAcjNc1TNjAjfX5OiKOWf8HrC5Wa527GVCprb0i2T0bb8aoy+rhOXdahNE61LrChKmBBeXlCnIohxK4K5+acxJvcSNje/mVE1E3jt2o6lIJyiKErpEF6KIM+/IsgnglG5g/l9QOCNZoqiKOWV8DUNRcX4DNdOqnAShVEURTk1CF9F0Pqy0pNDURTlFCKkikBE+orIGhFZLyI+VeFF5D4RWSkiy0TkZxFpEDJhcrNh46yCwwHv/e0xPP+R3iG7taIoyqlMyBSBiEQCo4GLgNbAIBFp7TVtMdDFGNMOmAC8GCp5WPmDx2EW7r0BF7WtSc3EuJDdWlEU5VQmlCuCrsB6Y8xGY0w28CVwqXOCMWaWMcZVJmw+UDdk0uxb43HoVASvXt3B/zk1TguZOIqiKKcKoVQEdYBtjuNUuy8QtwBT/A2IyFARWSgiC/fu3Xt80pzzkMdhlnE7iyvERPo/Z+hsePw476coilJGOCWcxSJyPdAFeMnfuDHmHWNMF2NMl2rVqh3fTaJieK/VhwWH+ZG+UUM+REb5jS5SFEUpT4RSEWwH6jmO69p9HohIH+AxYIAxJst7vKSYtyGN5xbHFhz/8XjfUN1KURSlTBFKRbAAaCYijUQkBrgWmOicICIdgbFYSmBPCGVh5c5Dnh2aWlpRFAUIoSIwxuQCw4BpwCrgK2PMChF5RkQG2NNeAioBX4vIEhGZGOByJ0xKJS8TjyoCRVEUIMQpJowxk4HJXn0jHe0+oby/k6oVVREoiqL445RwFp8MKsZ6PfgjAkQKKYqihBlhowg61kvi+csd+wJESk8YRVGUU4iwUQQiwuAz6vsO1AqwmUxRFCVMCG9D+YitEKWpJRRFCW/CWxHEJZa2BIqiKKVO2JiGFEVRFP+oIlAURQlzVBEoiqKEOaoIFEVRwpzwcxbfPAX2byptKRRFUU4Zwk8RNDjL+lEURVEANQ0piqKEPaoIFEVRwhxVBIqiKGGOKgJFUZQwRxWBoihKmKOKQFEUJcxRRaAoihLmqCJQFEUJc8QYU9oyFAsR2QtsOc7TU4B9JShOWUC/c3ig3zk8OJHv3MAYU83fQJlTBCeCiCw0xnQpbTlOJvqdwwP9zuFBqL6zmoYURVHCHFUEiqIoYU64KYJ3SluAUkC/c3ig3zk8CMl3DisfgaIoiuJLuK0IFEVRFC9UESiKooQ5YaMIRKSviKwRkfUiMqK05SkpRKSeiMwSkZUiskJEhtv9VUVkhoissz+r2P0iIq/bv4dlItKpdL/B8SEikSKyWER+so8bicgf9vcaLyIxdn+sfbzeHm9YmnIfLyKSJCITRGS1iKwSkTPD4G98r/1vermIfCEiceXx7ywiH4jIHhFZ7ugr9t9WRG6y568TkZuKI0NYKAIRiQRGAxcBrYFBItK6dKUqMXKB+40xrYFuwF32dxsB/GyMaQb8bB+D9TtoZv8MBd4++SKXCMOBVY7j/wD/Z4xpChwAbrH7bwEO2P3/Z88ri/wXmGqMaQm0x/ru5fZvLCJ1gHuALsaYtkAkcC3l8+/8EdDXq69Yf1sRqQo8CZwBdAWedCmPoDDGlPsf4ExgmuP4EeCR0pYrRN/1B+B8YA1Qy+6rBayx22OBQY75BfPKyg9Q1/7PcR7wEyBYuy2jvP/ewDTgTLsdZc+T0v4Oxfy+icAmb7nL+d+4DrANqGr/3X4CLiyvf2egIbD8eP+2wCBgrKPfY15RP2GxIsD9j8pFqt1XrrCXwx2BP4Aaxpid9tAuoIbdLg+/i9eAh4B8+zgZOGiMybWPnd+p4Pva4+n2/LJEI+D/27u/ECvKMI7j319sbemGGhRYRrEVEYFtBSFZIBheSFQXG0FmYV12412IdVHXUXQRJRRhtVRYa0g3hVsseFGrxvYHi1otbKVaibAMCrGni/c5Np012l23c3Tm94EDZ955GeY9z559Zt6Z88xh4KWcDntB0kJqHOOIOAQ8CRwEvqfEbS/1jnPVbGN7SjFvSiKoPUl9wFvAxoj4pbouyiFCLe4TlnQ7MBURe7u9Lx3UA9wAPBcR1wO/8fdUAVCvGAPktMadlCR4MbCQ6dMnjdCJ2DYlERwCLq0sL8u2WpB0NiUJDEXEcDb/KGlprl8KTGX7mf5ZrATukPQt8DpleugZYLGknuxTHdOJ8eb6RcBPndzheTAJTEbER7n8JiUx1DXGALcB30TE4Yg4BgxTYl/nOFfNNranFPOmJILdwFV5x8E5lItOO7q8T/NCkoAXgS8i4qnKqh1A686BByjXDlrt9+fdByuAI5VT0NNeRGyKiGURcTklju9HxDrgA2Awu7WPt/U5DGb/M+rIOSJ+AL6TdHU2rQb2UdMYp4PACkkL8m+8NebaxrnNbGP7LrBG0pI8m1qTbTPT7YskHbwYsxb4CtgPbO72/szjuG6hnDZ+Coznay1lfnQE+BrYCVyQ/UW5g2o/8Bnlroyuj2OOY18FvJPv+4ExYALYBvRm+7m5PJHr+7u933Mc6wCwJ+P8NrCk7jEGHge+BD4HXgF66xhn4DXKdZBjlLO/h+YSW+DBHP8EsGE2++ASE2ZmDdeUqSEzM/sXTgRmZg3nRGBm1nBOBGZmDedEYGbWcE4EZh0kaVWrYqrZ6cKJwMys4ZwIzE5C0n2SxiSNS9qSzz84KunprJE/IunC7Dsg6cOsD7+9Ujv+Skk7JX0i6WNJV+Tm+yrPFhjKX86adY0TgVkbSdcA9wArI2IAOA6soxQ+2xMR1wKjlPrvAC8Dj0TEcsqvPVvtQ8CzEXEdcDPl16NQKsRupDwbo59SQ8esa3r+u4tZ46wGbgR258H6eZSiX38Cb2SfV4FhSYuAxRExmu1bgW2SzgcuiYjtABHxO0BubywiJnN5nFKLftf/Pyyzk3MiMJtOwNaI2PSPRumxtn5zrc/yR+X9cfw9tC7z1JDZdCPAoKSL4MTzYy+jfF9alS/vBXZFxBHgZ0m3Zvt6YDQifgUmJd2V2+iVtKCjozCbIR+JmLWJiH2SHgXek3QWpSrkw5QHwtyU66Yo1xGglAl+Pv/RHwA2ZPt6YIukJ3Ibd3dwGGYz5uqjZjMk6WhE9HV7P8zmm6eGzMwazmcEZmYN5zMCM7OGcyIwM2s4JwIzs4ZzIjAzazgnAjOzhvsLYpSDMk3IlmMAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#predictions = model.predict_classes(x_testcnn)\n",
        "predictions = model.predict(x_testcnn)"
      ],
      "metadata": {
        "id": "A2xtrN5wDQgY"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_f_ohjXKDUpY",
        "outputId": "41563c20-51f6-4c29-9a27-243fbd6be919"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2.45538504e-05, 2.53202165e-06, 3.03311683e-02, 5.00794209e-04,\n",
              "        4.95684659e-03, 9.64184105e-01],\n",
              "       [7.39496574e-02, 8.96101356e-01, 1.60785881e-03, 1.74500346e-02,\n",
              "        8.91765766e-03, 1.97346206e-03],\n",
              "       [1.03725426e-04, 3.66595993e-03, 8.76672029e-01, 5.29381447e-04,\n",
              "        5.75798415e-02, 6.14490733e-02],\n",
              "       ...,\n",
              "       [1.31198045e-04, 6.46811486e-07, 2.75266473e-04, 6.92139962e-04,\n",
              "        9.69446063e-01, 2.94546206e-02],\n",
              "       [2.22526491e-02, 6.19887421e-03, 2.80504748e-02, 5.07400811e-01,\n",
              "        1.09788803e-02, 4.25118327e-01],\n",
              "       [2.75381804e-02, 1.79459974e-02, 2.58701835e-02, 4.05523647e-03,\n",
              "        9.24246192e-01, 3.44197091e-04]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmA2bFgsDW3D",
        "outputId": "21f63467-a931-43f8-806e-5f84c7bac47d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 1, 2, 5, 3, 4, 2, 0, 3, 1, 3, 2, 1, 4, 3, 0, 5, 0, 2, 5, 0, 0,\n",
              "       1, 3, 2, 3, 2, 5, 4, 2, 2, 3, 1, 2, 5, 5, 3, 5, 3, 3, 4, 1, 3, 2,\n",
              "       3, 1, 5, 3, 1, 2, 4, 4, 5, 5, 5, 1, 2, 3, 2, 5, 0, 2, 4, 2, 5, 4,\n",
              "       3, 0, 1, 2, 5, 2, 1, 2, 4, 5, 4, 1, 5, 5, 3, 1, 5, 3, 2, 4, 2, 5,\n",
              "       0, 5, 0, 5, 4, 0, 2, 4, 0, 2, 2, 0, 2, 3, 5, 3, 5, 5, 3, 1, 2, 2,\n",
              "       2, 1, 3, 2, 1, 3, 4, 4, 1, 1, 2, 4, 4, 1, 4, 2, 0, 4, 5, 5, 3, 1,\n",
              "       2, 1, 5, 5, 1, 2, 3, 3, 0, 3, 3, 0, 1, 0, 1, 2, 1, 5, 1, 2, 2, 0,\n",
              "       1, 1, 1, 1, 1, 4, 0, 1, 5, 1, 4, 5, 5, 4, 1, 0, 3, 5, 2, 3, 0, 4,\n",
              "       1, 3, 1, 0, 5, 5, 3, 5, 4, 5, 4, 0, 4, 1, 2, 1, 5, 0, 4, 3, 2, 1,\n",
              "       0, 3, 5, 0, 4, 5, 4, 5, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_Ytest = y_test.astype(int)"
      ],
      "metadata": {
        "id": "0PHDKWJWDY3g"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_Ytest"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k68v2i-pDbZE",
        "outputId": "baf0ab95-5ada-4648-c060-bbcdfa932245"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 1, 2, 5, 3, 4, 2, 0, 3, 1, 3, 2, 1, 4, 3, 0, 5, 0, 2, 5, 0, 0,\n",
              "       1, 3, 2, 3, 2, 5, 4, 2, 2, 3, 1, 2, 5, 5, 3, 5, 3, 3, 4, 1, 3, 2,\n",
              "       3, 1, 5, 3, 1, 2, 4, 4, 5, 5, 5, 1, 2, 3, 2, 5, 0, 2, 4, 2, 5, 4,\n",
              "       3, 0, 1, 2, 5, 2, 1, 2, 4, 5, 4, 1, 5, 5, 3, 1, 5, 3, 2, 4, 2, 5,\n",
              "       0, 5, 0, 5, 4, 0, 2, 4, 0, 2, 2, 0, 2, 3, 5, 3, 5, 5, 3, 1, 2, 2,\n",
              "       2, 1, 3, 2, 1, 3, 4, 4, 1, 1, 2, 4, 4, 1, 4, 2, 0, 4, 5, 5, 3, 1,\n",
              "       2, 1, 5, 5, 1, 2, 3, 3, 0, 3, 3, 0, 1, 0, 1, 2, 1, 5, 1, 2, 2, 0,\n",
              "       1, 1, 1, 1, 1, 4, 0, 1, 5, 1, 4, 5, 5, 4, 1, 0, 3, 5, 2, 3, 0, 4,\n",
              "       1, 3, 1, 0, 5, 5, 3, 5, 4, 5, 4, 0, 4, 1, 2, 1, 5, 0, 4, 3, 2, 1,\n",
              "       0, 3, 5, 0, 4, 5, 4, 5, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds1=predictions.argmax(axis=1)\n",
        "preds1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_cAI39VFhtp",
        "outputId": "edc0b769-ef5f-4d73-e029-4d92647d06ee"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 1, 2, 5, 1, 4, 2, 0, 3, 1, 3, 2, 1, 4, 3, 0, 5, 3, 2, 5, 2, 0,\n",
              "       1, 3, 2, 5, 2, 5, 2, 4, 2, 3, 1, 2, 5, 4, 1, 5, 3, 2, 4, 1, 0, 2,\n",
              "       4, 1, 5, 5, 1, 4, 4, 4, 5, 2, 5, 1, 2, 3, 2, 2, 0, 2, 4, 4, 5, 4,\n",
              "       0, 0, 1, 2, 3, 2, 1, 5, 4, 5, 4, 3, 5, 5, 3, 1, 5, 3, 2, 4, 2, 3,\n",
              "       0, 5, 0, 5, 4, 0, 3, 4, 3, 4, 2, 0, 2, 3, 0, 3, 5, 5, 5, 1, 2, 2,\n",
              "       2, 1, 3, 2, 1, 3, 4, 4, 1, 0, 2, 4, 4, 1, 4, 2, 2, 4, 4, 5, 5, 1,\n",
              "       4, 1, 5, 5, 1, 3, 3, 3, 0, 3, 5, 0, 1, 0, 1, 2, 1, 5, 0, 2, 4, 0,\n",
              "       1, 1, 1, 1, 2, 4, 0, 1, 5, 1, 4, 5, 4, 2, 1, 0, 5, 0, 2, 3, 3, 4,\n",
              "       1, 3, 1, 5, 5, 5, 3, 5, 4, 5, 4, 0, 0, 0, 2, 1, 5, 0, 4, 1, 2, 1,\n",
              "       1, 4, 5, 0, 4, 5, 4, 3, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "abc = preds1.astype(int).flatten()"
      ],
      "metadata": {
        "id": "H4WEmM60NfQn"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "c = confusion_matrix(new_Ytest, abc) \n",
        "c"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKEBeEBlFjB8",
        "outputId": "808a9b8d-66d5-4b71-ff8d-ad721b403478"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[18,  1,  2,  3,  0,  1],\n",
              "       [ 3, 34,  1,  1,  0,  0],\n",
              "       [ 0,  0, 29,  2,  6,  1],\n",
              "       [ 2,  3,  1, 19,  2,  6],\n",
              "       [ 1,  0,  2,  0, 27,  0],\n",
              "       [ 2,  0,  2,  3,  3, 32]])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4RjG7LWLSQx",
        "outputId": "6a77ffa4-67ae-4ef7-f772-eb2964eaeecb"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 1, 2, 5, 3, 4, 2, 0, 3, 1, 3, 2, 1, 4, 3, 0, 5, 0, 2, 5, 0, 0,\n",
              "       1, 3, 2, 3, 2, 5, 4, 2, 2, 3, 1, 2, 5, 5, 3, 5, 3, 3, 4, 1, 3, 2,\n",
              "       3, 1, 5, 3, 1, 2, 4, 4, 5, 5, 5, 1, 2, 3, 2, 5, 0, 2, 4, 2, 5, 4,\n",
              "       3, 0, 1, 2, 5, 2, 1, 2, 4, 5, 4, 1, 5, 5, 3, 1, 5, 3, 2, 4, 2, 5,\n",
              "       0, 5, 0, 5, 4, 0, 2, 4, 0, 2, 2, 0, 2, 3, 5, 3, 5, 5, 3, 1, 2, 2,\n",
              "       2, 1, 3, 2, 1, 3, 4, 4, 1, 1, 2, 4, 4, 1, 4, 2, 0, 4, 5, 5, 3, 1,\n",
              "       2, 1, 5, 5, 1, 2, 3, 3, 0, 3, 3, 0, 1, 0, 1, 2, 1, 5, 1, 2, 2, 0,\n",
              "       1, 1, 1, 1, 1, 4, 0, 1, 5, 1, 4, 5, 5, 4, 1, 0, 3, 5, 2, 3, 0, 4,\n",
              "       1, 3, 1, 0, 5, 5, 3, 5, 4, 5, 4, 0, 4, 1, 2, 1, 5, 0, 4, 3, 2, 1,\n",
              "       0, 3, 5, 0, 4, 5, 4, 5, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/drive/MyDrive/model2')\n",
        "print(\"MODEL SAVED\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsJBNeMWDl6Z",
        "outputId": "7a3d6bc0-d91a-4756-defd-610808296b39"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/model2/assets\n",
            "MODEL SAVED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_model=keras.models.load_model('/content/drive/MyDrive/model2')\n",
        "new_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3KjaO9GDoNl",
        "outputId": "99a0ac47-ecd5-4026-c504-a3ddfe6f99d3"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d (Conv1D)             (None, 40, 128)           1152      \n",
            "                                                                 \n",
            " activation (Activation)     (None, 40, 128)           0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 40, 128)           0         \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, 8, 128)           0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 8, 256)            262400    \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 8, 256)            0         \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 8, 256)            0         \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPooling  (None, 1, 256)           0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               65792     \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 256)               65792     \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 256)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 6)                 1542      \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 6)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 396,678\n",
            "Trainable params: 396,678\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, acc =model.evaluate(x_testcnn, y_test)\n",
        "print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3kzoqPkDqqS",
        "outputId": "3aa4cedd-993d-492e-ce1c-94eeea531bcd"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7/7 [==============================] - 0s 4ms/step - loss: 0.6258 - accuracy: 0.7681\n",
            "Restored model, accuracy: 76.81%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, acc =model.evaluate(X_train, y_train)\n",
        "print(\"Restored model train, accuracy: {:5.2f}%\".format(100*acc))"
      ],
      "metadata": {
        "id": "fJDTGH_OUX0z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41e9364e-7d4c-42b7-c7e6-dfd3386259fd"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "52/52 [==============================] - 0s 4ms/step - loss: 0.2702 - accuracy: 0.9274\n",
            "Restored model train, accuracy: 92.74%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix,accuracy_score\n",
        "import seaborn as sn\n",
        "\n",
        "print(classification_report(new_Ytest,abc))\n",
        "\n",
        "acc = float(accuracy_score(new_Ytest,abc))*100\n",
        "print(\"----accuracy score %s ----\" % acc)\n",
        "\n",
        "cm = confusion_matrix(new_Ytest,abc)\n",
        "#df_cm = pd.DataFrame(cm)\n",
        "class_names = ['neutral','calm', 'happy','sad','angry', 'fearful' ]\n",
        "df_cm = pd.DataFrame(cm, index=class_names, columns=class_names,)\n",
        "sn.heatmap(df_cm, annot=True, fmt='')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "id": "SfSC3El94LZg",
        "outputId": "af1762db-7dec-42e7-efd9-9660dbf68aa2"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.72      0.71        25\n",
            "           1       0.89      0.87      0.88        39\n",
            "           2       0.78      0.76      0.77        38\n",
            "           3       0.68      0.58      0.62        33\n",
            "           4       0.71      0.90      0.79        30\n",
            "           5       0.80      0.76      0.78        42\n",
            "\n",
            "    accuracy                           0.77       207\n",
            "   macro avg       0.76      0.77      0.76       207\n",
            "weighted avg       0.77      0.77      0.77       207\n",
            "\n",
            "----accuracy score 76.81159420289855 ----\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}