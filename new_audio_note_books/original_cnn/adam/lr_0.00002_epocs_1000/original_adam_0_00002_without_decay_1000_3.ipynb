{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "original_adam_0.00002_without decay_1000_3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SRCO-a_AlH5",
        "outputId": "6f29e1b5-a929-49e5-e624-c425a9d12138"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install librosa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Lo4mUwG9RMd",
        "outputId": "352bb062-9db4-42e9-a618-9a202e49a920"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.7/dist-packages (0.8.1)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.6.0)\n",
            "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.10.3.post1)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.0.2)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (2.1.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (21.3)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.2.2)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.51.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.21.6)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa) (57.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->librosa) (3.0.9)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa) (1.4.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2022.5.18.1)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.7/dist-packages (from resampy>=0.2.2->librosa) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.10.2->librosa) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa) (2.21)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJjcbxwy46bG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "718e6f0c-f665-4065-ae7e-21cec6ad4f34"
      },
      "source": [
        "# Orignial Notebook: https://github.com/MITESHPUTHRANNEU/Speech-Emotion-Analyzer/blob/master/final_results_gender_test.ipynb\n",
        "# This notebook author: Reza Chu\n",
        "# Last Editing Date: 31st May 2019\n",
        "\n",
        "## Python\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "\n",
        "import IPython\n",
        "from IPython.display import Audio\n",
        "from IPython.display import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "## Package\n",
        "import glob \n",
        "import keras\n",
        "import IPython.display as ipd\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.graph_objs as go\n",
        "import plotly.offline as py\n",
        "import plotly.tools as tls\n",
        "import seaborn as sns\n",
        "import scipy.io.wavfile\n",
        "import tensorflow as tf\n",
        "py.init_notebook_mode(connected=True)\n",
        "\n",
        "## Keras\n",
        "from keras import regularizers\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping\n",
        "from keras.callbacks import  History, ReduceLROnPlateau, CSVLogger\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM\n",
        "from keras.layers import Input, Flatten, Dropout, Activation, BatchNormalization\n",
        "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import np_utils\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "## Sklearn\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "## Rest\n",
        "from scipy.fftpack import fft\n",
        "from scipy import signal\n",
        "from scipy.io import wavfile\n",
        "from tqdm import tqdm\n",
        "\n",
        "input_duration=3\n",
        "# % pylab inline"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "        <script type=\"text/javascript\">\n",
              "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
              "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
              "        if (typeof require !== 'undefined') {\n",
              "        require.undef(\"plotly\");\n",
              "        requirejs.config({\n",
              "            paths: {\n",
              "                'plotly': ['https://cdn.plot.ly/plotly-2.8.3.min']\n",
              "            }\n",
              "        });\n",
              "        require(['plotly'], function(Plotly) {\n",
              "            window._Plotly = Plotly;\n",
              "        });\n",
              "        }\n",
              "        </script>\n",
              "        "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Saving joblib files to not load them again with the loop above\n",
        "\n",
        "# import joblib\n",
        "\n",
        "# X_name = 'x.joblib'\n",
        "# y_name = 'y.joblib'\n",
        "# save_dir = '/content/drive/My Drive/graduation project/audio/paper_code/features'\n",
        "\n",
        "# savedX = joblib.dump(X, os.path.join(save_dir, X_name))\n",
        "# savedy = joblib.dump(y, os.path.join(save_dir, y_name))"
      ],
      "metadata": {
        "id": "UCzic8rlDcuk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Loading saved models\n",
        "import joblib\n",
        "X = joblib.load('/content/drive/My Drive/graduation project/audio/paper_code/features/x.joblib')\n",
        "y = joblib.load('/content/drive/My Drive/graduation project/audio/paper_code/features/y.joblib')"
      ],
      "metadata": {
        "id": "Q35CN6zDrzg1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PSTurzjCo5K",
        "outputId": "464adaf7-b9ad-4e75-88cf-ec5f8669ac7a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2068, 40)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.1 ,shuffle = True\n",
        "                                                    , random_state=42)\n",
        "X_train , X_valid, y_train, y_valid = train_test_split(X_train,y_train, test_size=0.1112305212 , shuffle = True \n",
        "                                                       , random_state=42)"
      ],
      "metadata": {
        "id": "Ai4Fy5cPCiq1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "x_traincnn = np.expand_dims(X_train, axis=2)\n",
        "x_testcnn = np.expand_dims(X_test, axis=2)\n",
        "X_valid= np.expand_dims(X_valid, axis=2)"
      ],
      "metadata": {
        "id": "tp1Fm5K3CEXu"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_traincnn.shape, x_testcnn.shape , X_valid.shape\n",
        "#1861"
      ],
      "metadata": {
        "id": "RI0MxoIPBws5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75684de9-94a9-4f64-a277-d0c627d15344"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1654, 40, 1), (207, 40, 1), (207, 40, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oALhiMUd9G2Y",
        "outputId": "53c60509-edae-47fc-d1e7-83f11e8aab35"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.8.2+zzzcolab20220527125636)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (14.0.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (4.2.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.0.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.26.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.46.3)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.3.7)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.11.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.layers import Input, Flatten, Dropout, Activation\n",
        "from keras.layers import Conv1D, MaxPooling1D\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras import optimizers\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv1D(128, 8,padding='same', #classifier.add(Convolution2D(64, (3, 3), padding = 'same', input_shape = (128, 128, 3), activation = 'relu'))\n",
        "                 input_shape=(40,1)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(MaxPooling1D(pool_size=(5)))\n",
        "\n",
        "\n",
        "model.add(Conv1D(256,8,padding='same',))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(MaxPooling1D(pool_size=(5)))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(256))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(256))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Dense(6))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "opt = tf.keras.optimizers.Adam(lr=0.00002 , decay=0.0)\n",
        "#opt = tf.keras.optimizers.RMSprop(lr=0.0001, rho=0.9, epsilon=1e-07, decay=0.0)\n",
        "#opt = tf.keras.optimizers.SGD(lr=0.001, momentum=0.0, decay=0.0, nesterov=False)"
      ],
      "metadata": {
        "id": "g74fXWVAC4Cr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2b5746b-389d-4ed9-cefa-7c54d94cd225"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning:\n",
            "\n",
            "The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6ukOxAGC_I4",
        "outputId": "217c7e84-b584-44f2-e852-87f927e54edf"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d (Conv1D)             (None, 40, 128)           1152      \n",
            "                                                                 \n",
            " activation (Activation)     (None, 40, 128)           0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 40, 128)           0         \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, 8, 128)           0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 8, 256)            262400    \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 8, 256)            0         \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 8, 256)            0         \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPooling  (None, 1, 256)           0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               65792     \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 256)               65792     \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 256)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 6)                 1542      \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 6)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 396,678\n",
            "Trainable params: 396,678\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "AbMlLNk4DCBM"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an Instance of Early Stopping Callback.\n",
        "early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 25, mode = 'min', restore_best_weights = True)\n",
        "\n",
        "cnnhistory=model.fit(x_traincnn, y_train, batch_size=16, epochs=1000 , shuffle = True, \n",
        "                     validation_data=(X_valid, y_valid) \n",
        "                     , callbacks = [early_stopping_callback]\n",
        "                     )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RI1v2AuADFhy",
        "outputId": "b8ff8b7c-692d-4762-e028-b138b6c6ddf1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "104/104 [==============================] - 4s 7ms/step - loss: 6.9663 - accuracy: 0.1729 - val_loss: 2.3074 - val_accuracy: 0.1884\n",
            "Epoch 2/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 4.7941 - accuracy: 0.1790 - val_loss: 2.5467 - val_accuracy: 0.1739\n",
            "Epoch 3/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 4.0963 - accuracy: 0.1904 - val_loss: 2.2173 - val_accuracy: 0.1981\n",
            "Epoch 4/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 3.8151 - accuracy: 0.1778 - val_loss: 2.0498 - val_accuracy: 0.1981\n",
            "Epoch 5/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 3.4519 - accuracy: 0.1802 - val_loss: 1.9016 - val_accuracy: 0.2271\n",
            "Epoch 6/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 3.1831 - accuracy: 0.1802 - val_loss: 1.9175 - val_accuracy: 0.2560\n",
            "Epoch 7/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 2.9011 - accuracy: 0.1917 - val_loss: 1.8431 - val_accuracy: 0.2367\n",
            "Epoch 8/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 2.7594 - accuracy: 0.1935 - val_loss: 1.7917 - val_accuracy: 0.2512\n",
            "Epoch 9/1000\n",
            "104/104 [==============================] - 1s 8ms/step - loss: 2.6836 - accuracy: 0.1838 - val_loss: 1.8154 - val_accuracy: 0.2319\n",
            "Epoch 10/1000\n",
            "104/104 [==============================] - 1s 8ms/step - loss: 2.5256 - accuracy: 0.1898 - val_loss: 1.8757 - val_accuracy: 0.1691\n",
            "Epoch 11/1000\n",
            "104/104 [==============================] - 1s 8ms/step - loss: 2.4572 - accuracy: 0.1953 - val_loss: 1.7820 - val_accuracy: 0.2077\n",
            "Epoch 12/1000\n",
            "104/104 [==============================] - 1s 8ms/step - loss: 2.3424 - accuracy: 0.2007 - val_loss: 1.7540 - val_accuracy: 0.2319\n",
            "Epoch 13/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 2.2473 - accuracy: 0.2183 - val_loss: 1.7413 - val_accuracy: 0.2464\n",
            "Epoch 14/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 2.2592 - accuracy: 0.1989 - val_loss: 1.7238 - val_accuracy: 0.2464\n",
            "Epoch 15/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 2.1942 - accuracy: 0.2116 - val_loss: 1.7227 - val_accuracy: 0.2464\n",
            "Epoch 16/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 2.1230 - accuracy: 0.2243 - val_loss: 1.7225 - val_accuracy: 0.2802\n",
            "Epoch 17/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 2.1445 - accuracy: 0.2074 - val_loss: 1.7066 - val_accuracy: 0.2947\n",
            "Epoch 18/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 2.1234 - accuracy: 0.2201 - val_loss: 1.7113 - val_accuracy: 0.2415\n",
            "Epoch 19/1000\n",
            "104/104 [==============================] - 1s 8ms/step - loss: 2.0864 - accuracy: 0.2170 - val_loss: 1.7271 - val_accuracy: 0.2512\n",
            "Epoch 20/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 2.0588 - accuracy: 0.2170 - val_loss: 1.7181 - val_accuracy: 0.2319\n",
            "Epoch 21/1000\n",
            "104/104 [==============================] - 1s 8ms/step - loss: 2.0153 - accuracy: 0.2255 - val_loss: 1.6978 - val_accuracy: 0.2899\n",
            "Epoch 22/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 1.9916 - accuracy: 0.2273 - val_loss: 1.6941 - val_accuracy: 0.2754\n",
            "Epoch 23/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 1.9571 - accuracy: 0.2340 - val_loss: 1.7011 - val_accuracy: 0.2899\n",
            "Epoch 24/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 1.9595 - accuracy: 0.2334 - val_loss: 1.7234 - val_accuracy: 0.2415\n",
            "Epoch 25/1000\n",
            "104/104 [==============================] - 1s 8ms/step - loss: 1.9576 - accuracy: 0.2116 - val_loss: 1.7177 - val_accuracy: 0.2512\n",
            "Epoch 26/1000\n",
            "104/104 [==============================] - 1s 8ms/step - loss: 1.9145 - accuracy: 0.2382 - val_loss: 1.6715 - val_accuracy: 0.2850\n",
            "Epoch 27/1000\n",
            "104/104 [==============================] - 1s 8ms/step - loss: 1.9479 - accuracy: 0.2352 - val_loss: 1.7251 - val_accuracy: 0.2705\n",
            "Epoch 28/1000\n",
            "104/104 [==============================] - 1s 8ms/step - loss: 1.8867 - accuracy: 0.2249 - val_loss: 1.6967 - val_accuracy: 0.2560\n",
            "Epoch 29/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 1.8910 - accuracy: 0.2418 - val_loss: 1.6682 - val_accuracy: 0.3043\n",
            "Epoch 30/1000\n",
            "104/104 [==============================] - 1s 8ms/step - loss: 1.8342 - accuracy: 0.2557 - val_loss: 1.6720 - val_accuracy: 0.2947\n",
            "Epoch 31/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 1.8601 - accuracy: 0.2437 - val_loss: 1.6758 - val_accuracy: 0.2754\n",
            "Epoch 32/1000\n",
            "104/104 [==============================] - 1s 9ms/step - loss: 1.8260 - accuracy: 0.2703 - val_loss: 1.6853 - val_accuracy: 0.2899\n",
            "Epoch 33/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 1.8298 - accuracy: 0.2539 - val_loss: 1.6654 - val_accuracy: 0.3285\n",
            "Epoch 34/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 1.8451 - accuracy: 0.2406 - val_loss: 1.6722 - val_accuracy: 0.3382\n",
            "Epoch 35/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 1.7886 - accuracy: 0.2630 - val_loss: 1.6568 - val_accuracy: 0.3430\n",
            "Epoch 36/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 1.8062 - accuracy: 0.2509 - val_loss: 1.6540 - val_accuracy: 0.2995\n",
            "Epoch 37/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.8323 - accuracy: 0.2424 - val_loss: 1.6492 - val_accuracy: 0.3043\n",
            "Epoch 38/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.7976 - accuracy: 0.2509 - val_loss: 1.6536 - val_accuracy: 0.2947\n",
            "Epoch 39/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.7993 - accuracy: 0.2576 - val_loss: 1.6377 - val_accuracy: 0.3188\n",
            "Epoch 40/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.7925 - accuracy: 0.2551 - val_loss: 1.6363 - val_accuracy: 0.3285\n",
            "Epoch 41/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.7906 - accuracy: 0.2582 - val_loss: 1.6421 - val_accuracy: 0.3720\n",
            "Epoch 42/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.7560 - accuracy: 0.2588 - val_loss: 1.6442 - val_accuracy: 0.3623\n",
            "Epoch 43/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.7627 - accuracy: 0.2781 - val_loss: 1.6324 - val_accuracy: 0.3333\n",
            "Epoch 44/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.7507 - accuracy: 0.2920 - val_loss: 1.6310 - val_accuracy: 0.3188\n",
            "Epoch 45/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.7765 - accuracy: 0.2721 - val_loss: 1.6433 - val_accuracy: 0.3237\n",
            "Epoch 46/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.7499 - accuracy: 0.2678 - val_loss: 1.6309 - val_accuracy: 0.3575\n",
            "Epoch 47/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.7319 - accuracy: 0.2793 - val_loss: 1.6329 - val_accuracy: 0.3816\n",
            "Epoch 48/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.7278 - accuracy: 0.2787 - val_loss: 1.6254 - val_accuracy: 0.3671\n",
            "Epoch 49/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.7018 - accuracy: 0.2902 - val_loss: 1.6227 - val_accuracy: 0.3768\n",
            "Epoch 50/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.7114 - accuracy: 0.2739 - val_loss: 1.6233 - val_accuracy: 0.3575\n",
            "Epoch 51/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.7154 - accuracy: 0.2624 - val_loss: 1.6113 - val_accuracy: 0.3671\n",
            "Epoch 52/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.7362 - accuracy: 0.2678 - val_loss: 1.6121 - val_accuracy: 0.4058\n",
            "Epoch 53/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.7153 - accuracy: 0.2830 - val_loss: 1.6160 - val_accuracy: 0.3430\n",
            "Epoch 54/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.6953 - accuracy: 0.2866 - val_loss: 1.6127 - val_accuracy: 0.3333\n",
            "Epoch 55/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.6833 - accuracy: 0.2981 - val_loss: 1.6037 - val_accuracy: 0.3527\n",
            "Epoch 56/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.6852 - accuracy: 0.2914 - val_loss: 1.6053 - val_accuracy: 0.3478\n",
            "Epoch 57/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.6625 - accuracy: 0.2860 - val_loss: 1.6024 - val_accuracy: 0.3478\n",
            "Epoch 58/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.6741 - accuracy: 0.3077 - val_loss: 1.5999 - val_accuracy: 0.3285\n",
            "Epoch 59/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.6627 - accuracy: 0.2956 - val_loss: 1.5927 - val_accuracy: 0.3720\n",
            "Epoch 60/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.6507 - accuracy: 0.2896 - val_loss: 1.5928 - val_accuracy: 0.3188\n",
            "Epoch 61/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.6737 - accuracy: 0.3150 - val_loss: 1.5829 - val_accuracy: 0.3188\n",
            "Epoch 62/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.6688 - accuracy: 0.3011 - val_loss: 1.5636 - val_accuracy: 0.3913\n",
            "Epoch 63/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.6492 - accuracy: 0.2987 - val_loss: 1.5650 - val_accuracy: 0.4058\n",
            "Epoch 64/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.6363 - accuracy: 0.3204 - val_loss: 1.5719 - val_accuracy: 0.3913\n",
            "Epoch 65/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.6393 - accuracy: 0.3216 - val_loss: 1.5798 - val_accuracy: 0.3285\n",
            "Epoch 66/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.6499 - accuracy: 0.3083 - val_loss: 1.5718 - val_accuracy: 0.3913\n",
            "Epoch 67/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.6487 - accuracy: 0.3096 - val_loss: 1.5643 - val_accuracy: 0.4203\n",
            "Epoch 68/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.6293 - accuracy: 0.3162 - val_loss: 1.5587 - val_accuracy: 0.4203\n",
            "Epoch 69/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.6235 - accuracy: 0.3210 - val_loss: 1.5804 - val_accuracy: 0.3816\n",
            "Epoch 70/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.6108 - accuracy: 0.3289 - val_loss: 1.5642 - val_accuracy: 0.3478\n",
            "Epoch 71/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.6310 - accuracy: 0.3102 - val_loss: 1.5603 - val_accuracy: 0.3478\n",
            "Epoch 72/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.5942 - accuracy: 0.3289 - val_loss: 1.5633 - val_accuracy: 0.3527\n",
            "Epoch 73/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.6137 - accuracy: 0.3295 - val_loss: 1.5481 - val_accuracy: 0.3961\n",
            "Epoch 74/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.6067 - accuracy: 0.3222 - val_loss: 1.5307 - val_accuracy: 0.4396\n",
            "Epoch 75/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.5873 - accuracy: 0.3356 - val_loss: 1.5337 - val_accuracy: 0.3816\n",
            "Epoch 76/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.5926 - accuracy: 0.3476 - val_loss: 1.5346 - val_accuracy: 0.3720\n",
            "Epoch 77/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.5974 - accuracy: 0.3337 - val_loss: 1.5290 - val_accuracy: 0.3865\n",
            "Epoch 78/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.5606 - accuracy: 0.3609 - val_loss: 1.5230 - val_accuracy: 0.4106\n",
            "Epoch 79/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.5957 - accuracy: 0.3531 - val_loss: 1.5222 - val_accuracy: 0.4348\n",
            "Epoch 80/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.5705 - accuracy: 0.3446 - val_loss: 1.5362 - val_accuracy: 0.3816\n",
            "Epoch 81/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.5695 - accuracy: 0.3513 - val_loss: 1.5056 - val_accuracy: 0.4734\n",
            "Epoch 82/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.5562 - accuracy: 0.3640 - val_loss: 1.5088 - val_accuracy: 0.4251\n",
            "Epoch 83/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.5532 - accuracy: 0.3567 - val_loss: 1.4990 - val_accuracy: 0.4396\n",
            "Epoch 84/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.5443 - accuracy: 0.3646 - val_loss: 1.4925 - val_accuracy: 0.4541\n",
            "Epoch 85/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.5433 - accuracy: 0.3603 - val_loss: 1.4857 - val_accuracy: 0.4831\n",
            "Epoch 86/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.5528 - accuracy: 0.3676 - val_loss: 1.4846 - val_accuracy: 0.4831\n",
            "Epoch 87/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.5290 - accuracy: 0.3622 - val_loss: 1.4819 - val_accuracy: 0.4541\n",
            "Epoch 88/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.5323 - accuracy: 0.3597 - val_loss: 1.4798 - val_accuracy: 0.4493\n",
            "Epoch 89/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.5098 - accuracy: 0.3803 - val_loss: 1.4734 - val_accuracy: 0.4396\n",
            "Epoch 90/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.5222 - accuracy: 0.3742 - val_loss: 1.4750 - val_accuracy: 0.4589\n",
            "Epoch 91/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.5149 - accuracy: 0.3755 - val_loss: 1.4761 - val_accuracy: 0.4734\n",
            "Epoch 92/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.5294 - accuracy: 0.3761 - val_loss: 1.4646 - val_accuracy: 0.4783\n",
            "Epoch 93/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.5194 - accuracy: 0.3773 - val_loss: 1.4657 - val_accuracy: 0.4686\n",
            "Epoch 94/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.5048 - accuracy: 0.3936 - val_loss: 1.4495 - val_accuracy: 0.4541\n",
            "Epoch 95/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.5003 - accuracy: 0.3827 - val_loss: 1.4626 - val_accuracy: 0.4348\n",
            "Epoch 96/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.5042 - accuracy: 0.3942 - val_loss: 1.4467 - val_accuracy: 0.5024\n",
            "Epoch 97/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.4703 - accuracy: 0.3881 - val_loss: 1.4320 - val_accuracy: 0.4879\n",
            "Epoch 98/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.4830 - accuracy: 0.3736 - val_loss: 1.4401 - val_accuracy: 0.4686\n",
            "Epoch 99/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.4824 - accuracy: 0.3960 - val_loss: 1.4264 - val_accuracy: 0.4638\n",
            "Epoch 100/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.4660 - accuracy: 0.4099 - val_loss: 1.4253 - val_accuracy: 0.4783\n",
            "Epoch 101/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.4872 - accuracy: 0.3966 - val_loss: 1.4187 - val_accuracy: 0.4928\n",
            "Epoch 102/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.4649 - accuracy: 0.3954 - val_loss: 1.4270 - val_accuracy: 0.4638\n",
            "Epoch 103/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.4786 - accuracy: 0.3942 - val_loss: 1.4157 - val_accuracy: 0.4444\n",
            "Epoch 104/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.4539 - accuracy: 0.4027 - val_loss: 1.4062 - val_accuracy: 0.5217\n",
            "Epoch 105/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.4447 - accuracy: 0.4172 - val_loss: 1.4038 - val_accuracy: 0.4300\n",
            "Epoch 106/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.4380 - accuracy: 0.4099 - val_loss: 1.3981 - val_accuracy: 0.5217\n",
            "Epoch 107/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.4427 - accuracy: 0.4141 - val_loss: 1.3977 - val_accuracy: 0.4783\n",
            "Epoch 108/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.4196 - accuracy: 0.4383 - val_loss: 1.3840 - val_accuracy: 0.5217\n",
            "Epoch 109/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.4119 - accuracy: 0.4069 - val_loss: 1.3883 - val_accuracy: 0.4879\n",
            "Epoch 110/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.4164 - accuracy: 0.4341 - val_loss: 1.3792 - val_accuracy: 0.5362\n",
            "Epoch 111/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.4325 - accuracy: 0.4196 - val_loss: 1.3651 - val_accuracy: 0.4783\n",
            "Epoch 112/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.4187 - accuracy: 0.4299 - val_loss: 1.3682 - val_accuracy: 0.5121\n",
            "Epoch 113/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.4123 - accuracy: 0.4305 - val_loss: 1.3609 - val_accuracy: 0.5217\n",
            "Epoch 114/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.4190 - accuracy: 0.4226 - val_loss: 1.3600 - val_accuracy: 0.5169\n",
            "Epoch 115/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.3805 - accuracy: 0.4577 - val_loss: 1.3721 - val_accuracy: 0.5072\n",
            "Epoch 116/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.3965 - accuracy: 0.4311 - val_loss: 1.3573 - val_accuracy: 0.4831\n",
            "Epoch 117/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.3803 - accuracy: 0.4468 - val_loss: 1.3510 - val_accuracy: 0.5459\n",
            "Epoch 118/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.3829 - accuracy: 0.4341 - val_loss: 1.3406 - val_accuracy: 0.4734\n",
            "Epoch 119/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.3710 - accuracy: 0.4432 - val_loss: 1.3356 - val_accuracy: 0.5072\n",
            "Epoch 120/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.3654 - accuracy: 0.4565 - val_loss: 1.3316 - val_accuracy: 0.5556\n",
            "Epoch 121/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.3749 - accuracy: 0.4462 - val_loss: 1.3233 - val_accuracy: 0.5024\n",
            "Epoch 122/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.3644 - accuracy: 0.4401 - val_loss: 1.3271 - val_accuracy: 0.5266\n",
            "Epoch 123/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.3573 - accuracy: 0.4444 - val_loss: 1.3216 - val_accuracy: 0.5362\n",
            "Epoch 124/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.3538 - accuracy: 0.4613 - val_loss: 1.3355 - val_accuracy: 0.4396\n",
            "Epoch 125/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.3602 - accuracy: 0.4522 - val_loss: 1.3086 - val_accuracy: 0.5024\n",
            "Epoch 126/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.3399 - accuracy: 0.4516 - val_loss: 1.3119 - val_accuracy: 0.5024\n",
            "Epoch 127/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.3440 - accuracy: 0.4468 - val_loss: 1.3111 - val_accuracy: 0.5072\n",
            "Epoch 128/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.3629 - accuracy: 0.4577 - val_loss: 1.3138 - val_accuracy: 0.4928\n",
            "Epoch 129/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.3346 - accuracy: 0.4504 - val_loss: 1.2999 - val_accuracy: 0.5072\n",
            "Epoch 130/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.3273 - accuracy: 0.4643 - val_loss: 1.2983 - val_accuracy: 0.4928\n",
            "Epoch 131/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.3248 - accuracy: 0.4589 - val_loss: 1.3027 - val_accuracy: 0.4976\n",
            "Epoch 132/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.3059 - accuracy: 0.4728 - val_loss: 1.2948 - val_accuracy: 0.5121\n",
            "Epoch 133/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.2894 - accuracy: 0.4722 - val_loss: 1.2728 - val_accuracy: 0.5459\n",
            "Epoch 134/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.3221 - accuracy: 0.4601 - val_loss: 1.2821 - val_accuracy: 0.5217\n",
            "Epoch 135/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.3008 - accuracy: 0.4752 - val_loss: 1.2687 - val_accuracy: 0.5411\n",
            "Epoch 136/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.3052 - accuracy: 0.4800 - val_loss: 1.2783 - val_accuracy: 0.5362\n",
            "Epoch 137/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.2875 - accuracy: 0.4855 - val_loss: 1.2810 - val_accuracy: 0.5459\n",
            "Epoch 138/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.2940 - accuracy: 0.4891 - val_loss: 1.2689 - val_accuracy: 0.5024\n",
            "Epoch 139/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.2987 - accuracy: 0.4776 - val_loss: 1.2601 - val_accuracy: 0.5556\n",
            "Epoch 140/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 1.2890 - accuracy: 0.4891 - val_loss: 1.2587 - val_accuracy: 0.5217\n",
            "Epoch 141/1000\n",
            "104/104 [==============================] - 1s 13ms/step - loss: 1.3079 - accuracy: 0.4571 - val_loss: 1.2606 - val_accuracy: 0.4976\n",
            "Epoch 142/1000\n",
            "104/104 [==============================] - 1s 13ms/step - loss: 1.2758 - accuracy: 0.4819 - val_loss: 1.2534 - val_accuracy: 0.5217\n",
            "Epoch 143/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 1.2980 - accuracy: 0.4849 - val_loss: 1.2549 - val_accuracy: 0.5024\n",
            "Epoch 144/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 1.2753 - accuracy: 0.4873 - val_loss: 1.2540 - val_accuracy: 0.5507\n",
            "Epoch 145/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 1.2823 - accuracy: 0.4764 - val_loss: 1.2440 - val_accuracy: 0.5169\n",
            "Epoch 146/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.2729 - accuracy: 0.4879 - val_loss: 1.2378 - val_accuracy: 0.5411\n",
            "Epoch 147/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.2604 - accuracy: 0.4813 - val_loss: 1.2403 - val_accuracy: 0.5314\n",
            "Epoch 148/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.2655 - accuracy: 0.4800 - val_loss: 1.2349 - val_accuracy: 0.5072\n",
            "Epoch 149/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.2570 - accuracy: 0.4879 - val_loss: 1.2254 - val_accuracy: 0.5314\n",
            "Epoch 150/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.2533 - accuracy: 0.4861 - val_loss: 1.2287 - val_accuracy: 0.5459\n",
            "Epoch 151/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.2266 - accuracy: 0.5079 - val_loss: 1.2236 - val_accuracy: 0.5314\n",
            "Epoch 152/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.2585 - accuracy: 0.4946 - val_loss: 1.2048 - val_accuracy: 0.5845\n",
            "Epoch 153/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.2499 - accuracy: 0.4819 - val_loss: 1.2000 - val_accuracy: 0.5700\n",
            "Epoch 154/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.2234 - accuracy: 0.5097 - val_loss: 1.1978 - val_accuracy: 0.5362\n",
            "Epoch 155/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.2407 - accuracy: 0.5042 - val_loss: 1.1964 - val_accuracy: 0.5556\n",
            "Epoch 156/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.2316 - accuracy: 0.5030 - val_loss: 1.1949 - val_accuracy: 0.5556\n",
            "Epoch 157/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.2039 - accuracy: 0.5193 - val_loss: 1.1963 - val_accuracy: 0.5652\n",
            "Epoch 158/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.2119 - accuracy: 0.5181 - val_loss: 1.2010 - val_accuracy: 0.5507\n",
            "Epoch 159/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.2140 - accuracy: 0.5145 - val_loss: 1.1942 - val_accuracy: 0.5121\n",
            "Epoch 160/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.2258 - accuracy: 0.5018 - val_loss: 1.1772 - val_accuracy: 0.5507\n",
            "Epoch 161/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.2089 - accuracy: 0.5097 - val_loss: 1.1741 - val_accuracy: 0.5362\n",
            "Epoch 162/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.2112 - accuracy: 0.5157 - val_loss: 1.1765 - val_accuracy: 0.5314\n",
            "Epoch 163/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.2033 - accuracy: 0.5206 - val_loss: 1.1652 - val_accuracy: 0.5507\n",
            "Epoch 164/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.2315 - accuracy: 0.5097 - val_loss: 1.1771 - val_accuracy: 0.5556\n",
            "Epoch 165/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.1979 - accuracy: 0.5236 - val_loss: 1.1719 - val_accuracy: 0.5459\n",
            "Epoch 166/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.1929 - accuracy: 0.5193 - val_loss: 1.1733 - val_accuracy: 0.5411\n",
            "Epoch 167/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.1888 - accuracy: 0.5242 - val_loss: 1.1597 - val_accuracy: 0.5845\n",
            "Epoch 168/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.1963 - accuracy: 0.5278 - val_loss: 1.1626 - val_accuracy: 0.5700\n",
            "Epoch 169/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.1963 - accuracy: 0.5079 - val_loss: 1.1492 - val_accuracy: 0.5556\n",
            "Epoch 170/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.1674 - accuracy: 0.5387 - val_loss: 1.1646 - val_accuracy: 0.5749\n",
            "Epoch 171/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.1615 - accuracy: 0.5127 - val_loss: 1.1462 - val_accuracy: 0.5942\n",
            "Epoch 172/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.1805 - accuracy: 0.5175 - val_loss: 1.1472 - val_accuracy: 0.5652\n",
            "Epoch 173/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.1746 - accuracy: 0.5224 - val_loss: 1.1359 - val_accuracy: 0.5459\n",
            "Epoch 174/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.1584 - accuracy: 0.5212 - val_loss: 1.1375 - val_accuracy: 0.5797\n",
            "Epoch 175/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.1666 - accuracy: 0.5375 - val_loss: 1.1497 - val_accuracy: 0.5604\n",
            "Epoch 176/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.1754 - accuracy: 0.5272 - val_loss: 1.1468 - val_accuracy: 0.5507\n",
            "Epoch 177/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.1426 - accuracy: 0.5393 - val_loss: 1.1385 - val_accuracy: 0.5459\n",
            "Epoch 178/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.1455 - accuracy: 0.5514 - val_loss: 1.1328 - val_accuracy: 0.5362\n",
            "Epoch 179/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.1370 - accuracy: 0.5405 - val_loss: 1.1235 - val_accuracy: 0.5749\n",
            "Epoch 180/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.1761 - accuracy: 0.5206 - val_loss: 1.1124 - val_accuracy: 0.5797\n",
            "Epoch 181/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.1386 - accuracy: 0.5411 - val_loss: 1.1216 - val_accuracy: 0.5556\n",
            "Epoch 182/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.1312 - accuracy: 0.5453 - val_loss: 1.1128 - val_accuracy: 0.5604\n",
            "Epoch 183/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.1681 - accuracy: 0.5139 - val_loss: 1.1286 - val_accuracy: 0.5700\n",
            "Epoch 184/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.1416 - accuracy: 0.5296 - val_loss: 1.1158 - val_accuracy: 0.5894\n",
            "Epoch 185/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.1201 - accuracy: 0.5478 - val_loss: 1.0993 - val_accuracy: 0.5652\n",
            "Epoch 186/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.1129 - accuracy: 0.5580 - val_loss: 1.1169 - val_accuracy: 0.5314\n",
            "Epoch 187/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.1433 - accuracy: 0.5429 - val_loss: 1.1099 - val_accuracy: 0.5845\n",
            "Epoch 188/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.1262 - accuracy: 0.5599 - val_loss: 1.1077 - val_accuracy: 0.5845\n",
            "Epoch 189/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.1112 - accuracy: 0.5599 - val_loss: 1.0990 - val_accuracy: 0.5942\n",
            "Epoch 190/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.1240 - accuracy: 0.5514 - val_loss: 1.0913 - val_accuracy: 0.6135\n",
            "Epoch 191/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.1166 - accuracy: 0.5478 - val_loss: 1.0938 - val_accuracy: 0.5797\n",
            "Epoch 192/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.1096 - accuracy: 0.5556 - val_loss: 1.0887 - val_accuracy: 0.6087\n",
            "Epoch 193/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.1022 - accuracy: 0.5580 - val_loss: 1.0806 - val_accuracy: 0.5797\n",
            "Epoch 194/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.1279 - accuracy: 0.5605 - val_loss: 1.1057 - val_accuracy: 0.5604\n",
            "Epoch 195/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.1228 - accuracy: 0.5478 - val_loss: 1.0883 - val_accuracy: 0.5845\n",
            "Epoch 196/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.1027 - accuracy: 0.5707 - val_loss: 1.0766 - val_accuracy: 0.6184\n",
            "Epoch 197/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.1006 - accuracy: 0.5538 - val_loss: 1.0722 - val_accuracy: 0.5990\n",
            "Epoch 198/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.0699 - accuracy: 0.5689 - val_loss: 1.0705 - val_accuracy: 0.5894\n",
            "Epoch 199/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.1075 - accuracy: 0.5599 - val_loss: 1.0684 - val_accuracy: 0.6087\n",
            "Epoch 200/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.0872 - accuracy: 0.5635 - val_loss: 1.0966 - val_accuracy: 0.5507\n",
            "Epoch 201/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.0898 - accuracy: 0.5647 - val_loss: 1.0827 - val_accuracy: 0.5652\n",
            "Epoch 202/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.1030 - accuracy: 0.5647 - val_loss: 1.0648 - val_accuracy: 0.6087\n",
            "Epoch 203/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.0656 - accuracy: 0.5719 - val_loss: 1.0661 - val_accuracy: 0.6039\n",
            "Epoch 204/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.0613 - accuracy: 0.5689 - val_loss: 1.0606 - val_accuracy: 0.6184\n",
            "Epoch 205/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.0850 - accuracy: 0.5689 - val_loss: 1.0638 - val_accuracy: 0.6087\n",
            "Epoch 206/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.0817 - accuracy: 0.5647 - val_loss: 1.0632 - val_accuracy: 0.6425\n",
            "Epoch 207/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.0827 - accuracy: 0.5599 - val_loss: 1.0498 - val_accuracy: 0.6280\n",
            "Epoch 208/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.0470 - accuracy: 0.5925 - val_loss: 1.0575 - val_accuracy: 0.5749\n",
            "Epoch 209/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.0635 - accuracy: 0.5798 - val_loss: 1.0534 - val_accuracy: 0.5797\n",
            "Epoch 210/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.0415 - accuracy: 0.5865 - val_loss: 1.0305 - val_accuracy: 0.6377\n",
            "Epoch 211/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0546 - accuracy: 0.5840 - val_loss: 1.0253 - val_accuracy: 0.6135\n",
            "Epoch 212/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.0354 - accuracy: 0.5967 - val_loss: 1.0235 - val_accuracy: 0.6087\n",
            "Epoch 213/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.0430 - accuracy: 0.5859 - val_loss: 1.0390 - val_accuracy: 0.6039\n",
            "Epoch 214/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.0482 - accuracy: 0.5738 - val_loss: 1.0308 - val_accuracy: 0.5942\n",
            "Epoch 215/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.0390 - accuracy: 0.5834 - val_loss: 1.0290 - val_accuracy: 0.6135\n",
            "Epoch 216/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.0422 - accuracy: 0.5774 - val_loss: 1.0373 - val_accuracy: 0.5942\n",
            "Epoch 217/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.0586 - accuracy: 0.5599 - val_loss: 1.0406 - val_accuracy: 0.5894\n",
            "Epoch 218/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.0369 - accuracy: 0.5889 - val_loss: 1.0401 - val_accuracy: 0.6618\n",
            "Epoch 219/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.0345 - accuracy: 0.6010 - val_loss: 1.0386 - val_accuracy: 0.6184\n",
            "Epoch 220/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.0306 - accuracy: 0.5816 - val_loss: 1.0262 - val_accuracy: 0.5942\n",
            "Epoch 221/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.0446 - accuracy: 0.5804 - val_loss: 1.0406 - val_accuracy: 0.5942\n",
            "Epoch 222/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.0273 - accuracy: 0.5877 - val_loss: 1.0170 - val_accuracy: 0.6280\n",
            "Epoch 223/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.0229 - accuracy: 0.5846 - val_loss: 1.0306 - val_accuracy: 0.5894\n",
            "Epoch 224/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.0326 - accuracy: 0.6016 - val_loss: 1.0180 - val_accuracy: 0.6377\n",
            "Epoch 225/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.0242 - accuracy: 0.5925 - val_loss: 1.0150 - val_accuracy: 0.5990\n",
            "Epoch 226/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.9997 - accuracy: 0.6052 - val_loss: 1.0146 - val_accuracy: 0.6522\n",
            "Epoch 227/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.0326 - accuracy: 0.5859 - val_loss: 1.0216 - val_accuracy: 0.6425\n",
            "Epoch 228/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.0059 - accuracy: 0.5973 - val_loss: 1.0073 - val_accuracy: 0.6377\n",
            "Epoch 229/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.0062 - accuracy: 0.5913 - val_loss: 1.0022 - val_accuracy: 0.6425\n",
            "Epoch 230/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.0032 - accuracy: 0.6052 - val_loss: 1.0155 - val_accuracy: 0.6184\n",
            "Epoch 231/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.0141 - accuracy: 0.5859 - val_loss: 1.0191 - val_accuracy: 0.6087\n",
            "Epoch 232/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.9931 - accuracy: 0.6106 - val_loss: 1.0112 - val_accuracy: 0.6473\n",
            "Epoch 233/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.9912 - accuracy: 0.6125 - val_loss: 0.9953 - val_accuracy: 0.6280\n",
            "Epoch 234/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.0025 - accuracy: 0.6094 - val_loss: 1.0009 - val_accuracy: 0.6280\n",
            "Epoch 235/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.0050 - accuracy: 0.6034 - val_loss: 1.0001 - val_accuracy: 0.6377\n",
            "Epoch 236/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.9853 - accuracy: 0.6191 - val_loss: 0.9900 - val_accuracy: 0.6184\n",
            "Epoch 237/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.0137 - accuracy: 0.6058 - val_loss: 1.0076 - val_accuracy: 0.6618\n",
            "Epoch 238/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.0003 - accuracy: 0.6004 - val_loss: 0.9973 - val_accuracy: 0.6425\n",
            "Epoch 239/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.9724 - accuracy: 0.6185 - val_loss: 0.9951 - val_accuracy: 0.6135\n",
            "Epoch 240/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.9936 - accuracy: 0.6070 - val_loss: 1.0310 - val_accuracy: 0.6039\n",
            "Epoch 241/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.9759 - accuracy: 0.6088 - val_loss: 0.9902 - val_accuracy: 0.6667\n",
            "Epoch 242/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.9842 - accuracy: 0.6028 - val_loss: 1.0025 - val_accuracy: 0.6232\n",
            "Epoch 243/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.9797 - accuracy: 0.6064 - val_loss: 0.9852 - val_accuracy: 0.6232\n",
            "Epoch 244/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.9593 - accuracy: 0.6203 - val_loss: 0.9851 - val_accuracy: 0.6425\n",
            "Epoch 245/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.9569 - accuracy: 0.6227 - val_loss: 0.9907 - val_accuracy: 0.6087\n",
            "Epoch 246/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0029 - accuracy: 0.5985 - val_loss: 0.9811 - val_accuracy: 0.6329\n",
            "Epoch 247/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.9563 - accuracy: 0.6252 - val_loss: 0.9938 - val_accuracy: 0.6087\n",
            "Epoch 248/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.9668 - accuracy: 0.6245 - val_loss: 0.9736 - val_accuracy: 0.6425\n",
            "Epoch 249/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.9586 - accuracy: 0.6137 - val_loss: 0.9651 - val_accuracy: 0.6570\n",
            "Epoch 250/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.9645 - accuracy: 0.6100 - val_loss: 0.9752 - val_accuracy: 0.6522\n",
            "Epoch 251/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.9705 - accuracy: 0.6119 - val_loss: 0.9676 - val_accuracy: 0.6425\n",
            "Epoch 252/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.9657 - accuracy: 0.6191 - val_loss: 0.9689 - val_accuracy: 0.6184\n",
            "Epoch 253/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.9732 - accuracy: 0.6022 - val_loss: 0.9936 - val_accuracy: 0.6280\n",
            "Epoch 254/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.9799 - accuracy: 0.6179 - val_loss: 0.9807 - val_accuracy: 0.6377\n",
            "Epoch 255/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.9516 - accuracy: 0.6197 - val_loss: 0.9649 - val_accuracy: 0.6377\n",
            "Epoch 256/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.9541 - accuracy: 0.6215 - val_loss: 0.9539 - val_accuracy: 0.6715\n",
            "Epoch 257/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.9521 - accuracy: 0.6119 - val_loss: 0.9531 - val_accuracy: 0.6377\n",
            "Epoch 258/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.9522 - accuracy: 0.6191 - val_loss: 0.9552 - val_accuracy: 0.6570\n",
            "Epoch 259/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9501 - accuracy: 0.6360 - val_loss: 0.9685 - val_accuracy: 0.6329\n",
            "Epoch 260/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.9423 - accuracy: 0.6191 - val_loss: 0.9481 - val_accuracy: 0.6232\n",
            "Epoch 261/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.9146 - accuracy: 0.6318 - val_loss: 0.9386 - val_accuracy: 0.6377\n",
            "Epoch 262/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.9326 - accuracy: 0.6245 - val_loss: 0.9458 - val_accuracy: 0.6473\n",
            "Epoch 263/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.9321 - accuracy: 0.6119 - val_loss: 0.9518 - val_accuracy: 0.6570\n",
            "Epoch 264/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.9115 - accuracy: 0.6427 - val_loss: 0.9369 - val_accuracy: 0.6522\n",
            "Epoch 265/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.9340 - accuracy: 0.6324 - val_loss: 0.9429 - val_accuracy: 0.6618\n",
            "Epoch 266/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.9535 - accuracy: 0.6239 - val_loss: 0.9336 - val_accuracy: 0.6812\n",
            "Epoch 267/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.9385 - accuracy: 0.6300 - val_loss: 0.9464 - val_accuracy: 0.6618\n",
            "Epoch 268/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.9142 - accuracy: 0.6463 - val_loss: 0.9350 - val_accuracy: 0.6618\n",
            "Epoch 269/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.9533 - accuracy: 0.6149 - val_loss: 0.9292 - val_accuracy: 0.6425\n",
            "Epoch 270/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9267 - accuracy: 0.6300 - val_loss: 0.9325 - val_accuracy: 0.6667\n",
            "Epoch 271/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.9139 - accuracy: 0.6415 - val_loss: 0.9371 - val_accuracy: 0.6377\n",
            "Epoch 272/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.9161 - accuracy: 0.6518 - val_loss: 0.9368 - val_accuracy: 0.6667\n",
            "Epoch 273/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.9140 - accuracy: 0.6342 - val_loss: 0.9365 - val_accuracy: 0.6280\n",
            "Epoch 274/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.9230 - accuracy: 0.6342 - val_loss: 0.9396 - val_accuracy: 0.6763\n",
            "Epoch 275/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.9006 - accuracy: 0.6397 - val_loss: 0.9273 - val_accuracy: 0.6715\n",
            "Epoch 276/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.9114 - accuracy: 0.6391 - val_loss: 0.9221 - val_accuracy: 0.6667\n",
            "Epoch 277/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.9103 - accuracy: 0.6372 - val_loss: 0.9213 - val_accuracy: 0.6522\n",
            "Epoch 278/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.9171 - accuracy: 0.6427 - val_loss: 0.9329 - val_accuracy: 0.6522\n",
            "Epoch 279/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.9222 - accuracy: 0.6385 - val_loss: 0.9296 - val_accuracy: 0.6184\n",
            "Epoch 280/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.9063 - accuracy: 0.6463 - val_loss: 0.9240 - val_accuracy: 0.6570\n",
            "Epoch 281/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.9011 - accuracy: 0.6487 - val_loss: 0.9148 - val_accuracy: 0.6522\n",
            "Epoch 282/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8932 - accuracy: 0.6536 - val_loss: 0.9290 - val_accuracy: 0.6377\n",
            "Epoch 283/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.9032 - accuracy: 0.6354 - val_loss: 0.9195 - val_accuracy: 0.6715\n",
            "Epoch 284/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.8805 - accuracy: 0.6463 - val_loss: 0.9191 - val_accuracy: 0.6667\n",
            "Epoch 285/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.9034 - accuracy: 0.6312 - val_loss: 0.9216 - val_accuracy: 0.6812\n",
            "Epoch 286/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.9046 - accuracy: 0.6475 - val_loss: 0.8998 - val_accuracy: 0.6812\n",
            "Epoch 287/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8757 - accuracy: 0.6620 - val_loss: 0.9175 - val_accuracy: 0.6618\n",
            "Epoch 288/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.8950 - accuracy: 0.6524 - val_loss: 0.9024 - val_accuracy: 0.6763\n",
            "Epoch 289/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8841 - accuracy: 0.6548 - val_loss: 0.8991 - val_accuracy: 0.6667\n",
            "Epoch 290/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.9019 - accuracy: 0.6385 - val_loss: 0.9062 - val_accuracy: 0.6957\n",
            "Epoch 291/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.9055 - accuracy: 0.6475 - val_loss: 0.8995 - val_accuracy: 0.6957\n",
            "Epoch 292/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8927 - accuracy: 0.6524 - val_loss: 0.9124 - val_accuracy: 0.6522\n",
            "Epoch 293/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.8710 - accuracy: 0.6632 - val_loss: 0.9041 - val_accuracy: 0.6908\n",
            "Epoch 294/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.8931 - accuracy: 0.6421 - val_loss: 0.9108 - val_accuracy: 0.6860\n",
            "Epoch 295/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8854 - accuracy: 0.6457 - val_loss: 0.8868 - val_accuracy: 0.6715\n",
            "Epoch 296/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8669 - accuracy: 0.6614 - val_loss: 0.9130 - val_accuracy: 0.7005\n",
            "Epoch 297/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8629 - accuracy: 0.6524 - val_loss: 0.8900 - val_accuracy: 0.6618\n",
            "Epoch 298/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8643 - accuracy: 0.6663 - val_loss: 0.8932 - val_accuracy: 0.6957\n",
            "Epoch 299/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8576 - accuracy: 0.6584 - val_loss: 0.9001 - val_accuracy: 0.6522\n",
            "Epoch 300/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.8846 - accuracy: 0.6487 - val_loss: 0.8926 - val_accuracy: 0.6812\n",
            "Epoch 301/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8602 - accuracy: 0.6584 - val_loss: 0.8954 - val_accuracy: 0.6667\n",
            "Epoch 302/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8675 - accuracy: 0.6602 - val_loss: 0.9007 - val_accuracy: 0.6667\n",
            "Epoch 303/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8727 - accuracy: 0.6572 - val_loss: 0.8870 - val_accuracy: 0.6715\n",
            "Epoch 304/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.8576 - accuracy: 0.6602 - val_loss: 0.9081 - val_accuracy: 0.6860\n",
            "Epoch 305/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8456 - accuracy: 0.6790 - val_loss: 0.8891 - val_accuracy: 0.6957\n",
            "Epoch 306/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8523 - accuracy: 0.6711 - val_loss: 0.9121 - val_accuracy: 0.6618\n",
            "Epoch 307/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.8667 - accuracy: 0.6469 - val_loss: 0.8911 - val_accuracy: 0.6812\n",
            "Epoch 308/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8436 - accuracy: 0.6675 - val_loss: 0.8757 - val_accuracy: 0.6957\n",
            "Epoch 309/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.8593 - accuracy: 0.6596 - val_loss: 0.8830 - val_accuracy: 0.6908\n",
            "Epoch 310/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8651 - accuracy: 0.6614 - val_loss: 0.8767 - val_accuracy: 0.6812\n",
            "Epoch 311/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8519 - accuracy: 0.6705 - val_loss: 0.8927 - val_accuracy: 0.6812\n",
            "Epoch 312/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.8315 - accuracy: 0.6566 - val_loss: 0.8932 - val_accuracy: 0.6860\n",
            "Epoch 313/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.8455 - accuracy: 0.6802 - val_loss: 0.8937 - val_accuracy: 0.6763\n",
            "Epoch 314/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8363 - accuracy: 0.6759 - val_loss: 0.8706 - val_accuracy: 0.7005\n",
            "Epoch 315/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8499 - accuracy: 0.6584 - val_loss: 0.8635 - val_accuracy: 0.6957\n",
            "Epoch 316/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8269 - accuracy: 0.6759 - val_loss: 0.8707 - val_accuracy: 0.6908\n",
            "Epoch 317/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8336 - accuracy: 0.6729 - val_loss: 0.8528 - val_accuracy: 0.7053\n",
            "Epoch 318/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.8283 - accuracy: 0.6747 - val_loss: 0.8509 - val_accuracy: 0.6908\n",
            "Epoch 319/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.8411 - accuracy: 0.6596 - val_loss: 0.8756 - val_accuracy: 0.6860\n",
            "Epoch 320/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.8446 - accuracy: 0.6632 - val_loss: 0.8758 - val_accuracy: 0.6715\n",
            "Epoch 321/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.8511 - accuracy: 0.6578 - val_loss: 0.8678 - val_accuracy: 0.6667\n",
            "Epoch 322/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8236 - accuracy: 0.6590 - val_loss: 0.8590 - val_accuracy: 0.6763\n",
            "Epoch 323/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.8291 - accuracy: 0.6536 - val_loss: 0.8760 - val_accuracy: 0.6812\n",
            "Epoch 324/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7988 - accuracy: 0.6874 - val_loss: 0.8645 - val_accuracy: 0.6715\n",
            "Epoch 325/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8211 - accuracy: 0.6675 - val_loss: 0.8657 - val_accuracy: 0.6763\n",
            "Epoch 326/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8307 - accuracy: 0.6808 - val_loss: 0.8835 - val_accuracy: 0.6957\n",
            "Epoch 327/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8231 - accuracy: 0.6651 - val_loss: 0.8647 - val_accuracy: 0.6570\n",
            "Epoch 328/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.8109 - accuracy: 0.6644 - val_loss: 0.8609 - val_accuracy: 0.6957\n",
            "Epoch 329/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8469 - accuracy: 0.6626 - val_loss: 0.8553 - val_accuracy: 0.6860\n",
            "Epoch 330/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.8381 - accuracy: 0.6711 - val_loss: 0.8571 - val_accuracy: 0.7198\n",
            "Epoch 331/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.8290 - accuracy: 0.6681 - val_loss: 0.8550 - val_accuracy: 0.6812\n",
            "Epoch 332/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.8017 - accuracy: 0.6850 - val_loss: 0.8565 - val_accuracy: 0.6715\n",
            "Epoch 333/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8107 - accuracy: 0.6832 - val_loss: 0.8352 - val_accuracy: 0.6908\n",
            "Epoch 334/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.8197 - accuracy: 0.6681 - val_loss: 0.8495 - val_accuracy: 0.6908\n",
            "Epoch 335/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8111 - accuracy: 0.6820 - val_loss: 0.8485 - val_accuracy: 0.6812\n",
            "Epoch 336/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8339 - accuracy: 0.6717 - val_loss: 0.8495 - val_accuracy: 0.6763\n",
            "Epoch 337/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7831 - accuracy: 0.6868 - val_loss: 0.8540 - val_accuracy: 0.6908\n",
            "Epoch 338/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7852 - accuracy: 0.6953 - val_loss: 0.8508 - val_accuracy: 0.7005\n",
            "Epoch 339/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7942 - accuracy: 0.6856 - val_loss: 0.8567 - val_accuracy: 0.6715\n",
            "Epoch 340/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8066 - accuracy: 0.6693 - val_loss: 0.8430 - val_accuracy: 0.6908\n",
            "Epoch 341/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8112 - accuracy: 0.6663 - val_loss: 0.8294 - val_accuracy: 0.6957\n",
            "Epoch 342/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8069 - accuracy: 0.6778 - val_loss: 0.8405 - val_accuracy: 0.6908\n",
            "Epoch 343/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7824 - accuracy: 0.6838 - val_loss: 0.8415 - val_accuracy: 0.6763\n",
            "Epoch 344/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7926 - accuracy: 0.6844 - val_loss: 0.8452 - val_accuracy: 0.7198\n",
            "Epoch 345/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7899 - accuracy: 0.6808 - val_loss: 0.8370 - val_accuracy: 0.6715\n",
            "Epoch 346/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7786 - accuracy: 0.6995 - val_loss: 0.8299 - val_accuracy: 0.6812\n",
            "Epoch 347/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8059 - accuracy: 0.6802 - val_loss: 0.8249 - val_accuracy: 0.7101\n",
            "Epoch 348/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7943 - accuracy: 0.6832 - val_loss: 0.8432 - val_accuracy: 0.7053\n",
            "Epoch 349/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7974 - accuracy: 0.6838 - val_loss: 0.8237 - val_accuracy: 0.7053\n",
            "Epoch 350/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7878 - accuracy: 0.6874 - val_loss: 0.8222 - val_accuracy: 0.7101\n",
            "Epoch 351/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7919 - accuracy: 0.6935 - val_loss: 0.8192 - val_accuracy: 0.7005\n",
            "Epoch 352/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7801 - accuracy: 0.6995 - val_loss: 0.8170 - val_accuracy: 0.6957\n",
            "Epoch 353/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7917 - accuracy: 0.6814 - val_loss: 0.8378 - val_accuracy: 0.7101\n",
            "Epoch 354/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7845 - accuracy: 0.6935 - val_loss: 0.8159 - val_accuracy: 0.6957\n",
            "Epoch 355/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.7804 - accuracy: 0.6917 - val_loss: 0.8180 - val_accuracy: 0.6860\n",
            "Epoch 356/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7818 - accuracy: 0.6856 - val_loss: 0.8176 - val_accuracy: 0.7101\n",
            "Epoch 357/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7793 - accuracy: 0.6790 - val_loss: 0.8106 - val_accuracy: 0.7150\n",
            "Epoch 358/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8129 - accuracy: 0.6874 - val_loss: 0.8241 - val_accuracy: 0.6860\n",
            "Epoch 359/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7746 - accuracy: 0.6874 - val_loss: 0.8125 - val_accuracy: 0.7198\n",
            "Epoch 360/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7671 - accuracy: 0.6977 - val_loss: 0.8149 - val_accuracy: 0.7198\n",
            "Epoch 361/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7579 - accuracy: 0.6886 - val_loss: 0.8291 - val_accuracy: 0.6957\n",
            "Epoch 362/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7781 - accuracy: 0.6898 - val_loss: 0.8267 - val_accuracy: 0.7295\n",
            "Epoch 363/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.7741 - accuracy: 0.7031 - val_loss: 0.8270 - val_accuracy: 0.6908\n",
            "Epoch 364/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7652 - accuracy: 0.6929 - val_loss: 0.8123 - val_accuracy: 0.7005\n",
            "Epoch 365/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.7823 - accuracy: 0.6832 - val_loss: 0.8186 - val_accuracy: 0.7150\n",
            "Epoch 366/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7548 - accuracy: 0.7152 - val_loss: 0.8035 - val_accuracy: 0.7101\n",
            "Epoch 367/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7850 - accuracy: 0.6790 - val_loss: 0.8136 - val_accuracy: 0.7101\n",
            "Epoch 368/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7936 - accuracy: 0.6784 - val_loss: 0.8074 - val_accuracy: 0.7053\n",
            "Epoch 369/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.7688 - accuracy: 0.7007 - val_loss: 0.8062 - val_accuracy: 0.6908\n",
            "Epoch 370/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7870 - accuracy: 0.6911 - val_loss: 0.8112 - val_accuracy: 0.7295\n",
            "Epoch 371/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.7538 - accuracy: 0.7086 - val_loss: 0.7945 - val_accuracy: 0.7246\n",
            "Epoch 372/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7495 - accuracy: 0.7050 - val_loss: 0.8177 - val_accuracy: 0.7005\n",
            "Epoch 373/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7631 - accuracy: 0.7098 - val_loss: 0.8078 - val_accuracy: 0.7005\n",
            "Epoch 374/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.7574 - accuracy: 0.7007 - val_loss: 0.7929 - val_accuracy: 0.7198\n",
            "Epoch 375/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.7375 - accuracy: 0.7140 - val_loss: 0.8006 - val_accuracy: 0.7053\n",
            "Epoch 376/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.7607 - accuracy: 0.7146 - val_loss: 0.7892 - val_accuracy: 0.7343\n",
            "Epoch 377/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.7490 - accuracy: 0.7110 - val_loss: 0.7871 - val_accuracy: 0.7101\n",
            "Epoch 378/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7733 - accuracy: 0.6941 - val_loss: 0.7861 - val_accuracy: 0.7198\n",
            "Epoch 379/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7756 - accuracy: 0.6935 - val_loss: 0.8040 - val_accuracy: 0.7053\n",
            "Epoch 380/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.7759 - accuracy: 0.6892 - val_loss: 0.7990 - val_accuracy: 0.7101\n",
            "Epoch 381/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7699 - accuracy: 0.7007 - val_loss: 0.8089 - val_accuracy: 0.6957\n",
            "Epoch 382/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.7493 - accuracy: 0.7080 - val_loss: 0.7949 - val_accuracy: 0.7150\n",
            "Epoch 383/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7358 - accuracy: 0.7116 - val_loss: 0.8033 - val_accuracy: 0.7343\n",
            "Epoch 384/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7432 - accuracy: 0.7104 - val_loss: 0.7892 - val_accuracy: 0.7198\n",
            "Epoch 385/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7587 - accuracy: 0.7025 - val_loss: 0.8175 - val_accuracy: 0.6908\n",
            "Epoch 386/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7584 - accuracy: 0.6989 - val_loss: 0.7937 - val_accuracy: 0.7005\n",
            "Epoch 387/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7395 - accuracy: 0.7158 - val_loss: 0.7948 - val_accuracy: 0.7246\n",
            "Epoch 388/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7315 - accuracy: 0.7128 - val_loss: 0.8054 - val_accuracy: 0.7053\n",
            "Epoch 389/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7149 - accuracy: 0.7146 - val_loss: 0.8006 - val_accuracy: 0.7101\n",
            "Epoch 390/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7474 - accuracy: 0.7207 - val_loss: 0.7988 - val_accuracy: 0.7101\n",
            "Epoch 391/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7325 - accuracy: 0.7019 - val_loss: 0.8091 - val_accuracy: 0.7053\n",
            "Epoch 392/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7340 - accuracy: 0.7170 - val_loss: 0.7842 - val_accuracy: 0.7101\n",
            "Epoch 393/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7551 - accuracy: 0.6953 - val_loss: 0.7932 - val_accuracy: 0.7101\n",
            "Epoch 394/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7065 - accuracy: 0.7249 - val_loss: 0.7825 - val_accuracy: 0.7246\n",
            "Epoch 395/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7340 - accuracy: 0.7219 - val_loss: 0.7913 - val_accuracy: 0.7198\n",
            "Epoch 396/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 0.7379 - accuracy: 0.7164 - val_loss: 0.7939 - val_accuracy: 0.7246\n",
            "Epoch 397/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7367 - accuracy: 0.7110 - val_loss: 0.7927 - val_accuracy: 0.6812\n",
            "Epoch 398/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7365 - accuracy: 0.7086 - val_loss: 0.7898 - val_accuracy: 0.7150\n",
            "Epoch 399/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7290 - accuracy: 0.7031 - val_loss: 0.7921 - val_accuracy: 0.7005\n",
            "Epoch 400/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7143 - accuracy: 0.7213 - val_loss: 0.7676 - val_accuracy: 0.7391\n",
            "Epoch 401/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7259 - accuracy: 0.7110 - val_loss: 0.7785 - val_accuracy: 0.6957\n",
            "Epoch 402/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7303 - accuracy: 0.7080 - val_loss: 0.7791 - val_accuracy: 0.7246\n",
            "Epoch 403/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7328 - accuracy: 0.7177 - val_loss: 0.7816 - val_accuracy: 0.7343\n",
            "Epoch 404/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7305 - accuracy: 0.7146 - val_loss: 0.7790 - val_accuracy: 0.7246\n",
            "Epoch 405/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7140 - accuracy: 0.7273 - val_loss: 0.7821 - val_accuracy: 0.7343\n",
            "Epoch 406/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7265 - accuracy: 0.7050 - val_loss: 0.7837 - val_accuracy: 0.7150\n",
            "Epoch 407/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7119 - accuracy: 0.7195 - val_loss: 0.7834 - val_accuracy: 0.7101\n",
            "Epoch 408/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7128 - accuracy: 0.7183 - val_loss: 0.7811 - val_accuracy: 0.7295\n",
            "Epoch 409/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7137 - accuracy: 0.7092 - val_loss: 0.7844 - val_accuracy: 0.7150\n",
            "Epoch 410/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7117 - accuracy: 0.7213 - val_loss: 0.7757 - val_accuracy: 0.7150\n",
            "Epoch 411/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.7039 - accuracy: 0.7122 - val_loss: 0.7735 - val_accuracy: 0.7488\n",
            "Epoch 412/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7096 - accuracy: 0.7122 - val_loss: 0.7706 - val_accuracy: 0.7246\n",
            "Epoch 413/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7251 - accuracy: 0.7098 - val_loss: 0.7702 - val_accuracy: 0.7198\n",
            "Epoch 414/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7291 - accuracy: 0.6995 - val_loss: 0.7724 - val_accuracy: 0.7391\n",
            "Epoch 415/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7201 - accuracy: 0.7086 - val_loss: 0.7756 - val_accuracy: 0.7295\n",
            "Epoch 416/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7186 - accuracy: 0.7104 - val_loss: 0.7728 - val_accuracy: 0.7536\n",
            "Epoch 417/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7107 - accuracy: 0.7110 - val_loss: 0.7686 - val_accuracy: 0.7295\n",
            "Epoch 418/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7106 - accuracy: 0.7068 - val_loss: 0.7602 - val_accuracy: 0.7488\n",
            "Epoch 419/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7052 - accuracy: 0.7273 - val_loss: 0.7660 - val_accuracy: 0.7440\n",
            "Epoch 420/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6992 - accuracy: 0.7237 - val_loss: 0.7703 - val_accuracy: 0.7343\n",
            "Epoch 421/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7195 - accuracy: 0.7146 - val_loss: 0.7706 - val_accuracy: 0.7343\n",
            "Epoch 422/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6731 - accuracy: 0.7382 - val_loss: 0.7795 - val_accuracy: 0.7343\n",
            "Epoch 423/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6827 - accuracy: 0.7310 - val_loss: 0.7763 - val_accuracy: 0.7246\n",
            "Epoch 424/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6974 - accuracy: 0.7086 - val_loss: 0.7738 - val_accuracy: 0.7440\n",
            "Epoch 425/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6937 - accuracy: 0.7285 - val_loss: 0.7892 - val_accuracy: 0.7198\n",
            "Epoch 426/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.6788 - accuracy: 0.7291 - val_loss: 0.7768 - val_accuracy: 0.7198\n",
            "Epoch 427/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.6809 - accuracy: 0.7370 - val_loss: 0.7604 - val_accuracy: 0.7101\n",
            "Epoch 428/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.6924 - accuracy: 0.7297 - val_loss: 0.7701 - val_accuracy: 0.7391\n",
            "Epoch 429/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6838 - accuracy: 0.7437 - val_loss: 0.7584 - val_accuracy: 0.7246\n",
            "Epoch 430/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6949 - accuracy: 0.7237 - val_loss: 0.7627 - val_accuracy: 0.7101\n",
            "Epoch 431/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.6789 - accuracy: 0.7279 - val_loss: 0.7901 - val_accuracy: 0.7150\n",
            "Epoch 432/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6918 - accuracy: 0.7213 - val_loss: 0.7652 - val_accuracy: 0.7391\n",
            "Epoch 433/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6934 - accuracy: 0.7146 - val_loss: 0.7665 - val_accuracy: 0.7150\n",
            "Epoch 434/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6855 - accuracy: 0.7219 - val_loss: 0.7471 - val_accuracy: 0.7391\n",
            "Epoch 435/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6883 - accuracy: 0.7304 - val_loss: 0.7672 - val_accuracy: 0.7295\n",
            "Epoch 436/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6859 - accuracy: 0.7316 - val_loss: 0.7532 - val_accuracy: 0.7391\n",
            "Epoch 437/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.6764 - accuracy: 0.7177 - val_loss: 0.7606 - val_accuracy: 0.7295\n",
            "Epoch 438/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.6539 - accuracy: 0.7406 - val_loss: 0.7515 - val_accuracy: 0.7391\n",
            "Epoch 439/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.6814 - accuracy: 0.7189 - val_loss: 0.7482 - val_accuracy: 0.7391\n",
            "Epoch 440/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.6727 - accuracy: 0.7400 - val_loss: 0.7550 - val_accuracy: 0.7391\n",
            "Epoch 441/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.6575 - accuracy: 0.7340 - val_loss: 0.7449 - val_accuracy: 0.7246\n",
            "Epoch 442/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6720 - accuracy: 0.7346 - val_loss: 0.7499 - val_accuracy: 0.7391\n",
            "Epoch 443/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.6687 - accuracy: 0.7533 - val_loss: 0.7575 - val_accuracy: 0.7536\n",
            "Epoch 444/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6744 - accuracy: 0.7304 - val_loss: 0.7513 - val_accuracy: 0.7343\n",
            "Epoch 445/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.6840 - accuracy: 0.7261 - val_loss: 0.7581 - val_accuracy: 0.7440\n",
            "Epoch 446/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.6596 - accuracy: 0.7382 - val_loss: 0.7461 - val_accuracy: 0.7246\n",
            "Epoch 447/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6736 - accuracy: 0.7285 - val_loss: 0.7656 - val_accuracy: 0.6908\n",
            "Epoch 448/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6663 - accuracy: 0.7352 - val_loss: 0.7566 - val_accuracy: 0.7488\n",
            "Epoch 449/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6583 - accuracy: 0.7424 - val_loss: 0.7461 - val_accuracy: 0.7101\n",
            "Epoch 450/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6690 - accuracy: 0.7273 - val_loss: 0.7511 - val_accuracy: 0.7295\n",
            "Epoch 451/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6696 - accuracy: 0.7388 - val_loss: 0.7556 - val_accuracy: 0.7440\n",
            "Epoch 452/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6599 - accuracy: 0.7509 - val_loss: 0.7602 - val_accuracy: 0.7440\n",
            "Epoch 453/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6748 - accuracy: 0.7418 - val_loss: 0.7506 - val_accuracy: 0.7391\n",
            "Epoch 454/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6734 - accuracy: 0.7424 - val_loss: 0.7627 - val_accuracy: 0.7391\n",
            "Epoch 455/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6592 - accuracy: 0.7310 - val_loss: 0.7624 - val_accuracy: 0.7198\n",
            "Epoch 456/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.6828 - accuracy: 0.7213 - val_loss: 0.7580 - val_accuracy: 0.7295\n",
            "Epoch 457/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.6554 - accuracy: 0.7588 - val_loss: 0.7630 - val_accuracy: 0.7585\n",
            "Epoch 458/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6617 - accuracy: 0.7279 - val_loss: 0.7529 - val_accuracy: 0.7391\n",
            "Epoch 459/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.6628 - accuracy: 0.7485 - val_loss: 0.7571 - val_accuracy: 0.7246\n",
            "Epoch 460/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.6248 - accuracy: 0.7400 - val_loss: 0.7410 - val_accuracy: 0.7440\n",
            "Epoch 461/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6462 - accuracy: 0.7449 - val_loss: 0.7465 - val_accuracy: 0.7343\n",
            "Epoch 462/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6489 - accuracy: 0.7418 - val_loss: 0.7270 - val_accuracy: 0.7391\n",
            "Epoch 463/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6482 - accuracy: 0.7443 - val_loss: 0.7396 - val_accuracy: 0.7585\n",
            "Epoch 464/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.6524 - accuracy: 0.7340 - val_loss: 0.7355 - val_accuracy: 0.7391\n",
            "Epoch 465/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6536 - accuracy: 0.7430 - val_loss: 0.7502 - val_accuracy: 0.7246\n",
            "Epoch 466/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.6531 - accuracy: 0.7382 - val_loss: 0.7459 - val_accuracy: 0.7343\n",
            "Epoch 467/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.6531 - accuracy: 0.7406 - val_loss: 0.7475 - val_accuracy: 0.7488\n",
            "Epoch 468/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6547 - accuracy: 0.7382 - val_loss: 0.7359 - val_accuracy: 0.7440\n",
            "Epoch 469/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6361 - accuracy: 0.7449 - val_loss: 0.7490 - val_accuracy: 0.7488\n",
            "Epoch 470/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6467 - accuracy: 0.7443 - val_loss: 0.7508 - val_accuracy: 0.7391\n",
            "Epoch 471/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.6479 - accuracy: 0.7467 - val_loss: 0.7469 - val_accuracy: 0.7391\n",
            "Epoch 472/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.6390 - accuracy: 0.7455 - val_loss: 0.7323 - val_accuracy: 0.7391\n",
            "Epoch 473/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6507 - accuracy: 0.7376 - val_loss: 0.7394 - val_accuracy: 0.7488\n",
            "Epoch 474/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.6387 - accuracy: 0.7563 - val_loss: 0.7550 - val_accuracy: 0.7488\n",
            "Epoch 475/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6339 - accuracy: 0.7473 - val_loss: 0.7386 - val_accuracy: 0.7633\n",
            "Epoch 476/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.6274 - accuracy: 0.7485 - val_loss: 0.7363 - val_accuracy: 0.7440\n",
            "Epoch 477/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6396 - accuracy: 0.7497 - val_loss: 0.7272 - val_accuracy: 0.7391\n",
            "Epoch 478/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6235 - accuracy: 0.7642 - val_loss: 0.7337 - val_accuracy: 0.7440\n",
            "Epoch 479/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6267 - accuracy: 0.7509 - val_loss: 0.7387 - val_accuracy: 0.7440\n",
            "Epoch 480/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6548 - accuracy: 0.7328 - val_loss: 0.7332 - val_accuracy: 0.7440\n",
            "Epoch 481/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.6216 - accuracy: 0.7588 - val_loss: 0.7321 - val_accuracy: 0.7585\n",
            "Epoch 482/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6468 - accuracy: 0.7418 - val_loss: 0.7353 - val_accuracy: 0.7536\n",
            "Epoch 483/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.6229 - accuracy: 0.7557 - val_loss: 0.7162 - val_accuracy: 0.7536\n",
            "Epoch 484/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6245 - accuracy: 0.7545 - val_loss: 0.7156 - val_accuracy: 0.7488\n",
            "Epoch 485/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.6423 - accuracy: 0.7467 - val_loss: 0.7364 - val_accuracy: 0.7391\n",
            "Epoch 486/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6299 - accuracy: 0.7563 - val_loss: 0.7259 - val_accuracy: 0.7440\n",
            "Epoch 487/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6155 - accuracy: 0.7509 - val_loss: 0.7205 - val_accuracy: 0.7488\n",
            "Epoch 488/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6316 - accuracy: 0.7551 - val_loss: 0.7278 - val_accuracy: 0.7488\n",
            "Epoch 489/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6228 - accuracy: 0.7491 - val_loss: 0.7300 - val_accuracy: 0.7440\n",
            "Epoch 490/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6218 - accuracy: 0.7449 - val_loss: 0.7330 - val_accuracy: 0.7391\n",
            "Epoch 491/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6281 - accuracy: 0.7612 - val_loss: 0.7335 - val_accuracy: 0.7488\n",
            "Epoch 492/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6316 - accuracy: 0.7352 - val_loss: 0.7532 - val_accuracy: 0.7005\n",
            "Epoch 493/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6195 - accuracy: 0.7660 - val_loss: 0.7213 - val_accuracy: 0.7488\n",
            "Epoch 494/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.6119 - accuracy: 0.7563 - val_loss: 0.7176 - val_accuracy: 0.7391\n",
            "Epoch 495/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.6288 - accuracy: 0.7563 - val_loss: 0.7403 - val_accuracy: 0.7198\n",
            "Epoch 496/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6236 - accuracy: 0.7497 - val_loss: 0.7259 - val_accuracy: 0.7633\n",
            "Epoch 497/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6221 - accuracy: 0.7642 - val_loss: 0.7369 - val_accuracy: 0.7536\n",
            "Epoch 498/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6169 - accuracy: 0.7551 - val_loss: 0.7150 - val_accuracy: 0.7440\n",
            "Epoch 499/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6104 - accuracy: 0.7503 - val_loss: 0.7360 - val_accuracy: 0.7343\n",
            "Epoch 500/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.6261 - accuracy: 0.7576 - val_loss: 0.7270 - val_accuracy: 0.7391\n",
            "Epoch 501/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6003 - accuracy: 0.7696 - val_loss: 0.7301 - val_accuracy: 0.7440\n",
            "Epoch 502/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.6184 - accuracy: 0.7527 - val_loss: 0.7272 - val_accuracy: 0.7391\n",
            "Epoch 503/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.6258 - accuracy: 0.7630 - val_loss: 0.7344 - val_accuracy: 0.7536\n",
            "Epoch 504/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5873 - accuracy: 0.7612 - val_loss: 0.7272 - val_accuracy: 0.7536\n",
            "Epoch 505/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5977 - accuracy: 0.7648 - val_loss: 0.7261 - val_accuracy: 0.7585\n",
            "Epoch 506/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5969 - accuracy: 0.7684 - val_loss: 0.7090 - val_accuracy: 0.7440\n",
            "Epoch 507/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.6116 - accuracy: 0.7684 - val_loss: 0.7172 - val_accuracy: 0.7633\n",
            "Epoch 508/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5845 - accuracy: 0.7745 - val_loss: 0.7117 - val_accuracy: 0.7391\n",
            "Epoch 509/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6232 - accuracy: 0.7539 - val_loss: 0.7188 - val_accuracy: 0.7681\n",
            "Epoch 510/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6180 - accuracy: 0.7630 - val_loss: 0.7134 - val_accuracy: 0.7633\n",
            "Epoch 511/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.5902 - accuracy: 0.7636 - val_loss: 0.7286 - val_accuracy: 0.7246\n",
            "Epoch 512/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6005 - accuracy: 0.7648 - val_loss: 0.7111 - val_accuracy: 0.7585\n",
            "Epoch 513/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.6218 - accuracy: 0.7509 - val_loss: 0.7088 - val_accuracy: 0.7729\n",
            "Epoch 514/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.5964 - accuracy: 0.7751 - val_loss: 0.7163 - val_accuracy: 0.7440\n",
            "Epoch 515/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6016 - accuracy: 0.7745 - val_loss: 0.7197 - val_accuracy: 0.7246\n",
            "Epoch 516/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.5876 - accuracy: 0.7757 - val_loss: 0.7039 - val_accuracy: 0.7488\n",
            "Epoch 517/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6027 - accuracy: 0.7709 - val_loss: 0.7071 - val_accuracy: 0.7295\n",
            "Epoch 518/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6062 - accuracy: 0.7551 - val_loss: 0.7142 - val_accuracy: 0.7585\n",
            "Epoch 519/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.6032 - accuracy: 0.7557 - val_loss: 0.7152 - val_accuracy: 0.7585\n",
            "Epoch 520/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.5998 - accuracy: 0.7642 - val_loss: 0.7323 - val_accuracy: 0.7343\n",
            "Epoch 521/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5998 - accuracy: 0.7703 - val_loss: 0.7084 - val_accuracy: 0.7536\n",
            "Epoch 522/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5939 - accuracy: 0.7606 - val_loss: 0.7063 - val_accuracy: 0.7391\n",
            "Epoch 523/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5902 - accuracy: 0.7696 - val_loss: 0.7071 - val_accuracy: 0.7536\n",
            "Epoch 524/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5848 - accuracy: 0.7733 - val_loss: 0.7051 - val_accuracy: 0.7585\n",
            "Epoch 525/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5824 - accuracy: 0.7630 - val_loss: 0.7106 - val_accuracy: 0.7536\n",
            "Epoch 526/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5921 - accuracy: 0.7594 - val_loss: 0.6986 - val_accuracy: 0.7585\n",
            "Epoch 527/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5980 - accuracy: 0.7570 - val_loss: 0.7055 - val_accuracy: 0.7536\n",
            "Epoch 528/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5977 - accuracy: 0.7600 - val_loss: 0.7158 - val_accuracy: 0.7488\n",
            "Epoch 529/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5993 - accuracy: 0.7666 - val_loss: 0.7238 - val_accuracy: 0.7295\n",
            "Epoch 530/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5696 - accuracy: 0.7799 - val_loss: 0.6947 - val_accuracy: 0.7729\n",
            "Epoch 531/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.5755 - accuracy: 0.7805 - val_loss: 0.6989 - val_accuracy: 0.7391\n",
            "Epoch 532/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5933 - accuracy: 0.7648 - val_loss: 0.7023 - val_accuracy: 0.7585\n",
            "Epoch 533/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5723 - accuracy: 0.7793 - val_loss: 0.7170 - val_accuracy: 0.7488\n",
            "Epoch 534/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5801 - accuracy: 0.7727 - val_loss: 0.6984 - val_accuracy: 0.7488\n",
            "Epoch 535/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5745 - accuracy: 0.7727 - val_loss: 0.7139 - val_accuracy: 0.7391\n",
            "Epoch 536/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5414 - accuracy: 0.7836 - val_loss: 0.7081 - val_accuracy: 0.7391\n",
            "Epoch 537/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.5596 - accuracy: 0.7817 - val_loss: 0.7061 - val_accuracy: 0.7681\n",
            "Epoch 538/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5936 - accuracy: 0.7576 - val_loss: 0.6928 - val_accuracy: 0.7729\n",
            "Epoch 539/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5756 - accuracy: 0.7648 - val_loss: 0.6935 - val_accuracy: 0.7633\n",
            "Epoch 540/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5923 - accuracy: 0.7606 - val_loss: 0.6874 - val_accuracy: 0.7585\n",
            "Epoch 541/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5655 - accuracy: 0.7715 - val_loss: 0.7058 - val_accuracy: 0.7343\n",
            "Epoch 542/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5711 - accuracy: 0.7799 - val_loss: 0.6924 - val_accuracy: 0.7488\n",
            "Epoch 543/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.5668 - accuracy: 0.7842 - val_loss: 0.6927 - val_accuracy: 0.7633\n",
            "Epoch 544/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5538 - accuracy: 0.7872 - val_loss: 0.6978 - val_accuracy: 0.7633\n",
            "Epoch 545/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5641 - accuracy: 0.7787 - val_loss: 0.6888 - val_accuracy: 0.7536\n",
            "Epoch 546/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.5549 - accuracy: 0.7860 - val_loss: 0.7037 - val_accuracy: 0.7729\n",
            "Epoch 547/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5603 - accuracy: 0.7805 - val_loss: 0.6988 - val_accuracy: 0.7391\n",
            "Epoch 548/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5844 - accuracy: 0.7678 - val_loss: 0.7117 - val_accuracy: 0.7295\n",
            "Epoch 549/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.5662 - accuracy: 0.7751 - val_loss: 0.7055 - val_accuracy: 0.7536\n",
            "Epoch 550/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5466 - accuracy: 0.7969 - val_loss: 0.7009 - val_accuracy: 0.7246\n",
            "Epoch 551/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5562 - accuracy: 0.7817 - val_loss: 0.6939 - val_accuracy: 0.7440\n",
            "Epoch 552/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.5588 - accuracy: 0.7805 - val_loss: 0.6940 - val_accuracy: 0.7633\n",
            "Epoch 553/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5794 - accuracy: 0.7606 - val_loss: 0.7009 - val_accuracy: 0.7536\n",
            "Epoch 554/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.5592 - accuracy: 0.7914 - val_loss: 0.7039 - val_accuracy: 0.7585\n",
            "Epoch 555/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.5667 - accuracy: 0.7811 - val_loss: 0.6951 - val_accuracy: 0.7585\n",
            "Epoch 556/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5497 - accuracy: 0.7872 - val_loss: 0.6823 - val_accuracy: 0.7585\n",
            "Epoch 557/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5495 - accuracy: 0.7830 - val_loss: 0.6880 - val_accuracy: 0.7440\n",
            "Epoch 558/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5585 - accuracy: 0.7981 - val_loss: 0.6868 - val_accuracy: 0.7633\n",
            "Epoch 559/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5583 - accuracy: 0.7757 - val_loss: 0.6898 - val_accuracy: 0.7681\n",
            "Epoch 560/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.5371 - accuracy: 0.7908 - val_loss: 0.7119 - val_accuracy: 0.7343\n",
            "Epoch 561/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5609 - accuracy: 0.7872 - val_loss: 0.7013 - val_accuracy: 0.7536\n",
            "Epoch 562/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5466 - accuracy: 0.7823 - val_loss: 0.6754 - val_accuracy: 0.7729\n",
            "Epoch 563/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5364 - accuracy: 0.7842 - val_loss: 0.7058 - val_accuracy: 0.7729\n",
            "Epoch 564/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5654 - accuracy: 0.7696 - val_loss: 0.6849 - val_accuracy: 0.7681\n",
            "Epoch 565/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.5570 - accuracy: 0.7805 - val_loss: 0.6921 - val_accuracy: 0.7681\n",
            "Epoch 566/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5702 - accuracy: 0.7703 - val_loss: 0.6895 - val_accuracy: 0.7778\n",
            "Epoch 567/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.5414 - accuracy: 0.7854 - val_loss: 0.6766 - val_accuracy: 0.7729\n",
            "Epoch 568/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5473 - accuracy: 0.7811 - val_loss: 0.6868 - val_accuracy: 0.7826\n",
            "Epoch 569/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5434 - accuracy: 0.7860 - val_loss: 0.6834 - val_accuracy: 0.7633\n",
            "Epoch 570/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5342 - accuracy: 0.7999 - val_loss: 0.6861 - val_accuracy: 0.7585\n",
            "Epoch 571/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.5344 - accuracy: 0.7993 - val_loss: 0.6919 - val_accuracy: 0.7391\n",
            "Epoch 572/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5433 - accuracy: 0.7866 - val_loss: 0.6779 - val_accuracy: 0.7633\n",
            "Epoch 573/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5490 - accuracy: 0.7872 - val_loss: 0.7053 - val_accuracy: 0.7681\n",
            "Epoch 574/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5360 - accuracy: 0.7926 - val_loss: 0.6918 - val_accuracy: 0.7585\n",
            "Epoch 575/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5244 - accuracy: 0.7956 - val_loss: 0.6956 - val_accuracy: 0.7681\n",
            "Epoch 576/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5291 - accuracy: 0.7902 - val_loss: 0.6943 - val_accuracy: 0.7681\n",
            "Epoch 577/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5336 - accuracy: 0.7981 - val_loss: 0.6763 - val_accuracy: 0.7585\n",
            "Epoch 578/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5073 - accuracy: 0.8096 - val_loss: 0.6778 - val_accuracy: 0.7681\n",
            "Epoch 579/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5295 - accuracy: 0.7981 - val_loss: 0.6685 - val_accuracy: 0.7633\n",
            "Epoch 580/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5287 - accuracy: 0.7902 - val_loss: 0.6744 - val_accuracy: 0.7585\n",
            "Epoch 581/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.5402 - accuracy: 0.7860 - val_loss: 0.6724 - val_accuracy: 0.7729\n",
            "Epoch 582/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5085 - accuracy: 0.8041 - val_loss: 0.6712 - val_accuracy: 0.7826\n",
            "Epoch 583/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.5380 - accuracy: 0.7914 - val_loss: 0.6739 - val_accuracy: 0.7778\n",
            "Epoch 584/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.5400 - accuracy: 0.7902 - val_loss: 0.6783 - val_accuracy: 0.7778\n",
            "Epoch 585/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.5342 - accuracy: 0.7944 - val_loss: 0.6712 - val_accuracy: 0.7778\n",
            "Epoch 586/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5253 - accuracy: 0.7969 - val_loss: 0.6721 - val_accuracy: 0.7923\n",
            "Epoch 587/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5104 - accuracy: 0.8071 - val_loss: 0.6859 - val_accuracy: 0.7874\n",
            "Epoch 588/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5181 - accuracy: 0.7950 - val_loss: 0.6897 - val_accuracy: 0.7826\n",
            "Epoch 589/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.5197 - accuracy: 0.8065 - val_loss: 0.6769 - val_accuracy: 0.7826\n",
            "Epoch 590/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5213 - accuracy: 0.7981 - val_loss: 0.6692 - val_accuracy: 0.7633\n",
            "Epoch 591/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5391 - accuracy: 0.7914 - val_loss: 0.6695 - val_accuracy: 0.7536\n",
            "Epoch 592/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.5190 - accuracy: 0.8011 - val_loss: 0.6583 - val_accuracy: 0.7923\n",
            "Epoch 593/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5087 - accuracy: 0.7920 - val_loss: 0.6618 - val_accuracy: 0.7874\n",
            "Epoch 594/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5246 - accuracy: 0.7969 - val_loss: 0.6554 - val_accuracy: 0.7826\n",
            "Epoch 595/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.5061 - accuracy: 0.8096 - val_loss: 0.6606 - val_accuracy: 0.7874\n",
            "Epoch 596/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5068 - accuracy: 0.8059 - val_loss: 0.6559 - val_accuracy: 0.7729\n",
            "Epoch 597/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5025 - accuracy: 0.8096 - val_loss: 0.6638 - val_accuracy: 0.7778\n",
            "Epoch 598/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5057 - accuracy: 0.8077 - val_loss: 0.6918 - val_accuracy: 0.7440\n",
            "Epoch 599/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5158 - accuracy: 0.8011 - val_loss: 0.6687 - val_accuracy: 0.7681\n",
            "Epoch 600/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5144 - accuracy: 0.7896 - val_loss: 0.6522 - val_accuracy: 0.7681\n",
            "Epoch 601/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.5318 - accuracy: 0.7920 - val_loss: 0.6575 - val_accuracy: 0.7826\n",
            "Epoch 602/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4800 - accuracy: 0.8198 - val_loss: 0.6748 - val_accuracy: 0.7778\n",
            "Epoch 603/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5278 - accuracy: 0.7902 - val_loss: 0.6695 - val_accuracy: 0.7536\n",
            "Epoch 604/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5176 - accuracy: 0.8053 - val_loss: 0.6628 - val_accuracy: 0.7585\n",
            "Epoch 605/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5152 - accuracy: 0.7932 - val_loss: 0.6718 - val_accuracy: 0.7923\n",
            "Epoch 606/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5097 - accuracy: 0.8077 - val_loss: 0.6750 - val_accuracy: 0.7826\n",
            "Epoch 607/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.4915 - accuracy: 0.8096 - val_loss: 0.6706 - val_accuracy: 0.7778\n",
            "Epoch 608/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4944 - accuracy: 0.8071 - val_loss: 0.6785 - val_accuracy: 0.7729\n",
            "Epoch 609/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5244 - accuracy: 0.7987 - val_loss: 0.6757 - val_accuracy: 0.7778\n",
            "Epoch 610/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5060 - accuracy: 0.8059 - val_loss: 0.6798 - val_accuracy: 0.7778\n",
            "Epoch 611/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5260 - accuracy: 0.7932 - val_loss: 0.6767 - val_accuracy: 0.7778\n",
            "Epoch 612/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5188 - accuracy: 0.8017 - val_loss: 0.6653 - val_accuracy: 0.7826\n",
            "Epoch 613/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5214 - accuracy: 0.7878 - val_loss: 0.6772 - val_accuracy: 0.7681\n",
            "Epoch 614/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4889 - accuracy: 0.8035 - val_loss: 0.6688 - val_accuracy: 0.7729\n",
            "Epoch 615/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4903 - accuracy: 0.8120 - val_loss: 0.6503 - val_accuracy: 0.7729\n",
            "Epoch 616/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4860 - accuracy: 0.8077 - val_loss: 0.6701 - val_accuracy: 0.7729\n",
            "Epoch 617/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4951 - accuracy: 0.7999 - val_loss: 0.6627 - val_accuracy: 0.7681\n",
            "Epoch 618/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5158 - accuracy: 0.7890 - val_loss: 0.6691 - val_accuracy: 0.7585\n",
            "Epoch 619/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4902 - accuracy: 0.8162 - val_loss: 0.6741 - val_accuracy: 0.7778\n",
            "Epoch 620/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5152 - accuracy: 0.8017 - val_loss: 0.6688 - val_accuracy: 0.7681\n",
            "Epoch 621/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4926 - accuracy: 0.8047 - val_loss: 0.6561 - val_accuracy: 0.7826\n",
            "Epoch 622/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4597 - accuracy: 0.8180 - val_loss: 0.6612 - val_accuracy: 0.7826\n",
            "Epoch 623/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4904 - accuracy: 0.8114 - val_loss: 0.6584 - val_accuracy: 0.7826\n",
            "Epoch 624/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4914 - accuracy: 0.8120 - val_loss: 0.6641 - val_accuracy: 0.7923\n",
            "Epoch 625/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4966 - accuracy: 0.8005 - val_loss: 0.6576 - val_accuracy: 0.7633\n",
            "Epoch 626/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4704 - accuracy: 0.8156 - val_loss: 0.6703 - val_accuracy: 0.7633\n",
            "Epoch 627/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4827 - accuracy: 0.8089 - val_loss: 0.6541 - val_accuracy: 0.7778\n",
            "Epoch 628/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4848 - accuracy: 0.8041 - val_loss: 0.6710 - val_accuracy: 0.7826\n",
            "Epoch 629/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5059 - accuracy: 0.8096 - val_loss: 0.6589 - val_accuracy: 0.7826\n",
            "Epoch 630/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4832 - accuracy: 0.8108 - val_loss: 0.6648 - val_accuracy: 0.7681\n",
            "Epoch 631/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4956 - accuracy: 0.8047 - val_loss: 0.6661 - val_accuracy: 0.7778\n",
            "Epoch 632/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.4904 - accuracy: 0.8089 - val_loss: 0.6841 - val_accuracy: 0.7729\n",
            "Epoch 633/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5099 - accuracy: 0.7956 - val_loss: 0.6464 - val_accuracy: 0.7923\n",
            "Epoch 634/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4866 - accuracy: 0.8102 - val_loss: 0.6558 - val_accuracy: 0.7826\n",
            "Epoch 635/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 0.4797 - accuracy: 0.8210 - val_loss: 0.6602 - val_accuracy: 0.7729\n",
            "Epoch 636/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 0.4713 - accuracy: 0.8253 - val_loss: 0.6579 - val_accuracy: 0.7826\n",
            "Epoch 637/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4954 - accuracy: 0.8150 - val_loss: 0.6482 - val_accuracy: 0.7729\n",
            "Epoch 638/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 0.4722 - accuracy: 0.8126 - val_loss: 0.6614 - val_accuracy: 0.7778\n",
            "Epoch 639/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4691 - accuracy: 0.8144 - val_loss: 0.6441 - val_accuracy: 0.7778\n",
            "Epoch 640/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.4601 - accuracy: 0.8192 - val_loss: 0.6539 - val_accuracy: 0.7778\n",
            "Epoch 641/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4853 - accuracy: 0.8017 - val_loss: 0.6460 - val_accuracy: 0.7681\n",
            "Epoch 642/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.4870 - accuracy: 0.8096 - val_loss: 0.6379 - val_accuracy: 0.7874\n",
            "Epoch 643/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.4719 - accuracy: 0.8192 - val_loss: 0.6578 - val_accuracy: 0.7778\n",
            "Epoch 644/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.4494 - accuracy: 0.8349 - val_loss: 0.6469 - val_accuracy: 0.7778\n",
            "Epoch 645/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4716 - accuracy: 0.8222 - val_loss: 0.6651 - val_accuracy: 0.7488\n",
            "Epoch 646/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.4865 - accuracy: 0.8108 - val_loss: 0.6594 - val_accuracy: 0.7729\n",
            "Epoch 647/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4624 - accuracy: 0.8210 - val_loss: 0.6450 - val_accuracy: 0.7874\n",
            "Epoch 648/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.4690 - accuracy: 0.8186 - val_loss: 0.6498 - val_accuracy: 0.7633\n",
            "Epoch 649/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4713 - accuracy: 0.8186 - val_loss: 0.6540 - val_accuracy: 0.7826\n",
            "Epoch 650/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4629 - accuracy: 0.8186 - val_loss: 0.6664 - val_accuracy: 0.7681\n",
            "Epoch 651/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4762 - accuracy: 0.8186 - val_loss: 0.6516 - val_accuracy: 0.7681\n",
            "Epoch 652/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4525 - accuracy: 0.8283 - val_loss: 0.6621 - val_accuracy: 0.7681\n",
            "Epoch 653/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4625 - accuracy: 0.8265 - val_loss: 0.6652 - val_accuracy: 0.7826\n",
            "Epoch 654/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4614 - accuracy: 0.8174 - val_loss: 0.6687 - val_accuracy: 0.7874\n",
            "Epoch 655/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.4734 - accuracy: 0.8150 - val_loss: 0.6586 - val_accuracy: 0.7778\n",
            "Epoch 656/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.4457 - accuracy: 0.8222 - val_loss: 0.6600 - val_accuracy: 0.7681\n",
            "Epoch 657/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.4724 - accuracy: 0.8247 - val_loss: 0.6668 - val_accuracy: 0.7681\n",
            "Epoch 658/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.4515 - accuracy: 0.8259 - val_loss: 0.6465 - val_accuracy: 0.7874\n",
            "Epoch 659/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4458 - accuracy: 0.8362 - val_loss: 0.6544 - val_accuracy: 0.7826\n",
            "Epoch 660/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4660 - accuracy: 0.8186 - val_loss: 0.6447 - val_accuracy: 0.7778\n",
            "Epoch 661/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4533 - accuracy: 0.8265 - val_loss: 0.6364 - val_accuracy: 0.7826\n",
            "Epoch 662/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4575 - accuracy: 0.8259 - val_loss: 0.6406 - val_accuracy: 0.7923\n",
            "Epoch 663/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4654 - accuracy: 0.8210 - val_loss: 0.6410 - val_accuracy: 0.7971\n",
            "Epoch 664/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4628 - accuracy: 0.8271 - val_loss: 0.6513 - val_accuracy: 0.7729\n",
            "Epoch 665/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4775 - accuracy: 0.8198 - val_loss: 0.6563 - val_accuracy: 0.7826\n",
            "Epoch 666/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.4614 - accuracy: 0.8204 - val_loss: 0.6471 - val_accuracy: 0.7681\n",
            "Epoch 667/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.4589 - accuracy: 0.8216 - val_loss: 0.6380 - val_accuracy: 0.7826\n",
            "Epoch 668/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.4611 - accuracy: 0.8229 - val_loss: 0.6392 - val_accuracy: 0.7874\n",
            "Epoch 669/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4532 - accuracy: 0.8289 - val_loss: 0.6556 - val_accuracy: 0.7874\n",
            "Epoch 670/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4754 - accuracy: 0.8120 - val_loss: 0.6639 - val_accuracy: 0.7874\n",
            "Epoch 671/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.4690 - accuracy: 0.8180 - val_loss: 0.6860 - val_accuracy: 0.7488\n",
            "Epoch 672/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4529 - accuracy: 0.8265 - val_loss: 0.6543 - val_accuracy: 0.7826\n",
            "Epoch 673/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4525 - accuracy: 0.8247 - val_loss: 0.6500 - val_accuracy: 0.7778\n",
            "Epoch 674/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4571 - accuracy: 0.8319 - val_loss: 0.6516 - val_accuracy: 0.7826\n",
            "Epoch 675/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4407 - accuracy: 0.8428 - val_loss: 0.6497 - val_accuracy: 0.7585\n",
            "Epoch 676/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.4361 - accuracy: 0.8349 - val_loss: 0.6523 - val_accuracy: 0.7633\n",
            "Epoch 677/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4410 - accuracy: 0.8289 - val_loss: 0.6554 - val_accuracy: 0.7778\n",
            "Epoch 678/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4386 - accuracy: 0.8386 - val_loss: 0.6549 - val_accuracy: 0.7778\n",
            "Epoch 679/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.4355 - accuracy: 0.8277 - val_loss: 0.6333 - val_accuracy: 0.7971\n",
            "Epoch 680/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.4599 - accuracy: 0.8229 - val_loss: 0.6475 - val_accuracy: 0.7923\n",
            "Epoch 681/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.4360 - accuracy: 0.8283 - val_loss: 0.6463 - val_accuracy: 0.8068\n",
            "Epoch 682/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4472 - accuracy: 0.8265 - val_loss: 0.6372 - val_accuracy: 0.7971\n",
            "Epoch 683/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4674 - accuracy: 0.8253 - val_loss: 0.6422 - val_accuracy: 0.7729\n",
            "Epoch 684/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4325 - accuracy: 0.8313 - val_loss: 0.6548 - val_accuracy: 0.7778\n",
            "Epoch 685/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4383 - accuracy: 0.8319 - val_loss: 0.6646 - val_accuracy: 0.7633\n",
            "Epoch 686/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4433 - accuracy: 0.8259 - val_loss: 0.6502 - val_accuracy: 0.7874\n",
            "Epoch 687/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.4465 - accuracy: 0.8192 - val_loss: 0.6433 - val_accuracy: 0.7778\n",
            "Epoch 688/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4456 - accuracy: 0.8247 - val_loss: 0.6657 - val_accuracy: 0.7826\n",
            "Epoch 689/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4352 - accuracy: 0.8271 - val_loss: 0.6576 - val_accuracy: 0.7729\n",
            "Epoch 690/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4574 - accuracy: 0.8186 - val_loss: 0.6468 - val_accuracy: 0.7971\n",
            "Epoch 691/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.4421 - accuracy: 0.8301 - val_loss: 0.6445 - val_accuracy: 0.7923\n",
            "Epoch 692/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4325 - accuracy: 0.8349 - val_loss: 0.6630 - val_accuracy: 0.7778\n",
            "Epoch 693/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4513 - accuracy: 0.8356 - val_loss: 0.6665 - val_accuracy: 0.7826\n",
            "Epoch 694/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4288 - accuracy: 0.8283 - val_loss: 0.6444 - val_accuracy: 0.7874\n",
            "Epoch 695/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4421 - accuracy: 0.8386 - val_loss: 0.6472 - val_accuracy: 0.7874\n",
            "Epoch 696/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4194 - accuracy: 0.8404 - val_loss: 0.6386 - val_accuracy: 0.7971\n",
            "Epoch 697/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4094 - accuracy: 0.8543 - val_loss: 0.6568 - val_accuracy: 0.7826\n",
            "Epoch 698/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4134 - accuracy: 0.8495 - val_loss: 0.6341 - val_accuracy: 0.7874\n",
            "Epoch 699/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4262 - accuracy: 0.8422 - val_loss: 0.6457 - val_accuracy: 0.7874\n",
            "Epoch 700/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4151 - accuracy: 0.8392 - val_loss: 0.6317 - val_accuracy: 0.7729\n",
            "Epoch 701/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4287 - accuracy: 0.8398 - val_loss: 0.6366 - val_accuracy: 0.7874\n",
            "Epoch 702/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.4285 - accuracy: 0.8337 - val_loss: 0.6419 - val_accuracy: 0.7826\n",
            "Epoch 703/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4316 - accuracy: 0.8392 - val_loss: 0.6443 - val_accuracy: 0.7826\n",
            "Epoch 704/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.4164 - accuracy: 0.8470 - val_loss: 0.6417 - val_accuracy: 0.7826\n",
            "Epoch 705/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4408 - accuracy: 0.8253 - val_loss: 0.6430 - val_accuracy: 0.7633\n",
            "Epoch 706/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4225 - accuracy: 0.8313 - val_loss: 0.6421 - val_accuracy: 0.7826\n",
            "Epoch 707/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4287 - accuracy: 0.8277 - val_loss: 0.6333 - val_accuracy: 0.7729\n",
            "Epoch 708/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.4221 - accuracy: 0.8343 - val_loss: 0.6422 - val_accuracy: 0.7826\n",
            "Epoch 709/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4296 - accuracy: 0.8277 - val_loss: 0.6644 - val_accuracy: 0.7536\n",
            "Epoch 710/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4295 - accuracy: 0.8374 - val_loss: 0.6428 - val_accuracy: 0.7778\n",
            "Epoch 711/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4215 - accuracy: 0.8416 - val_loss: 0.6332 - val_accuracy: 0.7729\n",
            "Epoch 712/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4202 - accuracy: 0.8313 - val_loss: 0.6235 - val_accuracy: 0.7778\n",
            "Epoch 713/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3975 - accuracy: 0.8489 - val_loss: 0.6281 - val_accuracy: 0.7826\n",
            "Epoch 714/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4008 - accuracy: 0.8476 - val_loss: 0.6412 - val_accuracy: 0.7585\n",
            "Epoch 715/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4021 - accuracy: 0.8489 - val_loss: 0.6384 - val_accuracy: 0.7729\n",
            "Epoch 716/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4288 - accuracy: 0.8343 - val_loss: 0.6129 - val_accuracy: 0.7778\n",
            "Epoch 717/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4301 - accuracy: 0.8392 - val_loss: 0.6386 - val_accuracy: 0.7874\n",
            "Epoch 718/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3945 - accuracy: 0.8537 - val_loss: 0.6332 - val_accuracy: 0.7778\n",
            "Epoch 719/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3993 - accuracy: 0.8519 - val_loss: 0.6298 - val_accuracy: 0.7971\n",
            "Epoch 720/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4084 - accuracy: 0.8428 - val_loss: 0.6430 - val_accuracy: 0.7633\n",
            "Epoch 721/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4207 - accuracy: 0.8386 - val_loss: 0.6275 - val_accuracy: 0.7729\n",
            "Epoch 722/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4086 - accuracy: 0.8501 - val_loss: 0.6322 - val_accuracy: 0.7778\n",
            "Epoch 723/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4038 - accuracy: 0.8513 - val_loss: 0.6263 - val_accuracy: 0.7826\n",
            "Epoch 724/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3982 - accuracy: 0.8440 - val_loss: 0.6321 - val_accuracy: 0.7874\n",
            "Epoch 725/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4021 - accuracy: 0.8501 - val_loss: 0.6185 - val_accuracy: 0.7923\n",
            "Epoch 726/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4091 - accuracy: 0.8434 - val_loss: 0.6456 - val_accuracy: 0.7971\n",
            "Epoch 727/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4061 - accuracy: 0.8464 - val_loss: 0.6202 - val_accuracy: 0.7826\n",
            "Epoch 728/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.4029 - accuracy: 0.8356 - val_loss: 0.6242 - val_accuracy: 0.7874\n",
            "Epoch 729/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4128 - accuracy: 0.8416 - val_loss: 0.6380 - val_accuracy: 0.7826\n",
            "Epoch 730/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4001 - accuracy: 0.8585 - val_loss: 0.6370 - val_accuracy: 0.7826\n",
            "Epoch 731/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3912 - accuracy: 0.8525 - val_loss: 0.6423 - val_accuracy: 0.7826\n",
            "Epoch 732/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3903 - accuracy: 0.8549 - val_loss: 0.6516 - val_accuracy: 0.7729\n",
            "Epoch 733/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4050 - accuracy: 0.8356 - val_loss: 0.6423 - val_accuracy: 0.7633\n",
            "Epoch 734/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4017 - accuracy: 0.8428 - val_loss: 0.6321 - val_accuracy: 0.8164\n",
            "Epoch 735/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4090 - accuracy: 0.8440 - val_loss: 0.6386 - val_accuracy: 0.7826\n",
            "Epoch 736/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3999 - accuracy: 0.8464 - val_loss: 0.6409 - val_accuracy: 0.7874\n",
            "Epoch 737/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4069 - accuracy: 0.8446 - val_loss: 0.6307 - val_accuracy: 0.8068\n",
            "Epoch 738/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3925 - accuracy: 0.8501 - val_loss: 0.6334 - val_accuracy: 0.7923\n",
            "Epoch 739/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3924 - accuracy: 0.8507 - val_loss: 0.6236 - val_accuracy: 0.8019\n",
            "Epoch 740/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4030 - accuracy: 0.8579 - val_loss: 0.6373 - val_accuracy: 0.7874\n",
            "Epoch 741/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3920 - accuracy: 0.8476 - val_loss: 0.6317 - val_accuracy: 0.7874\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(cnnhistory.history['loss'])\n",
        "plt.plot(cnnhistory.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train','test'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "oQYnuaCrDH_A",
        "outputId": "390c07ed-0138-4780-cf60-bd0ccee68c44"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(cnnhistory.history['accuracy'])\n",
        "plt.plot(cnnhistory.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train','test'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IFkTuO8nDNdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(x_testcnn)"
      ],
      "metadata": {
        "id": "A2xtrN5wDQgY"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_f_ohjXKDUpY",
        "outputId": "c98f5337-f930-41a8-bbd1-6aeb5f0a2045"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2.9732284e-01, 1.4840640e-01, 2.6728356e-01, 1.5114616e-01,\n",
              "        1.1273303e-01, 2.3107985e-02],\n",
              "       [7.5663452e-04, 2.1081813e-05, 8.7182521e-08, 9.7904575e-01,\n",
              "        7.0539805e-05, 2.0105848e-02],\n",
              "       [2.8049931e-01, 7.6236129e-02, 6.1090060e-02, 3.3983958e-01,\n",
              "        1.9625047e-02, 2.2270992e-01],\n",
              "       ...,\n",
              "       [4.6376983e-04, 2.3338109e-06, 3.1670104e-04, 2.6566603e-03,\n",
              "        9.7192222e-01, 2.4638304e-02],\n",
              "       [5.9895094e-05, 4.6945295e-01, 5.0128108e-01, 1.0947744e-02,\n",
              "        4.0515377e-03, 1.4206865e-02],\n",
              "       [1.0799733e-04, 4.6087735e-05, 1.7034870e-02, 1.7994398e-04,\n",
              "        9.8092550e-01, 1.7056157e-03]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmA2bFgsDW3D",
        "outputId": "2fd2f9d3-9f1b-4189-9188-8e4fd0ddf6bc"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3, 3, 3, 0, 4, 4, 1, 2, 5, 0, 5, 3, 2, 2, 1, 4, 4, 5, 2, 4, 1, 2,\n",
              "       3, 5, 4, 2, 1, 2, 2, 2, 2, 5, 2, 5, 3, 1, 2, 0, 4, 5, 2, 3, 2, 2,\n",
              "       5, 2, 0, 1, 3, 4, 4, 5, 1, 1, 4, 4, 5, 1, 1, 1, 0, 1, 5, 0, 4, 1,\n",
              "       1, 0, 1, 4, 1, 0, 4, 1, 3, 1, 2, 2, 1, 2, 3, 0, 5, 3, 5, 5, 2, 4,\n",
              "       5, 1, 5, 1, 5, 3, 4, 1, 4, 1, 5, 4, 5, 1, 2, 1, 1, 5, 1, 3, 2, 5,\n",
              "       2, 1, 3, 5, 4, 4, 0, 0, 3, 1, 2, 2, 3, 5, 1, 5, 3, 4, 2, 2, 0, 2,\n",
              "       2, 5, 5, 0, 3, 3, 2, 2, 0, 3, 4, 0, 4, 2, 4, 5, 4, 1, 5, 3, 2, 2,\n",
              "       5, 4, 3, 3, 4, 3, 3, 4, 1, 5, 2, 3, 2, 2, 0, 2, 1, 3, 1, 2, 2, 5,\n",
              "       5, 1, 4, 2, 2, 1, 1, 3, 3, 5, 5, 2, 5, 1, 5, 0, 3, 3, 0, 5, 1, 4,\n",
              "       1, 4, 2, 5, 5, 1, 4, 2, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_Ytest = y_test.astype(int)"
      ],
      "metadata": {
        "id": "0PHDKWJWDY3g"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_Ytest"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k68v2i-pDbZE",
        "outputId": "d416255a-ae91-4215-d2ac-e6dc77903d7d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3, 3, 3, 0, 4, 4, 1, 2, 5, 0, 5, 3, 2, 2, 1, 4, 4, 5, 2, 4, 1, 2,\n",
              "       3, 5, 4, 2, 1, 2, 2, 2, 2, 5, 2, 5, 3, 1, 2, 0, 4, 5, 2, 3, 2, 2,\n",
              "       5, 2, 0, 1, 3, 4, 4, 5, 1, 1, 4, 4, 5, 1, 1, 1, 0, 1, 5, 0, 4, 1,\n",
              "       1, 0, 1, 4, 1, 0, 4, 1, 3, 1, 2, 2, 1, 2, 3, 0, 5, 3, 5, 5, 2, 4,\n",
              "       5, 1, 5, 1, 5, 3, 4, 1, 4, 1, 5, 4, 5, 1, 2, 1, 1, 5, 1, 3, 2, 5,\n",
              "       2, 1, 3, 5, 4, 4, 0, 0, 3, 1, 2, 2, 3, 5, 1, 5, 3, 4, 2, 2, 0, 2,\n",
              "       2, 5, 5, 0, 3, 3, 2, 2, 0, 3, 4, 0, 4, 2, 4, 5, 4, 1, 5, 3, 2, 2,\n",
              "       5, 4, 3, 3, 4, 3, 3, 4, 1, 5, 2, 3, 2, 2, 0, 2, 1, 3, 1, 2, 2, 5,\n",
              "       5, 1, 4, 2, 2, 1, 1, 3, 3, 5, 5, 2, 5, 1, 5, 0, 3, 3, 0, 5, 1, 4,\n",
              "       1, 4, 2, 5, 5, 1, 4, 2, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds1=predictions.argmax(axis=1)\n",
        "preds1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_cAI39VFhtp",
        "outputId": "7fc20042-487c-45de-fc27-4df263445982"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 3, 3, 2, 4, 4, 1, 2, 5, 1, 5, 3, 2, 2, 0, 4, 4, 3, 3, 4, 2, 2,\n",
              "       2, 3, 4, 2, 1, 2, 2, 2, 2, 5, 2, 5, 3, 1, 2, 0, 4, 5, 2, 5, 2, 2,\n",
              "       3, 2, 1, 1, 3, 4, 4, 2, 1, 1, 4, 4, 5, 1, 1, 1, 3, 1, 2, 0, 4, 1,\n",
              "       1, 1, 1, 4, 1, 2, 4, 1, 5, 0, 2, 2, 0, 2, 1, 0, 5, 3, 5, 5, 2, 3,\n",
              "       3, 1, 5, 1, 5, 3, 2, 2, 4, 1, 5, 4, 5, 1, 2, 1, 1, 5, 1, 5, 2, 5,\n",
              "       5, 2, 3, 5, 4, 4, 2, 0, 3, 1, 1, 2, 3, 3, 1, 3, 3, 0, 2, 2, 0, 2,\n",
              "       2, 3, 5, 0, 3, 3, 2, 4, 0, 5, 4, 1, 4, 4, 4, 4, 4, 1, 5, 3, 2, 2,\n",
              "       5, 4, 3, 3, 4, 5, 3, 4, 2, 5, 2, 3, 2, 2, 0, 2, 1, 3, 1, 4, 4, 5,\n",
              "       5, 1, 4, 2, 4, 1, 1, 3, 3, 5, 5, 2, 5, 1, 4, 1, 3, 3, 2, 5, 1, 4,\n",
              "       1, 3, 3, 2, 5, 2, 4, 2, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "abc = preds1.astype(int).flatten()"
      ],
      "metadata": {
        "id": "H4WEmM60NfQn"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "c = confusion_matrix(new_Ytest, abc) \n",
        "c"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKEBeEBlFjB8",
        "outputId": "3b91d132-c038-480f-b574-5b27be50ec4f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 8,  5,  4,  1,  0,  0],\n",
              "       [ 3, 33,  5,  0,  0,  0],\n",
              "       [ 0,  1, 36,  2,  5,  1],\n",
              "       [ 1,  1,  1, 23,  0,  5],\n",
              "       [ 1,  0,  1,  2, 29,  0],\n",
              "       [ 0,  0,  3,  7,  2, 27]])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4RjG7LWLSQx",
        "outputId": "2161fb73-2cce-4ab2-8292-c04a8553914c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3, 3, 3, 0, 4, 4, 1, 2, 5, 0, 5, 3, 2, 2, 1, 4, 4, 5, 2, 4, 1, 2,\n",
              "       3, 5, 4, 2, 1, 2, 2, 2, 2, 5, 2, 5, 3, 1, 2, 0, 4, 5, 2, 3, 2, 2,\n",
              "       5, 2, 0, 1, 3, 4, 4, 5, 1, 1, 4, 4, 5, 1, 1, 1, 0, 1, 5, 0, 4, 1,\n",
              "       1, 0, 1, 4, 1, 0, 4, 1, 3, 1, 2, 2, 1, 2, 3, 0, 5, 3, 5, 5, 2, 4,\n",
              "       5, 1, 5, 1, 5, 3, 4, 1, 4, 1, 5, 4, 5, 1, 2, 1, 1, 5, 1, 3, 2, 5,\n",
              "       2, 1, 3, 5, 4, 4, 0, 0, 3, 1, 2, 2, 3, 5, 1, 5, 3, 4, 2, 2, 0, 2,\n",
              "       2, 5, 5, 0, 3, 3, 2, 2, 0, 3, 4, 0, 4, 2, 4, 5, 4, 1, 5, 3, 2, 2,\n",
              "       5, 4, 3, 3, 4, 3, 3, 4, 1, 5, 2, 3, 2, 2, 0, 2, 1, 3, 1, 2, 2, 5,\n",
              "       5, 1, 4, 2, 2, 1, 1, 3, 3, 5, 5, 2, 5, 1, 5, 0, 3, 3, 0, 5, 1, 4,\n",
              "       1, 4, 2, 5, 5, 1, 4, 2, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, acc =model.evaluate(x_testcnn, y_test)\n",
        "print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3kzoqPkDqqS",
        "outputId": "70061ec6-8b60-45a0-be80-2b1fde8b64c5"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7/7 [==============================] - 0s 3ms/step - loss: 0.5864 - accuracy: 0.7536\n",
            "Restored model, accuracy: 75.36%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, acc =model.evaluate(X_train, y_train)\n",
        "print(\"Restored model train, accuracy: {:5.2f}%\".format(100*acc))"
      ],
      "metadata": {
        "id": "fJDTGH_OUX0z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b56666e-b714-4fb1-8a1e-2ddc32725c54"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "52/52 [==============================] - 0s 4ms/step - loss: 0.3374 - accuracy: 0.9172\n",
            "Restored model train, accuracy: 91.72%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix,accuracy_score\n",
        "import seaborn as sn\n",
        "\n",
        "print(classification_report(new_Ytest,abc))\n",
        "\n",
        "acc = float(accuracy_score(new_Ytest,abc))*100\n",
        "print(\"----accuracy score %s ----\" % acc)\n",
        "\n",
        "cm = confusion_matrix(new_Ytest,abc)\n",
        "#df_cm = pd.DataFrame(cm)\n",
        "class_names = ['neutral','calm', 'happy','sad','angry', 'fearful' ]\n",
        "df_cm = pd.DataFrame(cm, index=class_names, columns=class_names,)\n",
        "sn.heatmap(df_cm, annot=True, fmt='')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "id": "SfSC3El94LZg",
        "outputId": "1145e645-4551-45c2-d04e-fde0627b1a47"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.44      0.52        18\n",
            "           1       0.82      0.80      0.81        41\n",
            "           2       0.72      0.80      0.76        45\n",
            "           3       0.66      0.74      0.70        31\n",
            "           4       0.81      0.88      0.84        33\n",
            "           5       0.82      0.69      0.75        39\n",
            "\n",
            "    accuracy                           0.75       207\n",
            "   macro avg       0.74      0.73      0.73       207\n",
            "weighted avg       0.75      0.75      0.75       207\n",
            "\n",
            "----accuracy score 75.36231884057972 ----\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "A6UOIsB2xKek"
      },
      "execution_count": 28,
      "outputs": []
    }
  ]
}