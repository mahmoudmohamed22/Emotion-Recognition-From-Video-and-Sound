{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "original_model_no_augmentation_speech&song_RMS_1000 epoch try 2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install librosa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oi8pWV3yBVjO",
        "outputId": "39e74fa0-3b9a-4933-d52c-752388b0b111"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.7/dist-packages (0.8.1)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (2.1.9)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.2.2)\n",
            "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.10.3.post1)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (21.3)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.51.2)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.21.6)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa) (0.34.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->librosa) (3.0.9)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa) (2.23.0)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa) (1.4.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (1.24.3)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.7/dist-packages (from resampy>=0.2.2->librosa) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.10.2->librosa) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa) (2.21)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJjcbxwy46bG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "02f44328-1da3-4ffd-95de-9556c2538479"
      },
      "source": [
        "# Orignial Notebook: https://github.com/MITESHPUTHRANNEU/Speech-Emotion-Analyzer/blob/master/final_results_gender_test.ipynb\n",
        "# This notebook author: Reza Chu\n",
        "# Last Editing Date: 31st May 2019\n",
        "\n",
        "## Python\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "\n",
        "import IPython\n",
        "from IPython.display import Audio\n",
        "from IPython.display import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "## Package\n",
        "import glob \n",
        "import keras\n",
        "import IPython.display as ipd\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.graph_objs as go\n",
        "import plotly.offline as py\n",
        "import plotly.tools as tls\n",
        "import seaborn as sns\n",
        "import scipy.io.wavfile\n",
        "import tensorflow as tf\n",
        "py.init_notebook_mode(connected=True)\n",
        "\n",
        "## Keras\n",
        "from keras import regularizers\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping\n",
        "from keras.callbacks import  History, ReduceLROnPlateau, CSVLogger\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM\n",
        "from keras.layers import Input, Flatten, Dropout, Activation, BatchNormalization\n",
        "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import np_utils\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "## Sklearn\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "## Rest\n",
        "from scipy.fftpack import fft\n",
        "from scipy import signal\n",
        "from scipy.io import wavfile\n",
        "from tqdm import tqdm\n",
        "\n",
        "input_duration=3\n",
        "# % pylab inline"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "        <script type=\"text/javascript\">\n",
              "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
              "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
              "        if (typeof require !== 'undefined') {\n",
              "        require.undef(\"plotly\");\n",
              "        requirejs.config({\n",
              "            paths: {\n",
              "                'plotly': ['https://cdn.plot.ly/plotly-2.8.3.min']\n",
              "            }\n",
              "        });\n",
              "        require(['plotly'], function(Plotly) {\n",
              "            window._Plotly = Plotly;\n",
              "        });\n",
              "        }\n",
              "        </script>\n",
              "        "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#radvass speech and song\n",
        "import time\n",
        "import os\n",
        "import librosa\n",
        "from librosa import display\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "count=0\n",
        "path1 = '/content/drive/MyDrive/RAVDESS_speech'\n",
        "path2 = '/content/drive/MyDrive/RAVDESS_song'\n",
        "\n",
        "lst = []\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for subdir, dirs, files in os.walk(path2):\n",
        "  #print(dirs)\n",
        "  #print(subdir)\n",
        "  #print(files)\n",
        "  for file in files:\n",
        "      try:\n",
        "        count +=1\n",
        "        #Load librosa array, obtain mfcss, store the file and the mcss information in a new array\n",
        "        X, sample_rate = librosa.load(os.path.join(subdir,file), res_type='kaiser_fast')\n",
        "        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0) \n",
        "        # The instruction below converts the labels (from 1 to 8) to a series from 0 to 7\n",
        "        # This is because our predictor needs to start from 0 otherwise it will try to predict also 0.\n",
        "        file = int(file[7:8]) - 1 \n",
        "        arr = mfccs, file\n",
        "        lst.append(arr)\n",
        "      # If the file is not valid, skip it\n",
        "      except ValueError:\n",
        "        continue\n",
        "\n",
        "for subdir, dirs, files in os.walk(path1):\n",
        "  #print(dirs)\n",
        "  #print(subdir)\n",
        " # print(files)\n",
        "  for file in files:\n",
        "      try:\n",
        "        #Load librosa array, obtain mfcss, store the file and the mcss information in a new array\n",
        "        X, sample_rate = librosa.load(os.path.join(subdir,file), res_type='kaiser_fast')\n",
        "        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0) \n",
        "        # The instruction below converts the labels (from 1 to 8) to a series from 0 to 7\n",
        "        # This is because our predictor needs to start from 0 otherwise it will try to predict also 0.\n",
        "        file = int(file[7:8]) - 1 \n",
        "        if(file ==6 or file==7):\n",
        "          continue\n",
        "        count +=1\n",
        "        arr = mfccs, file\n",
        "        lst.append(arr)\n",
        "      # If the file is not valid, skip it\n",
        "      except ValueError:\n",
        "        continue\n",
        "\n",
        "\n",
        "\n",
        "print(\"--- Data loaded. Loading time: %s seconds ---\" % (time.time() - start_time))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfuyRZl42ccZ",
        "outputId": "52c0e3d6-a3b5-4354-bd5c-61f2fa5e48f5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Data loaded. Loading time: 207.48559093475342 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count"
      ],
      "metadata": {
        "id": "Ltlhi25L2AI5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcaab585-d958-4b13-d22c-de1c52bd44ef"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2068"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.core.fromnumeric import size\n",
        "size(lst)"
      ],
      "metadata": {
        "id": "HMOH7d7mzHB7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0aa896e-874f-409d-c596-aa7d413168bd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3208: VisibleDeprecationWarning:\n",
            "\n",
            "Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4136"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating X and y: zip makes a list of all the first elements, and a list of all the second elements.\n",
        "X, y = zip(*lst)"
      ],
      "metadata": {
        "id": "ocdL7tSTBoMt"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "X = np.asarray(X)\n",
        "y = np.asarray(y)\n",
        "X.shape, y.shape"
      ],
      "metadata": {
        "id": "vHKwUGTbBtnF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "885b84e6-613b-4700-f8d3-dffa0294d239"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2068, 40), (2068,))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "id": "2sUCtbfMztE5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03b6dee4-a402-43dc-e916-73c2ef0e0fe7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2068, 40)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PSTurzjCo5K",
        "outputId": "1756837f-6fd9-4eec-dfd6-8d8b56504710"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2068, 40)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2,train_size=0.8, random_state=42)\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(X_test,y_test, test_size=0.5,train_size=0.5, random_state=42)"
      ],
      "metadata": {
        "id": "Ai4Fy5cPCiq1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "x_traincnn = np.expand_dims(X_train, axis=2)\n",
        "x_testcnn = np.expand_dims(X_test, axis=2)\n",
        "X_valid= np.expand_dims(X_valid, axis=2)"
      ],
      "metadata": {
        "id": "tp1Fm5K3CEXu"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_traincnn.shape, x_testcnn.shape , X_valid.shape"
      ],
      "metadata": {
        "id": "RI0MxoIPBws5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c74e8653-08e0-4bbb-8174-5a78313a7064"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1654, 40, 1), (207, 40, 1), (207, 40, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "goxL4Y3jKICC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d83d830b-24e7-4475-e383-bc597f47abae"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.8.2+zzzcolab20220527125636)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (4.2.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.26.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.46.3)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (14.0.1)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.0.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.21.6)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.3.7)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.11.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras"
      ],
      "metadata": {
        "id": "EkcmN9UrKSxj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24fb238d-b000-4882-dd14-5d5bc740a72b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.8.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.layers import Input, Flatten, Dropout, Activation\n",
        "from keras.layers import Conv1D, MaxPooling1D\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras import optimizers\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv1D(128, 8,padding='same', #classifier.add(Convolution2D(64, (3, 3), padding = 'same', input_shape = (128, 128, 3), activation = 'relu'))\n",
        "                 input_shape=(40,1)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(MaxPooling1D(pool_size=(5)))\n",
        "\n",
        "\n",
        "model.add(Conv1D(256,8,padding='same',))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(MaxPooling1D(pool_size=(5)))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(256))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(256))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Dense(6))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "#opt = tf.keras.optimizers.Adam(lr=0.00002)\n",
        "opt = tf.keras.optimizers.RMSprop(lr=0.00002, rho=0.9, epsilon=1e-07, decay=0.0)\n",
        "#opt = tf.keras.optimizers.SGD(lr=0.001, momentum=0.0, decay=0.0, nesterov=False)"
      ],
      "metadata": {
        "id": "g74fXWVAC4Cr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4086cee-4166-407c-a2cb-47c2d96272af"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning:\n",
            "\n",
            "The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6ukOxAGC_I4",
        "outputId": "87ba7975-34cc-4df8-9b11-f256f103d478"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d (Conv1D)             (None, 40, 128)           1152      \n",
            "                                                                 \n",
            " activation (Activation)     (None, 40, 128)           0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 40, 128)           0         \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, 8, 128)           0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 8, 256)            262400    \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 8, 256)            0         \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 8, 256)            0         \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPooling  (None, 1, 256)           0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               65792     \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 256)               65792     \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 256)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 6)                 1542      \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 6)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 396,678\n",
            "Trainable params: 396,678\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "AbMlLNk4DCBM"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cnnhistory=model.fit(x_traincnn, y_train, batch_size=16, epochs=700, validation_data=(x_testcnn, y_test))\n",
        "cnnhistory=model.fit(x_traincnn, y_train, batch_size=16, epochs=1000 , validation_data=(X_valid, y_valid))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RI1v2AuADFhy",
        "outputId": "c4c09297-a18b-4b20-c44d-68ac49e81c55"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "104/104 [==============================] - 4s 7ms/step - loss: 5.9312 - accuracy: 0.1651 - val_loss: 3.0319 - val_accuracy: 0.1594\n",
            "Epoch 2/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 4.2740 - accuracy: 0.1820 - val_loss: 2.3754 - val_accuracy: 0.0870\n",
            "Epoch 3/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 3.7570 - accuracy: 0.1826 - val_loss: 2.3268 - val_accuracy: 0.0628\n",
            "Epoch 4/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 3.3655 - accuracy: 0.1886 - val_loss: 1.9962 - val_accuracy: 0.1691\n",
            "Epoch 5/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 3.1588 - accuracy: 0.1566 - val_loss: 1.8555 - val_accuracy: 0.0870\n",
            "Epoch 6/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 2.7925 - accuracy: 0.1832 - val_loss: 1.9559 - val_accuracy: 0.0628\n",
            "Epoch 7/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 2.6369 - accuracy: 0.1753 - val_loss: 2.0451 - val_accuracy: 0.0676\n",
            "Epoch 8/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 2.4473 - accuracy: 0.1959 - val_loss: 1.8883 - val_accuracy: 0.0628\n",
            "Epoch 9/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 2.3576 - accuracy: 0.2068 - val_loss: 1.8543 - val_accuracy: 0.1063\n",
            "Epoch 10/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 2.2802 - accuracy: 0.1983 - val_loss: 1.7607 - val_accuracy: 0.1643\n",
            "Epoch 11/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 2.1499 - accuracy: 0.1989 - val_loss: 1.7960 - val_accuracy: 0.1594\n",
            "Epoch 12/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 2.1202 - accuracy: 0.1898 - val_loss: 1.8031 - val_accuracy: 0.1208\n",
            "Epoch 13/1000\n",
            "104/104 [==============================] - 1s 8ms/step - loss: 2.0625 - accuracy: 0.2213 - val_loss: 1.7151 - val_accuracy: 0.2899\n",
            "Epoch 14/1000\n",
            "104/104 [==============================] - 1s 8ms/step - loss: 2.0008 - accuracy: 0.2056 - val_loss: 1.7436 - val_accuracy: 0.2657\n",
            "Epoch 15/1000\n",
            "104/104 [==============================] - 1s 9ms/step - loss: 2.0304 - accuracy: 0.2195 - val_loss: 1.7222 - val_accuracy: 0.2415\n",
            "Epoch 16/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 1.9766 - accuracy: 0.2074 - val_loss: 1.7209 - val_accuracy: 0.2705\n",
            "Epoch 17/1000\n",
            "104/104 [==============================] - 1s 9ms/step - loss: 1.9928 - accuracy: 0.2237 - val_loss: 1.7204 - val_accuracy: 0.2802\n",
            "Epoch 18/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 1.9576 - accuracy: 0.2104 - val_loss: 1.7338 - val_accuracy: 0.2560\n",
            "Epoch 19/1000\n",
            "104/104 [==============================] - 1s 9ms/step - loss: 1.9257 - accuracy: 0.2068 - val_loss: 1.6932 - val_accuracy: 0.3382\n",
            "Epoch 20/1000\n",
            "104/104 [==============================] - 1s 8ms/step - loss: 1.8768 - accuracy: 0.2279 - val_loss: 1.6838 - val_accuracy: 0.3140\n",
            "Epoch 21/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 1.8842 - accuracy: 0.2207 - val_loss: 1.6959 - val_accuracy: 0.3043\n",
            "Epoch 22/1000\n",
            "104/104 [==============================] - 1s 8ms/step - loss: 1.8587 - accuracy: 0.2340 - val_loss: 1.7148 - val_accuracy: 0.2271\n",
            "Epoch 23/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 1.8708 - accuracy: 0.2122 - val_loss: 1.7240 - val_accuracy: 0.1884\n",
            "Epoch 24/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 1.8585 - accuracy: 0.2237 - val_loss: 1.6985 - val_accuracy: 0.2367\n",
            "Epoch 25/1000\n",
            "104/104 [==============================] - 1s 8ms/step - loss: 1.8283 - accuracy: 0.2273 - val_loss: 1.7085 - val_accuracy: 0.2850\n",
            "Epoch 26/1000\n",
            "104/104 [==============================] - 1s 8ms/step - loss: 1.8388 - accuracy: 0.2443 - val_loss: 1.6829 - val_accuracy: 0.3768\n",
            "Epoch 27/1000\n",
            "104/104 [==============================] - 1s 9ms/step - loss: 1.8270 - accuracy: 0.2231 - val_loss: 1.7282 - val_accuracy: 0.3188\n",
            "Epoch 28/1000\n",
            "104/104 [==============================] - 1s 9ms/step - loss: 1.8187 - accuracy: 0.2304 - val_loss: 1.6952 - val_accuracy: 0.3478\n",
            "Epoch 29/1000\n",
            "104/104 [==============================] - 1s 8ms/step - loss: 1.8078 - accuracy: 0.2400 - val_loss: 1.6983 - val_accuracy: 0.2512\n",
            "Epoch 30/1000\n",
            "104/104 [==============================] - 1s 8ms/step - loss: 1.7901 - accuracy: 0.2418 - val_loss: 1.6934 - val_accuracy: 0.3478\n",
            "Epoch 31/1000\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 1.7790 - accuracy: 0.2437 - val_loss: 1.6698 - val_accuracy: 0.3430\n",
            "Epoch 32/1000\n",
            "104/104 [==============================] - 1s 9ms/step - loss: 1.8110 - accuracy: 0.2297 - val_loss: 1.6980 - val_accuracy: 0.3237\n",
            "Epoch 33/1000\n",
            "104/104 [==============================] - 1s 8ms/step - loss: 1.7645 - accuracy: 0.2672 - val_loss: 1.6948 - val_accuracy: 0.2899\n",
            "Epoch 34/1000\n",
            "104/104 [==============================] - 1s 9ms/step - loss: 1.7785 - accuracy: 0.2400 - val_loss: 1.6895 - val_accuracy: 0.3140\n",
            "Epoch 35/1000\n",
            "104/104 [==============================] - 1s 8ms/step - loss: 1.7518 - accuracy: 0.2678 - val_loss: 1.6716 - val_accuracy: 0.3188\n",
            "Epoch 36/1000\n",
            "104/104 [==============================] - 1s 8ms/step - loss: 1.7493 - accuracy: 0.2539 - val_loss: 1.6835 - val_accuracy: 0.3188\n",
            "Epoch 37/1000\n",
            "104/104 [==============================] - 1s 8ms/step - loss: 1.7570 - accuracy: 0.2576 - val_loss: 1.6505 - val_accuracy: 0.3720\n",
            "Epoch 38/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 1.7536 - accuracy: 0.2485 - val_loss: 1.6492 - val_accuracy: 0.4010\n",
            "Epoch 39/1000\n",
            "104/104 [==============================] - 1s 8ms/step - loss: 1.7321 - accuracy: 0.2696 - val_loss: 1.6312 - val_accuracy: 0.3478\n",
            "Epoch 40/1000\n",
            "104/104 [==============================] - 1s 8ms/step - loss: 1.7275 - accuracy: 0.2660 - val_loss: 1.6237 - val_accuracy: 0.3913\n",
            "Epoch 41/1000\n",
            "104/104 [==============================] - 1s 9ms/step - loss: 1.7368 - accuracy: 0.2666 - val_loss: 1.6329 - val_accuracy: 0.4203\n",
            "Epoch 42/1000\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 1.7338 - accuracy: 0.2642 - val_loss: 1.6435 - val_accuracy: 0.3575\n",
            "Epoch 43/1000\n",
            "104/104 [==============================] - 1s 8ms/step - loss: 1.7037 - accuracy: 0.2690 - val_loss: 1.6239 - val_accuracy: 0.4155\n",
            "Epoch 44/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 1.7144 - accuracy: 0.2848 - val_loss: 1.6257 - val_accuracy: 0.4010\n",
            "Epoch 45/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 1.7092 - accuracy: 0.2678 - val_loss: 1.6146 - val_accuracy: 0.3961\n",
            "Epoch 46/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 1.7132 - accuracy: 0.2848 - val_loss: 1.6062 - val_accuracy: 0.4300\n",
            "Epoch 47/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.7078 - accuracy: 0.2733 - val_loss: 1.6185 - val_accuracy: 0.4155\n",
            "Epoch 48/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.6815 - accuracy: 0.2890 - val_loss: 1.5986 - val_accuracy: 0.4589\n",
            "Epoch 49/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.6706 - accuracy: 0.2908 - val_loss: 1.6160 - val_accuracy: 0.3816\n",
            "Epoch 50/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.6692 - accuracy: 0.2938 - val_loss: 1.6063 - val_accuracy: 0.3768\n",
            "Epoch 51/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.6729 - accuracy: 0.3065 - val_loss: 1.6006 - val_accuracy: 0.3237\n",
            "Epoch 52/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.6403 - accuracy: 0.3114 - val_loss: 1.5924 - val_accuracy: 0.4155\n",
            "Epoch 53/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.6401 - accuracy: 0.3102 - val_loss: 1.6234 - val_accuracy: 0.3575\n",
            "Epoch 54/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.6559 - accuracy: 0.3011 - val_loss: 1.5652 - val_accuracy: 0.3865\n",
            "Epoch 55/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.6510 - accuracy: 0.3132 - val_loss: 1.5837 - val_accuracy: 0.3527\n",
            "Epoch 56/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.6435 - accuracy: 0.2999 - val_loss: 1.5866 - val_accuracy: 0.4203\n",
            "Epoch 57/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.6246 - accuracy: 0.3108 - val_loss: 1.5864 - val_accuracy: 0.3768\n",
            "Epoch 58/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.6244 - accuracy: 0.3168 - val_loss: 1.5931 - val_accuracy: 0.3623\n",
            "Epoch 59/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.6304 - accuracy: 0.3035 - val_loss: 1.5692 - val_accuracy: 0.4300\n",
            "Epoch 60/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.6153 - accuracy: 0.3247 - val_loss: 1.5958 - val_accuracy: 0.3865\n",
            "Epoch 61/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.6035 - accuracy: 0.3356 - val_loss: 1.5616 - val_accuracy: 0.4203\n",
            "Epoch 62/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.6179 - accuracy: 0.3440 - val_loss: 1.5711 - val_accuracy: 0.3768\n",
            "Epoch 63/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.5977 - accuracy: 0.3362 - val_loss: 1.5443 - val_accuracy: 0.4203\n",
            "Epoch 64/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.5942 - accuracy: 0.3343 - val_loss: 1.5377 - val_accuracy: 0.4155\n",
            "Epoch 65/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.5984 - accuracy: 0.3271 - val_loss: 1.5481 - val_accuracy: 0.4203\n",
            "Epoch 66/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.5880 - accuracy: 0.3579 - val_loss: 1.5219 - val_accuracy: 0.4638\n",
            "Epoch 67/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.5864 - accuracy: 0.3495 - val_loss: 1.5183 - val_accuracy: 0.4783\n",
            "Epoch 68/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.6034 - accuracy: 0.3277 - val_loss: 1.5347 - val_accuracy: 0.4251\n",
            "Epoch 69/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.5918 - accuracy: 0.3386 - val_loss: 1.5164 - val_accuracy: 0.4444\n",
            "Epoch 70/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.5780 - accuracy: 0.3434 - val_loss: 1.5251 - val_accuracy: 0.4106\n",
            "Epoch 71/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.5648 - accuracy: 0.3585 - val_loss: 1.5237 - val_accuracy: 0.4010\n",
            "Epoch 72/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.5891 - accuracy: 0.3495 - val_loss: 1.5036 - val_accuracy: 0.4155\n",
            "Epoch 73/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.5497 - accuracy: 0.3525 - val_loss: 1.5286 - val_accuracy: 0.4155\n",
            "Epoch 74/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.5453 - accuracy: 0.3561 - val_loss: 1.4943 - val_accuracy: 0.4493\n",
            "Epoch 75/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.5484 - accuracy: 0.3567 - val_loss: 1.4977 - val_accuracy: 0.4493\n",
            "Epoch 76/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.5456 - accuracy: 0.3622 - val_loss: 1.4869 - val_accuracy: 0.4928\n",
            "Epoch 77/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.5613 - accuracy: 0.3561 - val_loss: 1.4724 - val_accuracy: 0.4734\n",
            "Epoch 78/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.5285 - accuracy: 0.3664 - val_loss: 1.4616 - val_accuracy: 0.4686\n",
            "Epoch 79/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.5427 - accuracy: 0.3736 - val_loss: 1.4631 - val_accuracy: 0.5024\n",
            "Epoch 80/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.5265 - accuracy: 0.3664 - val_loss: 1.4772 - val_accuracy: 0.4348\n",
            "Epoch 81/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.5263 - accuracy: 0.3628 - val_loss: 1.4470 - val_accuracy: 0.4879\n",
            "Epoch 82/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.5213 - accuracy: 0.3664 - val_loss: 1.4429 - val_accuracy: 0.4831\n",
            "Epoch 83/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.5265 - accuracy: 0.3507 - val_loss: 1.4585 - val_accuracy: 0.4589\n",
            "Epoch 84/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.5134 - accuracy: 0.3724 - val_loss: 1.4538 - val_accuracy: 0.5024\n",
            "Epoch 85/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.5010 - accuracy: 0.3912 - val_loss: 1.4520 - val_accuracy: 0.5024\n",
            "Epoch 86/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.5006 - accuracy: 0.3845 - val_loss: 1.4490 - val_accuracy: 0.4444\n",
            "Epoch 87/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.4884 - accuracy: 0.3845 - val_loss: 1.4417 - val_accuracy: 0.4734\n",
            "Epoch 88/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.4880 - accuracy: 0.4069 - val_loss: 1.4273 - val_accuracy: 0.4444\n",
            "Epoch 89/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.4735 - accuracy: 0.3827 - val_loss: 1.4588 - val_accuracy: 0.4493\n",
            "Epoch 90/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.4829 - accuracy: 0.4087 - val_loss: 1.4039 - val_accuracy: 0.5121\n",
            "Epoch 91/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 1.4659 - accuracy: 0.4160 - val_loss: 1.4325 - val_accuracy: 0.4734\n",
            "Epoch 92/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 1.4675 - accuracy: 0.3990 - val_loss: 1.4242 - val_accuracy: 0.4589\n",
            "Epoch 93/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 1.4695 - accuracy: 0.4166 - val_loss: 1.4226 - val_accuracy: 0.4734\n",
            "Epoch 94/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 1.4630 - accuracy: 0.3990 - val_loss: 1.4305 - val_accuracy: 0.4396\n",
            "Epoch 95/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 1.4679 - accuracy: 0.4015 - val_loss: 1.4197 - val_accuracy: 0.4879\n",
            "Epoch 96/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.4583 - accuracy: 0.4015 - val_loss: 1.4012 - val_accuracy: 0.4928\n",
            "Epoch 97/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.4549 - accuracy: 0.4069 - val_loss: 1.4112 - val_accuracy: 0.5024\n",
            "Epoch 98/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.4303 - accuracy: 0.4178 - val_loss: 1.4031 - val_accuracy: 0.4783\n",
            "Epoch 99/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.4350 - accuracy: 0.4117 - val_loss: 1.4012 - val_accuracy: 0.4976\n",
            "Epoch 100/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.4371 - accuracy: 0.4051 - val_loss: 1.4186 - val_accuracy: 0.4396\n",
            "Epoch 101/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.4242 - accuracy: 0.4274 - val_loss: 1.3932 - val_accuracy: 0.4831\n",
            "Epoch 102/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.4472 - accuracy: 0.4099 - val_loss: 1.4073 - val_accuracy: 0.4638\n",
            "Epoch 103/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.4378 - accuracy: 0.4111 - val_loss: 1.3754 - val_accuracy: 0.5266\n",
            "Epoch 104/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.4130 - accuracy: 0.4268 - val_loss: 1.3676 - val_accuracy: 0.5024\n",
            "Epoch 105/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.4285 - accuracy: 0.4117 - val_loss: 1.3533 - val_accuracy: 0.5314\n",
            "Epoch 106/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.4073 - accuracy: 0.4244 - val_loss: 1.3604 - val_accuracy: 0.5362\n",
            "Epoch 107/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.4148 - accuracy: 0.4232 - val_loss: 1.3482 - val_accuracy: 0.5362\n",
            "Epoch 108/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.3885 - accuracy: 0.4432 - val_loss: 1.3447 - val_accuracy: 0.4976\n",
            "Epoch 109/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.4058 - accuracy: 0.4202 - val_loss: 1.3289 - val_accuracy: 0.4783\n",
            "Epoch 110/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.3756 - accuracy: 0.4371 - val_loss: 1.3227 - val_accuracy: 0.5556\n",
            "Epoch 111/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.3743 - accuracy: 0.4383 - val_loss: 1.3166 - val_accuracy: 0.5266\n",
            "Epoch 112/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.3999 - accuracy: 0.4172 - val_loss: 1.3370 - val_accuracy: 0.5266\n",
            "Epoch 113/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.3825 - accuracy: 0.4371 - val_loss: 1.3168 - val_accuracy: 0.5362\n",
            "Epoch 114/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.3543 - accuracy: 0.4486 - val_loss: 1.3260 - val_accuracy: 0.5024\n",
            "Epoch 115/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.3757 - accuracy: 0.4335 - val_loss: 1.3265 - val_accuracy: 0.5169\n",
            "Epoch 116/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.3615 - accuracy: 0.4468 - val_loss: 1.3104 - val_accuracy: 0.5072\n",
            "Epoch 117/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.3505 - accuracy: 0.4450 - val_loss: 1.2986 - val_accuracy: 0.5556\n",
            "Epoch 118/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.3649 - accuracy: 0.4438 - val_loss: 1.3303 - val_accuracy: 0.4638\n",
            "Epoch 119/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.3380 - accuracy: 0.4492 - val_loss: 1.2867 - val_accuracy: 0.5411\n",
            "Epoch 120/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.3426 - accuracy: 0.4553 - val_loss: 1.2996 - val_accuracy: 0.5314\n",
            "Epoch 121/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.3474 - accuracy: 0.4631 - val_loss: 1.3072 - val_accuracy: 0.5169\n",
            "Epoch 122/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.3430 - accuracy: 0.4426 - val_loss: 1.3183 - val_accuracy: 0.5072\n",
            "Epoch 123/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.3466 - accuracy: 0.4547 - val_loss: 1.2842 - val_accuracy: 0.5942\n",
            "Epoch 124/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.3221 - accuracy: 0.4541 - val_loss: 1.2626 - val_accuracy: 0.5894\n",
            "Epoch 125/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.3260 - accuracy: 0.4698 - val_loss: 1.2597 - val_accuracy: 0.5556\n",
            "Epoch 126/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.3205 - accuracy: 0.4619 - val_loss: 1.2661 - val_accuracy: 0.5700\n",
            "Epoch 127/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.3245 - accuracy: 0.4607 - val_loss: 1.2491 - val_accuracy: 0.5652\n",
            "Epoch 128/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.3189 - accuracy: 0.4619 - val_loss: 1.2473 - val_accuracy: 0.5652\n",
            "Epoch 129/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.3131 - accuracy: 0.4680 - val_loss: 1.2662 - val_accuracy: 0.5411\n",
            "Epoch 130/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.3053 - accuracy: 0.4637 - val_loss: 1.2475 - val_accuracy: 0.5652\n",
            "Epoch 131/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.2969 - accuracy: 0.4625 - val_loss: 1.2467 - val_accuracy: 0.5797\n",
            "Epoch 132/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.3083 - accuracy: 0.4704 - val_loss: 1.2406 - val_accuracy: 0.5604\n",
            "Epoch 133/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.2882 - accuracy: 0.4728 - val_loss: 1.2445 - val_accuracy: 0.5556\n",
            "Epoch 134/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.2954 - accuracy: 0.4849 - val_loss: 1.2538 - val_accuracy: 0.5749\n",
            "Epoch 135/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.3116 - accuracy: 0.4770 - val_loss: 1.2560 - val_accuracy: 0.5507\n",
            "Epoch 136/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.2667 - accuracy: 0.4885 - val_loss: 1.2433 - val_accuracy: 0.5652\n",
            "Epoch 137/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.2871 - accuracy: 0.4716 - val_loss: 1.2627 - val_accuracy: 0.5459\n",
            "Epoch 138/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 1.2632 - accuracy: 0.4946 - val_loss: 1.2396 - val_accuracy: 0.5604\n",
            "Epoch 139/1000\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 1.2736 - accuracy: 0.4921 - val_loss: 1.2240 - val_accuracy: 0.5652\n",
            "Epoch 140/1000\n",
            "104/104 [==============================] - 1s 8ms/step - loss: 1.2607 - accuracy: 0.4867 - val_loss: 1.2244 - val_accuracy: 0.5604\n",
            "Epoch 141/1000\n",
            "104/104 [==============================] - 1s 9ms/step - loss: 1.2792 - accuracy: 0.4692 - val_loss: 1.2417 - val_accuracy: 0.5459\n",
            "Epoch 142/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 1.2553 - accuracy: 0.4946 - val_loss: 1.2158 - val_accuracy: 0.5556\n",
            "Epoch 143/1000\n",
            "104/104 [==============================] - 1s 8ms/step - loss: 1.2669 - accuracy: 0.4655 - val_loss: 1.2022 - val_accuracy: 0.5942\n",
            "Epoch 144/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.2389 - accuracy: 0.5000 - val_loss: 1.2172 - val_accuracy: 0.5604\n",
            "Epoch 145/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.2344 - accuracy: 0.4976 - val_loss: 1.1937 - val_accuracy: 0.5845\n",
            "Epoch 146/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.2422 - accuracy: 0.5067 - val_loss: 1.2046 - val_accuracy: 0.5459\n",
            "Epoch 147/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.2489 - accuracy: 0.4946 - val_loss: 1.1986 - val_accuracy: 0.5990\n",
            "Epoch 148/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.2209 - accuracy: 0.5109 - val_loss: 1.1912 - val_accuracy: 0.5942\n",
            "Epoch 149/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.2435 - accuracy: 0.5000 - val_loss: 1.1835 - val_accuracy: 0.5942\n",
            "Epoch 150/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.2500 - accuracy: 0.4861 - val_loss: 1.1866 - val_accuracy: 0.5749\n",
            "Epoch 151/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.2185 - accuracy: 0.4994 - val_loss: 1.1775 - val_accuracy: 0.5700\n",
            "Epoch 152/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.2245 - accuracy: 0.5006 - val_loss: 1.1668 - val_accuracy: 0.5942\n",
            "Epoch 153/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.2161 - accuracy: 0.5109 - val_loss: 1.1633 - val_accuracy: 0.5894\n",
            "Epoch 154/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.2268 - accuracy: 0.5133 - val_loss: 1.1535 - val_accuracy: 0.5797\n",
            "Epoch 155/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.2094 - accuracy: 0.5103 - val_loss: 1.1530 - val_accuracy: 0.5797\n",
            "Epoch 156/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.2186 - accuracy: 0.4958 - val_loss: 1.1593 - val_accuracy: 0.5845\n",
            "Epoch 157/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.2040 - accuracy: 0.5036 - val_loss: 1.1826 - val_accuracy: 0.5556\n",
            "Epoch 158/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.2114 - accuracy: 0.5296 - val_loss: 1.1513 - val_accuracy: 0.5797\n",
            "Epoch 159/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.1901 - accuracy: 0.5042 - val_loss: 1.1615 - val_accuracy: 0.5507\n",
            "Epoch 160/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.1959 - accuracy: 0.5109 - val_loss: 1.1452 - val_accuracy: 0.5700\n",
            "Epoch 161/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.1989 - accuracy: 0.5157 - val_loss: 1.1446 - val_accuracy: 0.5749\n",
            "Epoch 162/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.2032 - accuracy: 0.5060 - val_loss: 1.1288 - val_accuracy: 0.5990\n",
            "Epoch 163/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.1929 - accuracy: 0.5109 - val_loss: 1.1319 - val_accuracy: 0.5749\n",
            "Epoch 164/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.1833 - accuracy: 0.5187 - val_loss: 1.1265 - val_accuracy: 0.5797\n",
            "Epoch 165/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.2006 - accuracy: 0.5169 - val_loss: 1.1319 - val_accuracy: 0.5990\n",
            "Epoch 166/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.1661 - accuracy: 0.5369 - val_loss: 1.1195 - val_accuracy: 0.5845\n",
            "Epoch 167/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.1931 - accuracy: 0.5109 - val_loss: 1.1590 - val_accuracy: 0.5749\n",
            "Epoch 168/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.1770 - accuracy: 0.5320 - val_loss: 1.1278 - val_accuracy: 0.5990\n",
            "Epoch 169/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.1859 - accuracy: 0.5333 - val_loss: 1.1109 - val_accuracy: 0.6039\n",
            "Epoch 170/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.1675 - accuracy: 0.5345 - val_loss: 1.1333 - val_accuracy: 0.5845\n",
            "Epoch 171/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.1657 - accuracy: 0.5375 - val_loss: 1.1369 - val_accuracy: 0.5797\n",
            "Epoch 172/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.1559 - accuracy: 0.5278 - val_loss: 1.1242 - val_accuracy: 0.5894\n",
            "Epoch 173/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.1632 - accuracy: 0.5357 - val_loss: 1.1098 - val_accuracy: 0.5845\n",
            "Epoch 174/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.1540 - accuracy: 0.5381 - val_loss: 1.1034 - val_accuracy: 0.5942\n",
            "Epoch 175/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.1488 - accuracy: 0.5472 - val_loss: 1.1107 - val_accuracy: 0.5700\n",
            "Epoch 176/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.1461 - accuracy: 0.5478 - val_loss: 1.0875 - val_accuracy: 0.6039\n",
            "Epoch 177/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.1533 - accuracy: 0.5333 - val_loss: 1.1160 - val_accuracy: 0.5652\n",
            "Epoch 178/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.1486 - accuracy: 0.5435 - val_loss: 1.0869 - val_accuracy: 0.6184\n",
            "Epoch 179/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.1405 - accuracy: 0.5441 - val_loss: 1.0851 - val_accuracy: 0.6232\n",
            "Epoch 180/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.1130 - accuracy: 0.5508 - val_loss: 1.0813 - val_accuracy: 0.5990\n",
            "Epoch 181/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.1300 - accuracy: 0.5441 - val_loss: 1.1113 - val_accuracy: 0.5797\n",
            "Epoch 182/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.1463 - accuracy: 0.5381 - val_loss: 1.1170 - val_accuracy: 0.5894\n",
            "Epoch 183/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.1474 - accuracy: 0.5405 - val_loss: 1.0910 - val_accuracy: 0.6135\n",
            "Epoch 184/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.1263 - accuracy: 0.5272 - val_loss: 1.0925 - val_accuracy: 0.6087\n",
            "Epoch 185/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0958 - accuracy: 0.5532 - val_loss: 1.0745 - val_accuracy: 0.6184\n",
            "Epoch 186/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.1198 - accuracy: 0.5568 - val_loss: 1.0816 - val_accuracy: 0.5700\n",
            "Epoch 187/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.1410 - accuracy: 0.5345 - val_loss: 1.0738 - val_accuracy: 0.5990\n",
            "Epoch 188/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.1226 - accuracy: 0.5423 - val_loss: 1.0752 - val_accuracy: 0.6039\n",
            "Epoch 189/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.1219 - accuracy: 0.5472 - val_loss: 1.0824 - val_accuracy: 0.6232\n",
            "Epoch 190/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.1395 - accuracy: 0.5562 - val_loss: 1.0828 - val_accuracy: 0.6087\n",
            "Epoch 191/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0994 - accuracy: 0.5653 - val_loss: 1.0893 - val_accuracy: 0.5845\n",
            "Epoch 192/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.1069 - accuracy: 0.5459 - val_loss: 1.0799 - val_accuracy: 0.5894\n",
            "Epoch 193/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.1295 - accuracy: 0.5441 - val_loss: 1.0812 - val_accuracy: 0.5845\n",
            "Epoch 194/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.0844 - accuracy: 0.5653 - val_loss: 1.0933 - val_accuracy: 0.5700\n",
            "Epoch 195/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0895 - accuracy: 0.5550 - val_loss: 1.0729 - val_accuracy: 0.5797\n",
            "Epoch 196/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.1111 - accuracy: 0.5641 - val_loss: 1.0820 - val_accuracy: 0.5894\n",
            "Epoch 197/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0925 - accuracy: 0.5568 - val_loss: 1.0801 - val_accuracy: 0.5942\n",
            "Epoch 198/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0742 - accuracy: 0.5726 - val_loss: 1.0616 - val_accuracy: 0.6135\n",
            "Epoch 199/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0782 - accuracy: 0.5611 - val_loss: 1.0556 - val_accuracy: 0.6135\n",
            "Epoch 200/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0881 - accuracy: 0.5574 - val_loss: 1.0910 - val_accuracy: 0.5700\n",
            "Epoch 201/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0817 - accuracy: 0.5562 - val_loss: 1.0652 - val_accuracy: 0.6135\n",
            "Epoch 202/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0871 - accuracy: 0.5647 - val_loss: 1.0628 - val_accuracy: 0.5894\n",
            "Epoch 203/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0817 - accuracy: 0.5677 - val_loss: 1.0674 - val_accuracy: 0.6425\n",
            "Epoch 204/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.0655 - accuracy: 0.5659 - val_loss: 1.0612 - val_accuracy: 0.5845\n",
            "Epoch 205/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0677 - accuracy: 0.5840 - val_loss: 1.0776 - val_accuracy: 0.5652\n",
            "Epoch 206/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.0770 - accuracy: 0.5768 - val_loss: 1.0272 - val_accuracy: 0.6473\n",
            "Epoch 207/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0665 - accuracy: 0.5750 - val_loss: 1.0487 - val_accuracy: 0.5990\n",
            "Epoch 208/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0689 - accuracy: 0.5756 - val_loss: 1.0346 - val_accuracy: 0.5990\n",
            "Epoch 209/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0611 - accuracy: 0.5707 - val_loss: 1.0369 - val_accuracy: 0.6135\n",
            "Epoch 210/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0511 - accuracy: 0.5780 - val_loss: 1.0360 - val_accuracy: 0.5990\n",
            "Epoch 211/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0492 - accuracy: 0.5780 - val_loss: 1.0475 - val_accuracy: 0.5942\n",
            "Epoch 212/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0386 - accuracy: 0.5846 - val_loss: 1.0264 - val_accuracy: 0.5894\n",
            "Epoch 213/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0650 - accuracy: 0.5828 - val_loss: 1.0321 - val_accuracy: 0.6039\n",
            "Epoch 214/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0566 - accuracy: 0.5701 - val_loss: 1.0307 - val_accuracy: 0.6087\n",
            "Epoch 215/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0626 - accuracy: 0.5689 - val_loss: 1.0133 - val_accuracy: 0.6425\n",
            "Epoch 216/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0663 - accuracy: 0.5810 - val_loss: 1.0208 - val_accuracy: 0.5942\n",
            "Epoch 217/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0356 - accuracy: 0.5774 - val_loss: 1.0197 - val_accuracy: 0.6184\n",
            "Epoch 218/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0525 - accuracy: 0.5732 - val_loss: 1.0436 - val_accuracy: 0.5990\n",
            "Epoch 219/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0322 - accuracy: 0.5871 - val_loss: 1.0404 - val_accuracy: 0.6184\n",
            "Epoch 220/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0399 - accuracy: 0.5834 - val_loss: 1.0673 - val_accuracy: 0.6087\n",
            "Epoch 221/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0271 - accuracy: 0.5925 - val_loss: 1.0508 - val_accuracy: 0.5797\n",
            "Epoch 222/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0343 - accuracy: 0.5822 - val_loss: 1.0215 - val_accuracy: 0.6135\n",
            "Epoch 223/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0383 - accuracy: 0.5907 - val_loss: 1.0389 - val_accuracy: 0.5894\n",
            "Epoch 224/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0313 - accuracy: 0.5840 - val_loss: 1.0100 - val_accuracy: 0.6135\n",
            "Epoch 225/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0161 - accuracy: 0.6088 - val_loss: 0.9967 - val_accuracy: 0.6570\n",
            "Epoch 226/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0251 - accuracy: 0.5901 - val_loss: 1.0190 - val_accuracy: 0.5942\n",
            "Epoch 227/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0304 - accuracy: 0.5895 - val_loss: 1.0274 - val_accuracy: 0.5797\n",
            "Epoch 228/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0195 - accuracy: 0.5907 - val_loss: 1.0102 - val_accuracy: 0.5942\n",
            "Epoch 229/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0325 - accuracy: 0.5937 - val_loss: 0.9960 - val_accuracy: 0.6184\n",
            "Epoch 230/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0056 - accuracy: 0.5913 - val_loss: 1.0018 - val_accuracy: 0.6087\n",
            "Epoch 231/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9983 - accuracy: 0.6028 - val_loss: 1.0096 - val_accuracy: 0.6087\n",
            "Epoch 232/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0237 - accuracy: 0.5859 - val_loss: 0.9917 - val_accuracy: 0.6329\n",
            "Epoch 233/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0112 - accuracy: 0.5877 - val_loss: 0.9983 - val_accuracy: 0.6280\n",
            "Epoch 234/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0030 - accuracy: 0.5955 - val_loss: 1.0009 - val_accuracy: 0.6135\n",
            "Epoch 235/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9997 - accuracy: 0.6028 - val_loss: 0.9849 - val_accuracy: 0.6425\n",
            "Epoch 236/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0087 - accuracy: 0.5992 - val_loss: 1.0132 - val_accuracy: 0.5652\n",
            "Epoch 237/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.9848 - accuracy: 0.6076 - val_loss: 0.9805 - val_accuracy: 0.6280\n",
            "Epoch 238/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.9763 - accuracy: 0.6252 - val_loss: 0.9772 - val_accuracy: 0.6087\n",
            "Epoch 239/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9950 - accuracy: 0.6155 - val_loss: 0.9860 - val_accuracy: 0.5894\n",
            "Epoch 240/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9905 - accuracy: 0.6064 - val_loss: 0.9833 - val_accuracy: 0.6280\n",
            "Epoch 241/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0012 - accuracy: 0.6022 - val_loss: 0.9935 - val_accuracy: 0.5990\n",
            "Epoch 242/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0010 - accuracy: 0.5961 - val_loss: 0.9790 - val_accuracy: 0.6280\n",
            "Epoch 243/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.9874 - accuracy: 0.6143 - val_loss: 0.9736 - val_accuracy: 0.6522\n",
            "Epoch 244/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0011 - accuracy: 0.6046 - val_loss: 0.9786 - val_accuracy: 0.5990\n",
            "Epoch 245/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9904 - accuracy: 0.6076 - val_loss: 0.9921 - val_accuracy: 0.6087\n",
            "Epoch 246/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9675 - accuracy: 0.6112 - val_loss: 0.9730 - val_accuracy: 0.6184\n",
            "Epoch 247/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.9753 - accuracy: 0.6252 - val_loss: 0.9827 - val_accuracy: 0.6135\n",
            "Epoch 248/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9631 - accuracy: 0.6215 - val_loss: 1.0103 - val_accuracy: 0.6377\n",
            "Epoch 249/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.9929 - accuracy: 0.6088 - val_loss: 0.9656 - val_accuracy: 0.6425\n",
            "Epoch 250/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9822 - accuracy: 0.6022 - val_loss: 0.9635 - val_accuracy: 0.6377\n",
            "Epoch 251/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9877 - accuracy: 0.6082 - val_loss: 0.9571 - val_accuracy: 0.6473\n",
            "Epoch 252/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9523 - accuracy: 0.6227 - val_loss: 0.9634 - val_accuracy: 0.6377\n",
            "Epoch 253/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.9706 - accuracy: 0.6252 - val_loss: 0.9674 - val_accuracy: 0.6232\n",
            "Epoch 254/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9813 - accuracy: 0.6137 - val_loss: 0.9682 - val_accuracy: 0.6135\n",
            "Epoch 255/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9786 - accuracy: 0.6088 - val_loss: 0.9728 - val_accuracy: 0.6522\n",
            "Epoch 256/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9569 - accuracy: 0.6161 - val_loss: 0.9612 - val_accuracy: 0.6329\n",
            "Epoch 257/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9538 - accuracy: 0.6215 - val_loss: 0.9687 - val_accuracy: 0.6667\n",
            "Epoch 258/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9621 - accuracy: 0.6252 - val_loss: 0.9822 - val_accuracy: 0.6377\n",
            "Epoch 259/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9470 - accuracy: 0.6215 - val_loss: 0.9496 - val_accuracy: 0.6522\n",
            "Epoch 260/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9569 - accuracy: 0.6082 - val_loss: 0.9762 - val_accuracy: 0.5990\n",
            "Epoch 261/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9424 - accuracy: 0.6131 - val_loss: 0.9616 - val_accuracy: 0.6232\n",
            "Epoch 262/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9636 - accuracy: 0.6155 - val_loss: 0.9399 - val_accuracy: 0.6715\n",
            "Epoch 263/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9497 - accuracy: 0.6245 - val_loss: 0.9353 - val_accuracy: 0.6522\n",
            "Epoch 264/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9369 - accuracy: 0.6324 - val_loss: 0.9694 - val_accuracy: 0.6135\n",
            "Epoch 265/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9541 - accuracy: 0.6203 - val_loss: 0.9642 - val_accuracy: 0.6184\n",
            "Epoch 266/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9583 - accuracy: 0.6131 - val_loss: 0.9553 - val_accuracy: 0.6184\n",
            "Epoch 267/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9438 - accuracy: 0.6155 - val_loss: 0.9392 - val_accuracy: 0.6377\n",
            "Epoch 268/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9413 - accuracy: 0.6403 - val_loss: 0.9375 - val_accuracy: 0.6377\n",
            "Epoch 269/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9190 - accuracy: 0.6282 - val_loss: 0.9457 - val_accuracy: 0.6087\n",
            "Epoch 270/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9294 - accuracy: 0.6397 - val_loss: 0.9398 - val_accuracy: 0.6570\n",
            "Epoch 271/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9376 - accuracy: 0.6270 - val_loss: 0.9403 - val_accuracy: 0.6618\n",
            "Epoch 272/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9551 - accuracy: 0.6094 - val_loss: 0.9535 - val_accuracy: 0.6473\n",
            "Epoch 273/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9441 - accuracy: 0.6088 - val_loss: 0.9496 - val_accuracy: 0.6570\n",
            "Epoch 274/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9212 - accuracy: 0.6348 - val_loss: 0.9382 - val_accuracy: 0.6570\n",
            "Epoch 275/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9357 - accuracy: 0.6209 - val_loss: 0.9366 - val_accuracy: 0.6377\n",
            "Epoch 276/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9244 - accuracy: 0.6427 - val_loss: 0.9514 - val_accuracy: 0.6232\n",
            "Epoch 277/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9267 - accuracy: 0.6300 - val_loss: 0.9355 - val_accuracy: 0.6425\n",
            "Epoch 278/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9105 - accuracy: 0.6385 - val_loss: 0.9377 - val_accuracy: 0.6570\n",
            "Epoch 279/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.9023 - accuracy: 0.6421 - val_loss: 0.9277 - val_accuracy: 0.6667\n",
            "Epoch 280/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9332 - accuracy: 0.6427 - val_loss: 0.9491 - val_accuracy: 0.6473\n",
            "Epoch 281/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9145 - accuracy: 0.6445 - val_loss: 0.9308 - val_accuracy: 0.6329\n",
            "Epoch 282/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9183 - accuracy: 0.6306 - val_loss: 0.9380 - val_accuracy: 0.6280\n",
            "Epoch 283/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9141 - accuracy: 0.6397 - val_loss: 0.9343 - val_accuracy: 0.6280\n",
            "Epoch 284/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9113 - accuracy: 0.6481 - val_loss: 0.9162 - val_accuracy: 0.6715\n",
            "Epoch 285/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9225 - accuracy: 0.6342 - val_loss: 0.9128 - val_accuracy: 0.6763\n",
            "Epoch 286/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9130 - accuracy: 0.6378 - val_loss: 0.9158 - val_accuracy: 0.6473\n",
            "Epoch 287/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9077 - accuracy: 0.6511 - val_loss: 0.9463 - val_accuracy: 0.5845\n",
            "Epoch 288/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8943 - accuracy: 0.6499 - val_loss: 0.8988 - val_accuracy: 0.6715\n",
            "Epoch 289/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8952 - accuracy: 0.6451 - val_loss: 0.9018 - val_accuracy: 0.6618\n",
            "Epoch 290/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9113 - accuracy: 0.6276 - val_loss: 0.9377 - val_accuracy: 0.6425\n",
            "Epoch 291/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9097 - accuracy: 0.6469 - val_loss: 0.9107 - val_accuracy: 0.6473\n",
            "Epoch 292/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8932 - accuracy: 0.6518 - val_loss: 0.9161 - val_accuracy: 0.6522\n",
            "Epoch 293/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8964 - accuracy: 0.6487 - val_loss: 0.9083 - val_accuracy: 0.6618\n",
            "Epoch 294/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8870 - accuracy: 0.6524 - val_loss: 0.9053 - val_accuracy: 0.6860\n",
            "Epoch 295/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8679 - accuracy: 0.6475 - val_loss: 0.9017 - val_accuracy: 0.6377\n",
            "Epoch 296/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.8924 - accuracy: 0.6445 - val_loss: 0.9091 - val_accuracy: 0.6522\n",
            "Epoch 297/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.8955 - accuracy: 0.6457 - val_loss: 0.8955 - val_accuracy: 0.6763\n",
            "Epoch 298/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.8913 - accuracy: 0.6487 - val_loss: 0.9093 - val_accuracy: 0.6618\n",
            "Epoch 299/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8918 - accuracy: 0.6439 - val_loss: 0.9019 - val_accuracy: 0.6570\n",
            "Epoch 300/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8935 - accuracy: 0.6427 - val_loss: 0.9108 - val_accuracy: 0.6763\n",
            "Epoch 301/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8760 - accuracy: 0.6530 - val_loss: 0.8956 - val_accuracy: 0.6667\n",
            "Epoch 302/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.8801 - accuracy: 0.6590 - val_loss: 0.9095 - val_accuracy: 0.6522\n",
            "Epoch 303/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8808 - accuracy: 0.6360 - val_loss: 0.8929 - val_accuracy: 0.6667\n",
            "Epoch 304/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8684 - accuracy: 0.6596 - val_loss: 0.8988 - val_accuracy: 0.6473\n",
            "Epoch 305/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8912 - accuracy: 0.6463 - val_loss: 0.8964 - val_accuracy: 0.6618\n",
            "Epoch 306/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8718 - accuracy: 0.6596 - val_loss: 0.8913 - val_accuracy: 0.6860\n",
            "Epoch 307/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8647 - accuracy: 0.6518 - val_loss: 0.8910 - val_accuracy: 0.6860\n",
            "Epoch 308/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.8648 - accuracy: 0.6711 - val_loss: 0.8734 - val_accuracy: 0.6908\n",
            "Epoch 309/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.8516 - accuracy: 0.6693 - val_loss: 0.9133 - val_accuracy: 0.6473\n",
            "Epoch 310/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8622 - accuracy: 0.6657 - val_loss: 0.8818 - val_accuracy: 0.6860\n",
            "Epoch 311/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8720 - accuracy: 0.6415 - val_loss: 0.8963 - val_accuracy: 0.6715\n",
            "Epoch 312/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8868 - accuracy: 0.6469 - val_loss: 0.8903 - val_accuracy: 0.6570\n",
            "Epoch 313/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.8511 - accuracy: 0.6602 - val_loss: 0.8868 - val_accuracy: 0.6763\n",
            "Epoch 314/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8758 - accuracy: 0.6421 - val_loss: 0.8855 - val_accuracy: 0.6618\n",
            "Epoch 315/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8461 - accuracy: 0.6560 - val_loss: 0.8772 - val_accuracy: 0.6715\n",
            "Epoch 316/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.8542 - accuracy: 0.6735 - val_loss: 0.8841 - val_accuracy: 0.6618\n",
            "Epoch 317/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8802 - accuracy: 0.6511 - val_loss: 0.8711 - val_accuracy: 0.6957\n",
            "Epoch 318/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8803 - accuracy: 0.6584 - val_loss: 0.8814 - val_accuracy: 0.6715\n",
            "Epoch 319/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8452 - accuracy: 0.6590 - val_loss: 0.8870 - val_accuracy: 0.6667\n",
            "Epoch 320/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8621 - accuracy: 0.6499 - val_loss: 0.8812 - val_accuracy: 0.6908\n",
            "Epoch 321/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8402 - accuracy: 0.6584 - val_loss: 0.8797 - val_accuracy: 0.6812\n",
            "Epoch 322/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8525 - accuracy: 0.6542 - val_loss: 0.8784 - val_accuracy: 0.6812\n",
            "Epoch 323/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.8520 - accuracy: 0.6530 - val_loss: 0.8786 - val_accuracy: 0.6812\n",
            "Epoch 324/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8186 - accuracy: 0.6759 - val_loss: 0.8618 - val_accuracy: 0.6908\n",
            "Epoch 325/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8599 - accuracy: 0.6626 - val_loss: 0.8885 - val_accuracy: 0.6763\n",
            "Epoch 326/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8621 - accuracy: 0.6584 - val_loss: 0.8881 - val_accuracy: 0.6425\n",
            "Epoch 327/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8280 - accuracy: 0.6759 - val_loss: 0.8785 - val_accuracy: 0.6570\n",
            "Epoch 328/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8296 - accuracy: 0.6735 - val_loss: 0.8682 - val_accuracy: 0.6667\n",
            "Epoch 329/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8430 - accuracy: 0.6560 - val_loss: 0.8661 - val_accuracy: 0.6908\n",
            "Epoch 330/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.8397 - accuracy: 0.6681 - val_loss: 0.8741 - val_accuracy: 0.6522\n",
            "Epoch 331/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8479 - accuracy: 0.6826 - val_loss: 0.8671 - val_accuracy: 0.6715\n",
            "Epoch 332/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8200 - accuracy: 0.6771 - val_loss: 0.8617 - val_accuracy: 0.7101\n",
            "Epoch 333/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8395 - accuracy: 0.6644 - val_loss: 0.8707 - val_accuracy: 0.6860\n",
            "Epoch 334/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.8421 - accuracy: 0.6687 - val_loss: 0.8891 - val_accuracy: 0.6425\n",
            "Epoch 335/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8462 - accuracy: 0.6632 - val_loss: 0.8868 - val_accuracy: 0.6425\n",
            "Epoch 336/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8367 - accuracy: 0.6651 - val_loss: 0.8687 - val_accuracy: 0.6715\n",
            "Epoch 337/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.8376 - accuracy: 0.6590 - val_loss: 0.8789 - val_accuracy: 0.6570\n",
            "Epoch 338/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.8298 - accuracy: 0.6796 - val_loss: 0.8636 - val_accuracy: 0.6860\n",
            "Epoch 339/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8260 - accuracy: 0.6941 - val_loss: 0.8685 - val_accuracy: 0.6715\n",
            "Epoch 340/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8207 - accuracy: 0.6778 - val_loss: 0.8594 - val_accuracy: 0.6860\n",
            "Epoch 341/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.8325 - accuracy: 0.6850 - val_loss: 0.8686 - val_accuracy: 0.6763\n",
            "Epoch 342/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.8337 - accuracy: 0.6771 - val_loss: 0.8686 - val_accuracy: 0.6957\n",
            "Epoch 343/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.8271 - accuracy: 0.6796 - val_loss: 0.8554 - val_accuracy: 0.6860\n",
            "Epoch 344/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8100 - accuracy: 0.6862 - val_loss: 0.8589 - val_accuracy: 0.6763\n",
            "Epoch 345/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.8234 - accuracy: 0.6669 - val_loss: 0.8588 - val_accuracy: 0.6763\n",
            "Epoch 346/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8064 - accuracy: 0.6820 - val_loss: 0.8536 - val_accuracy: 0.6812\n",
            "Epoch 347/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8112 - accuracy: 0.6784 - val_loss: 0.8459 - val_accuracy: 0.7246\n",
            "Epoch 348/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7899 - accuracy: 0.6977 - val_loss: 0.8526 - val_accuracy: 0.6908\n",
            "Epoch 349/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8334 - accuracy: 0.6771 - val_loss: 0.8511 - val_accuracy: 0.6957\n",
            "Epoch 350/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.8165 - accuracy: 0.6790 - val_loss: 0.8362 - val_accuracy: 0.6908\n",
            "Epoch 351/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8052 - accuracy: 0.6874 - val_loss: 0.8606 - val_accuracy: 0.6570\n",
            "Epoch 352/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7993 - accuracy: 0.6856 - val_loss: 0.8648 - val_accuracy: 0.6763\n",
            "Epoch 353/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8116 - accuracy: 0.6771 - val_loss: 0.8459 - val_accuracy: 0.6763\n",
            "Epoch 354/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8219 - accuracy: 0.6790 - val_loss: 0.8498 - val_accuracy: 0.6812\n",
            "Epoch 355/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8107 - accuracy: 0.6820 - val_loss: 0.8355 - val_accuracy: 0.6860\n",
            "Epoch 356/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8120 - accuracy: 0.6802 - val_loss: 0.8574 - val_accuracy: 0.6522\n",
            "Epoch 357/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8281 - accuracy: 0.6802 - val_loss: 0.8449 - val_accuracy: 0.6618\n",
            "Epoch 358/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7763 - accuracy: 0.6850 - val_loss: 0.8415 - val_accuracy: 0.6618\n",
            "Epoch 359/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.8127 - accuracy: 0.6850 - val_loss: 0.8379 - val_accuracy: 0.6667\n",
            "Epoch 360/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.8023 - accuracy: 0.6826 - val_loss: 0.8383 - val_accuracy: 0.6618\n",
            "Epoch 361/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.8016 - accuracy: 0.6880 - val_loss: 0.8497 - val_accuracy: 0.6812\n",
            "Epoch 362/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7708 - accuracy: 0.7019 - val_loss: 0.8528 - val_accuracy: 0.6425\n",
            "Epoch 363/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.8105 - accuracy: 0.6814 - val_loss: 0.8483 - val_accuracy: 0.6812\n",
            "Epoch 364/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7906 - accuracy: 0.6959 - val_loss: 0.8247 - val_accuracy: 0.7005\n",
            "Epoch 365/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7822 - accuracy: 0.6965 - val_loss: 0.8340 - val_accuracy: 0.6957\n",
            "Epoch 366/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7982 - accuracy: 0.6778 - val_loss: 0.8207 - val_accuracy: 0.6908\n",
            "Epoch 367/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7884 - accuracy: 0.6856 - val_loss: 0.8311 - val_accuracy: 0.6763\n",
            "Epoch 368/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7963 - accuracy: 0.6941 - val_loss: 0.8190 - val_accuracy: 0.6908\n",
            "Epoch 369/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8036 - accuracy: 0.6802 - val_loss: 0.8256 - val_accuracy: 0.7005\n",
            "Epoch 370/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7551 - accuracy: 0.7086 - val_loss: 0.8412 - val_accuracy: 0.6715\n",
            "Epoch 371/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7801 - accuracy: 0.6953 - val_loss: 0.8325 - val_accuracy: 0.6763\n",
            "Epoch 372/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8090 - accuracy: 0.6729 - val_loss: 0.8308 - val_accuracy: 0.6860\n",
            "Epoch 373/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8019 - accuracy: 0.6759 - val_loss: 0.8375 - val_accuracy: 0.6812\n",
            "Epoch 374/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7510 - accuracy: 0.7086 - val_loss: 0.8247 - val_accuracy: 0.6957\n",
            "Epoch 375/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7869 - accuracy: 0.6965 - val_loss: 0.8323 - val_accuracy: 0.6570\n",
            "Epoch 376/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7826 - accuracy: 0.6892 - val_loss: 0.8203 - val_accuracy: 0.6812\n",
            "Epoch 377/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7832 - accuracy: 0.6989 - val_loss: 0.8393 - val_accuracy: 0.6570\n",
            "Epoch 378/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8020 - accuracy: 0.6898 - val_loss: 0.8463 - val_accuracy: 0.6570\n",
            "Epoch 379/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7913 - accuracy: 0.6983 - val_loss: 0.8408 - val_accuracy: 0.7005\n",
            "Epoch 380/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7692 - accuracy: 0.6977 - val_loss: 0.8279 - val_accuracy: 0.7053\n",
            "Epoch 381/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7716 - accuracy: 0.7013 - val_loss: 0.8285 - val_accuracy: 0.7053\n",
            "Epoch 382/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7714 - accuracy: 0.7001 - val_loss: 0.8266 - val_accuracy: 0.6957\n",
            "Epoch 383/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7706 - accuracy: 0.6911 - val_loss: 0.8290 - val_accuracy: 0.7101\n",
            "Epoch 384/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7676 - accuracy: 0.6977 - val_loss: 0.8331 - val_accuracy: 0.7005\n",
            "Epoch 385/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7694 - accuracy: 0.6953 - val_loss: 0.8168 - val_accuracy: 0.6812\n",
            "Epoch 386/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7811 - accuracy: 0.6892 - val_loss: 0.8374 - val_accuracy: 0.6908\n",
            "Epoch 387/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7923 - accuracy: 0.6892 - val_loss: 0.8109 - val_accuracy: 0.6860\n",
            "Epoch 388/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7627 - accuracy: 0.7068 - val_loss: 0.8111 - val_accuracy: 0.7101\n",
            "Epoch 389/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7619 - accuracy: 0.6953 - val_loss: 0.8205 - val_accuracy: 0.6957\n",
            "Epoch 390/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7657 - accuracy: 0.7092 - val_loss: 0.8310 - val_accuracy: 0.6860\n",
            "Epoch 391/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7548 - accuracy: 0.7031 - val_loss: 0.8306 - val_accuracy: 0.6763\n",
            "Epoch 392/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7596 - accuracy: 0.7037 - val_loss: 0.8342 - val_accuracy: 0.6957\n",
            "Epoch 393/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7604 - accuracy: 0.6959 - val_loss: 0.8244 - val_accuracy: 0.6667\n",
            "Epoch 394/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7556 - accuracy: 0.6911 - val_loss: 0.8166 - val_accuracy: 0.6570\n",
            "Epoch 395/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7648 - accuracy: 0.6965 - val_loss: 0.8116 - val_accuracy: 0.6957\n",
            "Epoch 396/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7527 - accuracy: 0.6941 - val_loss: 0.8163 - val_accuracy: 0.6763\n",
            "Epoch 397/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7527 - accuracy: 0.7104 - val_loss: 0.8333 - val_accuracy: 0.7053\n",
            "Epoch 398/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7719 - accuracy: 0.6965 - val_loss: 0.8112 - val_accuracy: 0.7150\n",
            "Epoch 399/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7414 - accuracy: 0.7110 - val_loss: 0.8093 - val_accuracy: 0.7005\n",
            "Epoch 400/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7409 - accuracy: 0.7134 - val_loss: 0.8130 - val_accuracy: 0.6957\n",
            "Epoch 401/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7645 - accuracy: 0.6953 - val_loss: 0.8057 - val_accuracy: 0.7005\n",
            "Epoch 402/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7286 - accuracy: 0.7170 - val_loss: 0.8172 - val_accuracy: 0.6763\n",
            "Epoch 403/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7435 - accuracy: 0.7104 - val_loss: 0.8024 - val_accuracy: 0.7053\n",
            "Epoch 404/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7480 - accuracy: 0.7104 - val_loss: 0.8048 - val_accuracy: 0.7053\n",
            "Epoch 405/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7356 - accuracy: 0.6971 - val_loss: 0.8080 - val_accuracy: 0.6763\n",
            "Epoch 406/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7378 - accuracy: 0.7116 - val_loss: 0.8061 - val_accuracy: 0.6908\n",
            "Epoch 407/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7354 - accuracy: 0.7146 - val_loss: 0.8109 - val_accuracy: 0.6908\n",
            "Epoch 408/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7345 - accuracy: 0.7056 - val_loss: 0.8027 - val_accuracy: 0.6908\n",
            "Epoch 409/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7325 - accuracy: 0.7098 - val_loss: 0.7996 - val_accuracy: 0.6860\n",
            "Epoch 410/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7281 - accuracy: 0.7213 - val_loss: 0.8071 - val_accuracy: 0.6860\n",
            "Epoch 411/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7285 - accuracy: 0.7134 - val_loss: 0.7881 - val_accuracy: 0.7005\n",
            "Epoch 412/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7474 - accuracy: 0.7110 - val_loss: 0.8025 - val_accuracy: 0.6763\n",
            "Epoch 413/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7158 - accuracy: 0.7189 - val_loss: 0.7982 - val_accuracy: 0.7005\n",
            "Epoch 414/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7394 - accuracy: 0.7177 - val_loss: 0.7903 - val_accuracy: 0.7295\n",
            "Epoch 415/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7327 - accuracy: 0.7025 - val_loss: 0.7920 - val_accuracy: 0.7053\n",
            "Epoch 416/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7299 - accuracy: 0.7074 - val_loss: 0.7984 - val_accuracy: 0.7053\n",
            "Epoch 417/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7447 - accuracy: 0.7110 - val_loss: 0.8023 - val_accuracy: 0.6860\n",
            "Epoch 418/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7538 - accuracy: 0.7031 - val_loss: 0.8056 - val_accuracy: 0.7005\n",
            "Epoch 419/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7273 - accuracy: 0.7116 - val_loss: 0.7980 - val_accuracy: 0.7101\n",
            "Epoch 420/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7395 - accuracy: 0.7170 - val_loss: 0.8121 - val_accuracy: 0.6812\n",
            "Epoch 421/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7064 - accuracy: 0.7170 - val_loss: 0.8095 - val_accuracy: 0.6667\n",
            "Epoch 422/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7170 - accuracy: 0.7152 - val_loss: 0.8073 - val_accuracy: 0.6908\n",
            "Epoch 423/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7249 - accuracy: 0.7207 - val_loss: 0.8038 - val_accuracy: 0.7005\n",
            "Epoch 424/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7238 - accuracy: 0.7158 - val_loss: 0.7935 - val_accuracy: 0.7150\n",
            "Epoch 425/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7110 - accuracy: 0.7189 - val_loss: 0.8201 - val_accuracy: 0.6473\n",
            "Epoch 426/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7120 - accuracy: 0.7177 - val_loss: 0.7744 - val_accuracy: 0.7198\n",
            "Epoch 427/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7056 - accuracy: 0.7285 - val_loss: 0.7939 - val_accuracy: 0.7053\n",
            "Epoch 428/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7206 - accuracy: 0.7273 - val_loss: 0.7764 - val_accuracy: 0.7295\n",
            "Epoch 429/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6981 - accuracy: 0.7382 - val_loss: 0.7750 - val_accuracy: 0.7150\n",
            "Epoch 430/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7168 - accuracy: 0.7189 - val_loss: 0.7889 - val_accuracy: 0.6957\n",
            "Epoch 431/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7076 - accuracy: 0.7134 - val_loss: 0.7934 - val_accuracy: 0.6763\n",
            "Epoch 432/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6969 - accuracy: 0.7231 - val_loss: 0.7888 - val_accuracy: 0.6763\n",
            "Epoch 433/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7254 - accuracy: 0.7164 - val_loss: 0.7745 - val_accuracy: 0.7053\n",
            "Epoch 434/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7119 - accuracy: 0.7261 - val_loss: 0.7780 - val_accuracy: 0.7101\n",
            "Epoch 435/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6934 - accuracy: 0.7376 - val_loss: 0.7969 - val_accuracy: 0.6908\n",
            "Epoch 436/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7150 - accuracy: 0.7310 - val_loss: 0.7914 - val_accuracy: 0.6860\n",
            "Epoch 437/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7078 - accuracy: 0.7195 - val_loss: 0.7691 - val_accuracy: 0.7150\n",
            "Epoch 438/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7204 - accuracy: 0.7098 - val_loss: 0.7809 - val_accuracy: 0.7005\n",
            "Epoch 439/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7012 - accuracy: 0.7207 - val_loss: 0.7879 - val_accuracy: 0.7053\n",
            "Epoch 440/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7103 - accuracy: 0.7225 - val_loss: 0.7839 - val_accuracy: 0.6908\n",
            "Epoch 441/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7004 - accuracy: 0.7189 - val_loss: 0.7879 - val_accuracy: 0.6908\n",
            "Epoch 442/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7076 - accuracy: 0.7207 - val_loss: 0.7869 - val_accuracy: 0.6860\n",
            "Epoch 443/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6888 - accuracy: 0.7177 - val_loss: 0.8121 - val_accuracy: 0.6618\n",
            "Epoch 444/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7204 - accuracy: 0.7219 - val_loss: 0.7638 - val_accuracy: 0.7246\n",
            "Epoch 445/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7022 - accuracy: 0.7285 - val_loss: 0.7688 - val_accuracy: 0.7295\n",
            "Epoch 446/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7260 - accuracy: 0.7080 - val_loss: 0.7804 - val_accuracy: 0.6957\n",
            "Epoch 447/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6941 - accuracy: 0.7237 - val_loss: 0.7758 - val_accuracy: 0.7005\n",
            "Epoch 448/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6939 - accuracy: 0.7322 - val_loss: 0.7709 - val_accuracy: 0.7198\n",
            "Epoch 449/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6868 - accuracy: 0.7364 - val_loss: 0.7801 - val_accuracy: 0.6908\n",
            "Epoch 450/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6878 - accuracy: 0.7322 - val_loss: 0.7633 - val_accuracy: 0.7295\n",
            "Epoch 451/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7016 - accuracy: 0.7207 - val_loss: 0.7613 - val_accuracy: 0.7053\n",
            "Epoch 452/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6941 - accuracy: 0.7279 - val_loss: 0.7735 - val_accuracy: 0.6957\n",
            "Epoch 453/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6910 - accuracy: 0.7279 - val_loss: 0.7662 - val_accuracy: 0.7198\n",
            "Epoch 454/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7026 - accuracy: 0.7261 - val_loss: 0.7874 - val_accuracy: 0.6908\n",
            "Epoch 455/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6838 - accuracy: 0.7310 - val_loss: 0.7674 - val_accuracy: 0.7053\n",
            "Epoch 456/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6890 - accuracy: 0.7352 - val_loss: 0.7805 - val_accuracy: 0.6812\n",
            "Epoch 457/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6857 - accuracy: 0.7418 - val_loss: 0.7746 - val_accuracy: 0.7005\n",
            "Epoch 458/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6828 - accuracy: 0.7273 - val_loss: 0.7708 - val_accuracy: 0.7053\n",
            "Epoch 459/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6869 - accuracy: 0.7225 - val_loss: 0.7882 - val_accuracy: 0.7005\n",
            "Epoch 460/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6814 - accuracy: 0.7364 - val_loss: 0.7555 - val_accuracy: 0.7246\n",
            "Epoch 461/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6932 - accuracy: 0.7225 - val_loss: 0.7568 - val_accuracy: 0.7053\n",
            "Epoch 462/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6799 - accuracy: 0.7412 - val_loss: 0.7570 - val_accuracy: 0.7246\n",
            "Epoch 463/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6921 - accuracy: 0.7231 - val_loss: 0.7652 - val_accuracy: 0.7198\n",
            "Epoch 464/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6679 - accuracy: 0.7394 - val_loss: 0.7860 - val_accuracy: 0.7101\n",
            "Epoch 465/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6923 - accuracy: 0.7237 - val_loss: 0.7772 - val_accuracy: 0.6908\n",
            "Epoch 466/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6540 - accuracy: 0.7443 - val_loss: 0.7679 - val_accuracy: 0.6957\n",
            "Epoch 467/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6806 - accuracy: 0.7340 - val_loss: 0.7672 - val_accuracy: 0.7101\n",
            "Epoch 468/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6722 - accuracy: 0.7388 - val_loss: 0.7641 - val_accuracy: 0.6763\n",
            "Epoch 469/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6793 - accuracy: 0.7310 - val_loss: 0.7633 - val_accuracy: 0.6957\n",
            "Epoch 470/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6645 - accuracy: 0.7394 - val_loss: 0.7665 - val_accuracy: 0.6715\n",
            "Epoch 471/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6818 - accuracy: 0.7267 - val_loss: 0.7626 - val_accuracy: 0.7053\n",
            "Epoch 472/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7046 - accuracy: 0.7225 - val_loss: 0.7566 - val_accuracy: 0.7198\n",
            "Epoch 473/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6616 - accuracy: 0.7370 - val_loss: 0.7589 - val_accuracy: 0.6908\n",
            "Epoch 474/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6874 - accuracy: 0.7310 - val_loss: 0.7582 - val_accuracy: 0.7391\n",
            "Epoch 475/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6812 - accuracy: 0.7261 - val_loss: 0.7710 - val_accuracy: 0.7053\n",
            "Epoch 476/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6610 - accuracy: 0.7334 - val_loss: 0.7503 - val_accuracy: 0.7343\n",
            "Epoch 477/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6427 - accuracy: 0.7588 - val_loss: 0.7824 - val_accuracy: 0.6957\n",
            "Epoch 478/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6843 - accuracy: 0.7225 - val_loss: 0.7870 - val_accuracy: 0.6763\n",
            "Epoch 479/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6818 - accuracy: 0.7237 - val_loss: 0.7745 - val_accuracy: 0.7053\n",
            "Epoch 480/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6867 - accuracy: 0.7273 - val_loss: 0.7676 - val_accuracy: 0.7053\n",
            "Epoch 481/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6879 - accuracy: 0.7394 - val_loss: 0.7747 - val_accuracy: 0.7246\n",
            "Epoch 482/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6720 - accuracy: 0.7285 - val_loss: 0.7712 - val_accuracy: 0.7150\n",
            "Epoch 483/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6643 - accuracy: 0.7382 - val_loss: 0.7656 - val_accuracy: 0.7005\n",
            "Epoch 484/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6410 - accuracy: 0.7533 - val_loss: 0.7479 - val_accuracy: 0.7246\n",
            "Epoch 485/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6727 - accuracy: 0.7382 - val_loss: 0.7677 - val_accuracy: 0.7150\n",
            "Epoch 486/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6615 - accuracy: 0.7394 - val_loss: 0.7471 - val_accuracy: 0.7150\n",
            "Epoch 487/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6622 - accuracy: 0.7388 - val_loss: 0.7630 - val_accuracy: 0.7150\n",
            "Epoch 488/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6841 - accuracy: 0.7322 - val_loss: 0.7687 - val_accuracy: 0.7198\n",
            "Epoch 489/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6425 - accuracy: 0.7437 - val_loss: 0.7585 - val_accuracy: 0.7150\n",
            "Epoch 490/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6504 - accuracy: 0.7473 - val_loss: 0.7571 - val_accuracy: 0.7101\n",
            "Epoch 491/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6604 - accuracy: 0.7370 - val_loss: 0.7513 - val_accuracy: 0.7101\n",
            "Epoch 492/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6715 - accuracy: 0.7340 - val_loss: 0.7707 - val_accuracy: 0.6957\n",
            "Epoch 493/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6582 - accuracy: 0.7316 - val_loss: 0.7502 - val_accuracy: 0.7246\n",
            "Epoch 494/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6570 - accuracy: 0.7418 - val_loss: 0.7505 - val_accuracy: 0.7053\n",
            "Epoch 495/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6636 - accuracy: 0.7430 - val_loss: 0.7490 - val_accuracy: 0.7246\n",
            "Epoch 496/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6407 - accuracy: 0.7443 - val_loss: 0.7543 - val_accuracy: 0.7246\n",
            "Epoch 497/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6505 - accuracy: 0.7545 - val_loss: 0.7652 - val_accuracy: 0.6667\n",
            "Epoch 498/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6500 - accuracy: 0.7424 - val_loss: 0.7515 - val_accuracy: 0.7246\n",
            "Epoch 499/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6437 - accuracy: 0.7521 - val_loss: 0.7618 - val_accuracy: 0.7005\n",
            "Epoch 500/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6385 - accuracy: 0.7430 - val_loss: 0.7802 - val_accuracy: 0.6763\n",
            "Epoch 501/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6290 - accuracy: 0.7430 - val_loss: 0.7792 - val_accuracy: 0.6860\n",
            "Epoch 502/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6470 - accuracy: 0.7382 - val_loss: 0.7535 - val_accuracy: 0.7053\n",
            "Epoch 503/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6452 - accuracy: 0.7437 - val_loss: 0.7417 - val_accuracy: 0.7101\n",
            "Epoch 504/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6307 - accuracy: 0.7527 - val_loss: 0.7464 - val_accuracy: 0.7053\n",
            "Epoch 505/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6365 - accuracy: 0.7509 - val_loss: 0.7368 - val_accuracy: 0.7150\n",
            "Epoch 506/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6327 - accuracy: 0.7503 - val_loss: 0.7408 - val_accuracy: 0.7101\n",
            "Epoch 507/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6416 - accuracy: 0.7467 - val_loss: 0.7517 - val_accuracy: 0.7295\n",
            "Epoch 508/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6268 - accuracy: 0.7654 - val_loss: 0.7591 - val_accuracy: 0.6812\n",
            "Epoch 509/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 0.6305 - accuracy: 0.7527 - val_loss: 0.7345 - val_accuracy: 0.7488\n",
            "Epoch 510/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 0.6707 - accuracy: 0.7316 - val_loss: 0.7593 - val_accuracy: 0.6908\n",
            "Epoch 511/1000\n",
            "104/104 [==============================] - 1s 8ms/step - loss: 0.6400 - accuracy: 0.7515 - val_loss: 0.7520 - val_accuracy: 0.7295\n",
            "Epoch 512/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 0.6186 - accuracy: 0.7503 - val_loss: 0.7513 - val_accuracy: 0.7053\n",
            "Epoch 513/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 0.6277 - accuracy: 0.7527 - val_loss: 0.7517 - val_accuracy: 0.7005\n",
            "Epoch 514/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6285 - accuracy: 0.7612 - val_loss: 0.7420 - val_accuracy: 0.7053\n",
            "Epoch 515/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6367 - accuracy: 0.7479 - val_loss: 0.7410 - val_accuracy: 0.6908\n",
            "Epoch 516/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6429 - accuracy: 0.7503 - val_loss: 0.7407 - val_accuracy: 0.7198\n",
            "Epoch 517/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6366 - accuracy: 0.7509 - val_loss: 0.7325 - val_accuracy: 0.7391\n",
            "Epoch 518/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6397 - accuracy: 0.7509 - val_loss: 0.7500 - val_accuracy: 0.7053\n",
            "Epoch 519/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6376 - accuracy: 0.7467 - val_loss: 0.7800 - val_accuracy: 0.7005\n",
            "Epoch 520/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6472 - accuracy: 0.7467 - val_loss: 0.7698 - val_accuracy: 0.6715\n",
            "Epoch 521/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6263 - accuracy: 0.7449 - val_loss: 0.7479 - val_accuracy: 0.7246\n",
            "Epoch 522/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6517 - accuracy: 0.7473 - val_loss: 0.7454 - val_accuracy: 0.7246\n",
            "Epoch 523/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6302 - accuracy: 0.7557 - val_loss: 0.7474 - val_accuracy: 0.7150\n",
            "Epoch 524/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6151 - accuracy: 0.7594 - val_loss: 0.7482 - val_accuracy: 0.7005\n",
            "Epoch 525/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6194 - accuracy: 0.7618 - val_loss: 0.7311 - val_accuracy: 0.7440\n",
            "Epoch 526/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6285 - accuracy: 0.7636 - val_loss: 0.7433 - val_accuracy: 0.7053\n",
            "Epoch 527/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6176 - accuracy: 0.7551 - val_loss: 0.7633 - val_accuracy: 0.6908\n",
            "Epoch 528/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6097 - accuracy: 0.7588 - val_loss: 0.7368 - val_accuracy: 0.7343\n",
            "Epoch 529/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6427 - accuracy: 0.7443 - val_loss: 0.7355 - val_accuracy: 0.7391\n",
            "Epoch 530/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6322 - accuracy: 0.7467 - val_loss: 0.7440 - val_accuracy: 0.7005\n",
            "Epoch 531/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6226 - accuracy: 0.7533 - val_loss: 0.7424 - val_accuracy: 0.7053\n",
            "Epoch 532/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5923 - accuracy: 0.7660 - val_loss: 0.7370 - val_accuracy: 0.7246\n",
            "Epoch 533/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5990 - accuracy: 0.7582 - val_loss: 0.7217 - val_accuracy: 0.7343\n",
            "Epoch 534/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6072 - accuracy: 0.7642 - val_loss: 0.7614 - val_accuracy: 0.6957\n",
            "Epoch 535/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6051 - accuracy: 0.7666 - val_loss: 0.7171 - val_accuracy: 0.7488\n",
            "Epoch 536/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6238 - accuracy: 0.7563 - val_loss: 0.7149 - val_accuracy: 0.7391\n",
            "Epoch 537/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6148 - accuracy: 0.7570 - val_loss: 0.7283 - val_accuracy: 0.7246\n",
            "Epoch 538/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6131 - accuracy: 0.7527 - val_loss: 0.7364 - val_accuracy: 0.7198\n",
            "Epoch 539/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5990 - accuracy: 0.7612 - val_loss: 0.7312 - val_accuracy: 0.7150\n",
            "Epoch 540/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6191 - accuracy: 0.7539 - val_loss: 0.7365 - val_accuracy: 0.7198\n",
            "Epoch 541/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6182 - accuracy: 0.7600 - val_loss: 0.7360 - val_accuracy: 0.7198\n",
            "Epoch 542/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6016 - accuracy: 0.7648 - val_loss: 0.7516 - val_accuracy: 0.7005\n",
            "Epoch 543/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6140 - accuracy: 0.7666 - val_loss: 0.7315 - val_accuracy: 0.7343\n",
            "Epoch 544/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6253 - accuracy: 0.7533 - val_loss: 0.7223 - val_accuracy: 0.7536\n",
            "Epoch 545/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6099 - accuracy: 0.7654 - val_loss: 0.7312 - val_accuracy: 0.6957\n",
            "Epoch 546/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5914 - accuracy: 0.7739 - val_loss: 0.7260 - val_accuracy: 0.7101\n",
            "Epoch 547/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5860 - accuracy: 0.7630 - val_loss: 0.7514 - val_accuracy: 0.7053\n",
            "Epoch 548/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5979 - accuracy: 0.7672 - val_loss: 0.7402 - val_accuracy: 0.7150\n",
            "Epoch 549/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6134 - accuracy: 0.7684 - val_loss: 0.7401 - val_accuracy: 0.6957\n",
            "Epoch 550/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6149 - accuracy: 0.7527 - val_loss: 0.7209 - val_accuracy: 0.7246\n",
            "Epoch 551/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5784 - accuracy: 0.7757 - val_loss: 0.7567 - val_accuracy: 0.6618\n",
            "Epoch 552/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6037 - accuracy: 0.7642 - val_loss: 0.7399 - val_accuracy: 0.7101\n",
            "Epoch 553/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5814 - accuracy: 0.7751 - val_loss: 0.7265 - val_accuracy: 0.7246\n",
            "Epoch 554/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6126 - accuracy: 0.7588 - val_loss: 0.7393 - val_accuracy: 0.6957\n",
            "Epoch 555/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6028 - accuracy: 0.7600 - val_loss: 0.7187 - val_accuracy: 0.7536\n",
            "Epoch 556/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5968 - accuracy: 0.7684 - val_loss: 0.7276 - val_accuracy: 0.7198\n",
            "Epoch 557/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5828 - accuracy: 0.7715 - val_loss: 0.7243 - val_accuracy: 0.7198\n",
            "Epoch 558/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5964 - accuracy: 0.7636 - val_loss: 0.7216 - val_accuracy: 0.7246\n",
            "Epoch 559/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5837 - accuracy: 0.7830 - val_loss: 0.7179 - val_accuracy: 0.7198\n",
            "Epoch 560/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5693 - accuracy: 0.7757 - val_loss: 0.7104 - val_accuracy: 0.7343\n",
            "Epoch 561/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5882 - accuracy: 0.7793 - val_loss: 0.7219 - val_accuracy: 0.7488\n",
            "Epoch 562/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6070 - accuracy: 0.7576 - val_loss: 0.7193 - val_accuracy: 0.7391\n",
            "Epoch 563/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5766 - accuracy: 0.7769 - val_loss: 0.7220 - val_accuracy: 0.7488\n",
            "Epoch 564/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6005 - accuracy: 0.7672 - val_loss: 0.7126 - val_accuracy: 0.7295\n",
            "Epoch 565/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5760 - accuracy: 0.7823 - val_loss: 0.7141 - val_accuracy: 0.7391\n",
            "Epoch 566/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5935 - accuracy: 0.7727 - val_loss: 0.7250 - val_accuracy: 0.7150\n",
            "Epoch 567/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5969 - accuracy: 0.7660 - val_loss: 0.7165 - val_accuracy: 0.7295\n",
            "Epoch 568/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6022 - accuracy: 0.7588 - val_loss: 0.7152 - val_accuracy: 0.7246\n",
            "Epoch 569/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5881 - accuracy: 0.7588 - val_loss: 0.7404 - val_accuracy: 0.7053\n",
            "Epoch 570/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5759 - accuracy: 0.7733 - val_loss: 0.7169 - val_accuracy: 0.7343\n",
            "Epoch 571/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5627 - accuracy: 0.7836 - val_loss: 0.7228 - val_accuracy: 0.7246\n",
            "Epoch 572/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5898 - accuracy: 0.7690 - val_loss: 0.7223 - val_accuracy: 0.7101\n",
            "Epoch 573/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6023 - accuracy: 0.7660 - val_loss: 0.7299 - val_accuracy: 0.7150\n",
            "Epoch 574/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5678 - accuracy: 0.7715 - val_loss: 0.7238 - val_accuracy: 0.7150\n",
            "Epoch 575/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5849 - accuracy: 0.7739 - val_loss: 0.7245 - val_accuracy: 0.7295\n",
            "Epoch 576/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5898 - accuracy: 0.7666 - val_loss: 0.7325 - val_accuracy: 0.7150\n",
            "Epoch 577/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5731 - accuracy: 0.7775 - val_loss: 0.7122 - val_accuracy: 0.7295\n",
            "Epoch 578/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5736 - accuracy: 0.7715 - val_loss: 0.7132 - val_accuracy: 0.7295\n",
            "Epoch 579/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5650 - accuracy: 0.7787 - val_loss: 0.7143 - val_accuracy: 0.7343\n",
            "Epoch 580/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5710 - accuracy: 0.7787 - val_loss: 0.7266 - val_accuracy: 0.7150\n",
            "Epoch 581/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6000 - accuracy: 0.7654 - val_loss: 0.7315 - val_accuracy: 0.7150\n",
            "Epoch 582/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5673 - accuracy: 0.7757 - val_loss: 0.7247 - val_accuracy: 0.7198\n",
            "Epoch 583/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5653 - accuracy: 0.7775 - val_loss: 0.7367 - val_accuracy: 0.7053\n",
            "Epoch 584/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5756 - accuracy: 0.7799 - val_loss: 0.7428 - val_accuracy: 0.7101\n",
            "Epoch 585/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5863 - accuracy: 0.7666 - val_loss: 0.7274 - val_accuracy: 0.7005\n",
            "Epoch 586/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5764 - accuracy: 0.7715 - val_loss: 0.7186 - val_accuracy: 0.7295\n",
            "Epoch 587/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5743 - accuracy: 0.7733 - val_loss: 0.7002 - val_accuracy: 0.7488\n",
            "Epoch 588/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5731 - accuracy: 0.7733 - val_loss: 0.7134 - val_accuracy: 0.7101\n",
            "Epoch 589/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5597 - accuracy: 0.7896 - val_loss: 0.7168 - val_accuracy: 0.7198\n",
            "Epoch 590/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5661 - accuracy: 0.7787 - val_loss: 0.7133 - val_accuracy: 0.7198\n",
            "Epoch 591/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5591 - accuracy: 0.7884 - val_loss: 0.7191 - val_accuracy: 0.7391\n",
            "Epoch 592/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5602 - accuracy: 0.7793 - val_loss: 0.7215 - val_accuracy: 0.7488\n",
            "Epoch 593/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5694 - accuracy: 0.7787 - val_loss: 0.7374 - val_accuracy: 0.6957\n",
            "Epoch 594/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5532 - accuracy: 0.7775 - val_loss: 0.7144 - val_accuracy: 0.7246\n",
            "Epoch 595/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5342 - accuracy: 0.7963 - val_loss: 0.7255 - val_accuracy: 0.7053\n",
            "Epoch 596/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5697 - accuracy: 0.7787 - val_loss: 0.7254 - val_accuracy: 0.6957\n",
            "Epoch 597/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5579 - accuracy: 0.7842 - val_loss: 0.7204 - val_accuracy: 0.7150\n",
            "Epoch 598/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5604 - accuracy: 0.7836 - val_loss: 0.7370 - val_accuracy: 0.7198\n",
            "Epoch 599/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5635 - accuracy: 0.7811 - val_loss: 0.7146 - val_accuracy: 0.7391\n",
            "Epoch 600/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5535 - accuracy: 0.7811 - val_loss: 0.7235 - val_accuracy: 0.7246\n",
            "Epoch 601/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5646 - accuracy: 0.7721 - val_loss: 0.7287 - val_accuracy: 0.7005\n",
            "Epoch 602/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5683 - accuracy: 0.7781 - val_loss: 0.7247 - val_accuracy: 0.7150\n",
            "Epoch 603/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5537 - accuracy: 0.7890 - val_loss: 0.7097 - val_accuracy: 0.7053\n",
            "Epoch 604/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5533 - accuracy: 0.7830 - val_loss: 0.7131 - val_accuracy: 0.7246\n",
            "Epoch 605/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5478 - accuracy: 0.7914 - val_loss: 0.7105 - val_accuracy: 0.7150\n",
            "Epoch 606/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5373 - accuracy: 0.7848 - val_loss: 0.7028 - val_accuracy: 0.7198\n",
            "Epoch 607/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5370 - accuracy: 0.7908 - val_loss: 0.7056 - val_accuracy: 0.7150\n",
            "Epoch 608/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5407 - accuracy: 0.7866 - val_loss: 0.7235 - val_accuracy: 0.7053\n",
            "Epoch 609/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5387 - accuracy: 0.7890 - val_loss: 0.7191 - val_accuracy: 0.7343\n",
            "Epoch 610/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5336 - accuracy: 0.7981 - val_loss: 0.7197 - val_accuracy: 0.7101\n",
            "Epoch 611/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5511 - accuracy: 0.7950 - val_loss: 0.6989 - val_accuracy: 0.7295\n",
            "Epoch 612/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5376 - accuracy: 0.7944 - val_loss: 0.7123 - val_accuracy: 0.7053\n",
            "Epoch 613/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5557 - accuracy: 0.7866 - val_loss: 0.7041 - val_accuracy: 0.7295\n",
            "Epoch 614/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5772 - accuracy: 0.7703 - val_loss: 0.7145 - val_accuracy: 0.7295\n",
            "Epoch 615/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5450 - accuracy: 0.7811 - val_loss: 0.6932 - val_accuracy: 0.7295\n",
            "Epoch 616/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5573 - accuracy: 0.7866 - val_loss: 0.6868 - val_accuracy: 0.7488\n",
            "Epoch 617/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5315 - accuracy: 0.7926 - val_loss: 0.6967 - val_accuracy: 0.7391\n",
            "Epoch 618/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5371 - accuracy: 0.7878 - val_loss: 0.6927 - val_accuracy: 0.7536\n",
            "Epoch 619/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5448 - accuracy: 0.7866 - val_loss: 0.7109 - val_accuracy: 0.7101\n",
            "Epoch 620/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5534 - accuracy: 0.7787 - val_loss: 0.7007 - val_accuracy: 0.7440\n",
            "Epoch 621/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5577 - accuracy: 0.7751 - val_loss: 0.6899 - val_accuracy: 0.7440\n",
            "Epoch 622/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5423 - accuracy: 0.7926 - val_loss: 0.7096 - val_accuracy: 0.7295\n",
            "Epoch 623/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5329 - accuracy: 0.7914 - val_loss: 0.7211 - val_accuracy: 0.7150\n",
            "Epoch 624/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5449 - accuracy: 0.7817 - val_loss: 0.7140 - val_accuracy: 0.7295\n",
            "Epoch 625/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5467 - accuracy: 0.7823 - val_loss: 0.7174 - val_accuracy: 0.7295\n",
            "Epoch 626/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5289 - accuracy: 0.7975 - val_loss: 0.7222 - val_accuracy: 0.7295\n",
            "Epoch 627/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5206 - accuracy: 0.7975 - val_loss: 0.6916 - val_accuracy: 0.7246\n",
            "Epoch 628/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5365 - accuracy: 0.7956 - val_loss: 0.6896 - val_accuracy: 0.7246\n",
            "Epoch 629/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5596 - accuracy: 0.7878 - val_loss: 0.6885 - val_accuracy: 0.7536\n",
            "Epoch 630/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5261 - accuracy: 0.7987 - val_loss: 0.7204 - val_accuracy: 0.7150\n",
            "Epoch 631/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5369 - accuracy: 0.7926 - val_loss: 0.7015 - val_accuracy: 0.7343\n",
            "Epoch 632/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5466 - accuracy: 0.7769 - val_loss: 0.6964 - val_accuracy: 0.7343\n",
            "Epoch 633/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5292 - accuracy: 0.7920 - val_loss: 0.7091 - val_accuracy: 0.7198\n",
            "Epoch 634/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5311 - accuracy: 0.7902 - val_loss: 0.7041 - val_accuracy: 0.7246\n",
            "Epoch 635/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5205 - accuracy: 0.7963 - val_loss: 0.7146 - val_accuracy: 0.7343\n",
            "Epoch 636/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5475 - accuracy: 0.7757 - val_loss: 0.7177 - val_accuracy: 0.7053\n",
            "Epoch 637/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5452 - accuracy: 0.7811 - val_loss: 0.7371 - val_accuracy: 0.7053\n",
            "Epoch 638/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5289 - accuracy: 0.7932 - val_loss: 0.6956 - val_accuracy: 0.7343\n",
            "Epoch 639/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5226 - accuracy: 0.8041 - val_loss: 0.7169 - val_accuracy: 0.7198\n",
            "Epoch 640/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5385 - accuracy: 0.7884 - val_loss: 0.6976 - val_accuracy: 0.7246\n",
            "Epoch 641/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5184 - accuracy: 0.8065 - val_loss: 0.7195 - val_accuracy: 0.7101\n",
            "Epoch 642/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5242 - accuracy: 0.8035 - val_loss: 0.7073 - val_accuracy: 0.7005\n",
            "Epoch 643/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5156 - accuracy: 0.8132 - val_loss: 0.7049 - val_accuracy: 0.7198\n",
            "Epoch 644/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5219 - accuracy: 0.7950 - val_loss: 0.6901 - val_accuracy: 0.7101\n",
            "Epoch 645/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5224 - accuracy: 0.7987 - val_loss: 0.6720 - val_accuracy: 0.7536\n",
            "Epoch 646/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5356 - accuracy: 0.7902 - val_loss: 0.6865 - val_accuracy: 0.7536\n",
            "Epoch 647/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5297 - accuracy: 0.8005 - val_loss: 0.6942 - val_accuracy: 0.7343\n",
            "Epoch 648/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5152 - accuracy: 0.8041 - val_loss: 0.6881 - val_accuracy: 0.7536\n",
            "Epoch 649/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5384 - accuracy: 0.7890 - val_loss: 0.6921 - val_accuracy: 0.7391\n",
            "Epoch 650/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5374 - accuracy: 0.7963 - val_loss: 0.6803 - val_accuracy: 0.7488\n",
            "Epoch 651/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5001 - accuracy: 0.7975 - val_loss: 0.6987 - val_accuracy: 0.7440\n",
            "Epoch 652/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5199 - accuracy: 0.7956 - val_loss: 0.6989 - val_accuracy: 0.7295\n",
            "Epoch 653/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5157 - accuracy: 0.8053 - val_loss: 0.7098 - val_accuracy: 0.7391\n",
            "Epoch 654/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5343 - accuracy: 0.7932 - val_loss: 0.7150 - val_accuracy: 0.7198\n",
            "Epoch 655/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5266 - accuracy: 0.7999 - val_loss: 0.7229 - val_accuracy: 0.7246\n",
            "Epoch 656/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5007 - accuracy: 0.8108 - val_loss: 0.7004 - val_accuracy: 0.7198\n",
            "Epoch 657/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5205 - accuracy: 0.7914 - val_loss: 0.6895 - val_accuracy: 0.7585\n",
            "Epoch 658/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5189 - accuracy: 0.8065 - val_loss: 0.6958 - val_accuracy: 0.7391\n",
            "Epoch 659/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5489 - accuracy: 0.7920 - val_loss: 0.7157 - val_accuracy: 0.7246\n",
            "Epoch 660/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5166 - accuracy: 0.7902 - val_loss: 0.7165 - val_accuracy: 0.7150\n",
            "Epoch 661/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5103 - accuracy: 0.8047 - val_loss: 0.6767 - val_accuracy: 0.7585\n",
            "Epoch 662/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5058 - accuracy: 0.8035 - val_loss: 0.7094 - val_accuracy: 0.7295\n",
            "Epoch 663/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5092 - accuracy: 0.8047 - val_loss: 0.6889 - val_accuracy: 0.7536\n",
            "Epoch 664/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5002 - accuracy: 0.8047 - val_loss: 0.7004 - val_accuracy: 0.7246\n",
            "Epoch 665/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5071 - accuracy: 0.7963 - val_loss: 0.7047 - val_accuracy: 0.7295\n",
            "Epoch 666/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5026 - accuracy: 0.8120 - val_loss: 0.7015 - val_accuracy: 0.7150\n",
            "Epoch 667/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5380 - accuracy: 0.7878 - val_loss: 0.6817 - val_accuracy: 0.7488\n",
            "Epoch 668/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4949 - accuracy: 0.8041 - val_loss: 0.6954 - val_accuracy: 0.7053\n",
            "Epoch 669/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5230 - accuracy: 0.7981 - val_loss: 0.6998 - val_accuracy: 0.7053\n",
            "Epoch 670/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4896 - accuracy: 0.8192 - val_loss: 0.6774 - val_accuracy: 0.7440\n",
            "Epoch 671/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5081 - accuracy: 0.7944 - val_loss: 0.6796 - val_accuracy: 0.7440\n",
            "Epoch 672/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4902 - accuracy: 0.8114 - val_loss: 0.6749 - val_accuracy: 0.7585\n",
            "Epoch 673/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5239 - accuracy: 0.7938 - val_loss: 0.6922 - val_accuracy: 0.7246\n",
            "Epoch 674/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5079 - accuracy: 0.8065 - val_loss: 0.6925 - val_accuracy: 0.7101\n",
            "Epoch 675/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5035 - accuracy: 0.8156 - val_loss: 0.6838 - val_accuracy: 0.7343\n",
            "Epoch 676/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4990 - accuracy: 0.7950 - val_loss: 0.6905 - val_accuracy: 0.7150\n",
            "Epoch 677/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4907 - accuracy: 0.8120 - val_loss: 0.6848 - val_accuracy: 0.7005\n",
            "Epoch 678/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4886 - accuracy: 0.8077 - val_loss: 0.6636 - val_accuracy: 0.7440\n",
            "Epoch 679/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5171 - accuracy: 0.8017 - val_loss: 0.6909 - val_accuracy: 0.7150\n",
            "Epoch 680/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5009 - accuracy: 0.8168 - val_loss: 0.6864 - val_accuracy: 0.7391\n",
            "Epoch 681/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4943 - accuracy: 0.8065 - val_loss: 0.6850 - val_accuracy: 0.7198\n",
            "Epoch 682/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4712 - accuracy: 0.8210 - val_loss: 0.7092 - val_accuracy: 0.7246\n",
            "Epoch 683/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5045 - accuracy: 0.8053 - val_loss: 0.7083 - val_accuracy: 0.7150\n",
            "Epoch 684/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5073 - accuracy: 0.7987 - val_loss: 0.6737 - val_accuracy: 0.7391\n",
            "Epoch 685/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4903 - accuracy: 0.8138 - val_loss: 0.6818 - val_accuracy: 0.7150\n",
            "Epoch 686/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4860 - accuracy: 0.8229 - val_loss: 0.6997 - val_accuracy: 0.7101\n",
            "Epoch 687/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4898 - accuracy: 0.7999 - val_loss: 0.6664 - val_accuracy: 0.7585\n",
            "Epoch 688/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5043 - accuracy: 0.8029 - val_loss: 0.6836 - val_accuracy: 0.7198\n",
            "Epoch 689/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4886 - accuracy: 0.8102 - val_loss: 0.6883 - val_accuracy: 0.7246\n",
            "Epoch 690/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4865 - accuracy: 0.8096 - val_loss: 0.6884 - val_accuracy: 0.7246\n",
            "Epoch 691/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4927 - accuracy: 0.8102 - val_loss: 0.6975 - val_accuracy: 0.7295\n",
            "Epoch 692/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4921 - accuracy: 0.8108 - val_loss: 0.6924 - val_accuracy: 0.7246\n",
            "Epoch 693/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5107 - accuracy: 0.7981 - val_loss: 0.6770 - val_accuracy: 0.7198\n",
            "Epoch 694/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5003 - accuracy: 0.8059 - val_loss: 0.6863 - val_accuracy: 0.7391\n",
            "Epoch 695/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4829 - accuracy: 0.8162 - val_loss: 0.6680 - val_accuracy: 0.7343\n",
            "Epoch 696/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4751 - accuracy: 0.8204 - val_loss: 0.6726 - val_accuracy: 0.7198\n",
            "Epoch 697/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4777 - accuracy: 0.8126 - val_loss: 0.7025 - val_accuracy: 0.6957\n",
            "Epoch 698/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4597 - accuracy: 0.8222 - val_loss: 0.6901 - val_accuracy: 0.7391\n",
            "Epoch 699/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4923 - accuracy: 0.8108 - val_loss: 0.6868 - val_accuracy: 0.7101\n",
            "Epoch 700/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4989 - accuracy: 0.8138 - val_loss: 0.6793 - val_accuracy: 0.7440\n",
            "Epoch 701/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4911 - accuracy: 0.8005 - val_loss: 0.6719 - val_accuracy: 0.7440\n",
            "Epoch 702/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5019 - accuracy: 0.7993 - val_loss: 0.6992 - val_accuracy: 0.7005\n",
            "Epoch 703/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4735 - accuracy: 0.8313 - val_loss: 0.6732 - val_accuracy: 0.7343\n",
            "Epoch 704/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4725 - accuracy: 0.8132 - val_loss: 0.6920 - val_accuracy: 0.6860\n",
            "Epoch 705/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4979 - accuracy: 0.8053 - val_loss: 0.6598 - val_accuracy: 0.7488\n",
            "Epoch 706/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4752 - accuracy: 0.8174 - val_loss: 0.7016 - val_accuracy: 0.7440\n",
            "Epoch 707/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4690 - accuracy: 0.8180 - val_loss: 0.7001 - val_accuracy: 0.7295\n",
            "Epoch 708/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4721 - accuracy: 0.8235 - val_loss: 0.7024 - val_accuracy: 0.7343\n",
            "Epoch 709/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4926 - accuracy: 0.8089 - val_loss: 0.7064 - val_accuracy: 0.7295\n",
            "Epoch 710/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4668 - accuracy: 0.8289 - val_loss: 0.6822 - val_accuracy: 0.7343\n",
            "Epoch 711/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4647 - accuracy: 0.8198 - val_loss: 0.6937 - val_accuracy: 0.7343\n",
            "Epoch 712/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4691 - accuracy: 0.8126 - val_loss: 0.6973 - val_accuracy: 0.7198\n",
            "Epoch 713/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4809 - accuracy: 0.8108 - val_loss: 0.6652 - val_accuracy: 0.7488\n",
            "Epoch 714/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4524 - accuracy: 0.8277 - val_loss: 0.6761 - val_accuracy: 0.7488\n",
            "Epoch 715/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4643 - accuracy: 0.8180 - val_loss: 0.7038 - val_accuracy: 0.7343\n",
            "Epoch 716/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4557 - accuracy: 0.8253 - val_loss: 0.6807 - val_accuracy: 0.7246\n",
            "Epoch 717/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4702 - accuracy: 0.8241 - val_loss: 0.6688 - val_accuracy: 0.7440\n",
            "Epoch 718/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4600 - accuracy: 0.8138 - val_loss: 0.6740 - val_accuracy: 0.7391\n",
            "Epoch 719/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4694 - accuracy: 0.8192 - val_loss: 0.6769 - val_accuracy: 0.7295\n",
            "Epoch 720/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4687 - accuracy: 0.8180 - val_loss: 0.6657 - val_accuracy: 0.7391\n",
            "Epoch 721/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4756 - accuracy: 0.8174 - val_loss: 0.6845 - val_accuracy: 0.7246\n",
            "Epoch 722/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4771 - accuracy: 0.8150 - val_loss: 0.6782 - val_accuracy: 0.7246\n",
            "Epoch 723/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4585 - accuracy: 0.8271 - val_loss: 0.6842 - val_accuracy: 0.7246\n",
            "Epoch 724/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4737 - accuracy: 0.8114 - val_loss: 0.6750 - val_accuracy: 0.7246\n",
            "Epoch 725/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4655 - accuracy: 0.8174 - val_loss: 0.6924 - val_accuracy: 0.7198\n",
            "Epoch 726/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4617 - accuracy: 0.8186 - val_loss: 0.6720 - val_accuracy: 0.7101\n",
            "Epoch 727/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4665 - accuracy: 0.8138 - val_loss: 0.6924 - val_accuracy: 0.7150\n",
            "Epoch 728/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4650 - accuracy: 0.8144 - val_loss: 0.6830 - val_accuracy: 0.7005\n",
            "Epoch 729/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4616 - accuracy: 0.8253 - val_loss: 0.6868 - val_accuracy: 0.7343\n",
            "Epoch 730/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4732 - accuracy: 0.8114 - val_loss: 0.6708 - val_accuracy: 0.7198\n",
            "Epoch 731/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4701 - accuracy: 0.8168 - val_loss: 0.6885 - val_accuracy: 0.7150\n",
            "Epoch 732/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4547 - accuracy: 0.8271 - val_loss: 0.6710 - val_accuracy: 0.7343\n",
            "Epoch 733/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4721 - accuracy: 0.8089 - val_loss: 0.6754 - val_accuracy: 0.7150\n",
            "Epoch 734/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4428 - accuracy: 0.8301 - val_loss: 0.6804 - val_accuracy: 0.7150\n",
            "Epoch 735/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4580 - accuracy: 0.8265 - val_loss: 0.6636 - val_accuracy: 0.7440\n",
            "Epoch 736/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4517 - accuracy: 0.8235 - val_loss: 0.6817 - val_accuracy: 0.7198\n",
            "Epoch 737/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4578 - accuracy: 0.8271 - val_loss: 0.6680 - val_accuracy: 0.7198\n",
            "Epoch 738/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4707 - accuracy: 0.8089 - val_loss: 0.6636 - val_accuracy: 0.7295\n",
            "Epoch 739/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4731 - accuracy: 0.8102 - val_loss: 0.6704 - val_accuracy: 0.7198\n",
            "Epoch 740/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4701 - accuracy: 0.8198 - val_loss: 0.6674 - val_accuracy: 0.7246\n",
            "Epoch 741/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4619 - accuracy: 0.8180 - val_loss: 0.6738 - val_accuracy: 0.7295\n",
            "Epoch 742/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4288 - accuracy: 0.8337 - val_loss: 0.6584 - val_accuracy: 0.7391\n",
            "Epoch 743/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4595 - accuracy: 0.8222 - val_loss: 0.6680 - val_accuracy: 0.7391\n",
            "Epoch 744/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4584 - accuracy: 0.8247 - val_loss: 0.6647 - val_accuracy: 0.7198\n",
            "Epoch 745/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4430 - accuracy: 0.8337 - val_loss: 0.6606 - val_accuracy: 0.7536\n",
            "Epoch 746/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4371 - accuracy: 0.8253 - val_loss: 0.6869 - val_accuracy: 0.7391\n",
            "Epoch 747/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4497 - accuracy: 0.8241 - val_loss: 0.6707 - val_accuracy: 0.7343\n",
            "Epoch 748/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4501 - accuracy: 0.8271 - val_loss: 0.6857 - val_accuracy: 0.7198\n",
            "Epoch 749/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4561 - accuracy: 0.8307 - val_loss: 0.6705 - val_accuracy: 0.7246\n",
            "Epoch 750/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4576 - accuracy: 0.8265 - val_loss: 0.6685 - val_accuracy: 0.7343\n",
            "Epoch 751/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4489 - accuracy: 0.8253 - val_loss: 0.6744 - val_accuracy: 0.7198\n",
            "Epoch 752/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4512 - accuracy: 0.8295 - val_loss: 0.6668 - val_accuracy: 0.7295\n",
            "Epoch 753/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4722 - accuracy: 0.8241 - val_loss: 0.6801 - val_accuracy: 0.7053\n",
            "Epoch 754/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4406 - accuracy: 0.8458 - val_loss: 0.6641 - val_accuracy: 0.7343\n",
            "Epoch 755/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4436 - accuracy: 0.8319 - val_loss: 0.6559 - val_accuracy: 0.7440\n",
            "Epoch 756/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4496 - accuracy: 0.8210 - val_loss: 0.6666 - val_accuracy: 0.7391\n",
            "Epoch 757/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4649 - accuracy: 0.8277 - val_loss: 0.6876 - val_accuracy: 0.7005\n",
            "Epoch 758/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4372 - accuracy: 0.8253 - val_loss: 0.6696 - val_accuracy: 0.7295\n",
            "Epoch 759/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4469 - accuracy: 0.8343 - val_loss: 0.6516 - val_accuracy: 0.7343\n",
            "Epoch 760/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4500 - accuracy: 0.8283 - val_loss: 0.6690 - val_accuracy: 0.7198\n",
            "Epoch 761/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4364 - accuracy: 0.8325 - val_loss: 0.6967 - val_accuracy: 0.7101\n",
            "Epoch 762/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4313 - accuracy: 0.8319 - val_loss: 0.6596 - val_accuracy: 0.7391\n",
            "Epoch 763/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4391 - accuracy: 0.8295 - val_loss: 0.6638 - val_accuracy: 0.7391\n",
            "Epoch 764/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4268 - accuracy: 0.8374 - val_loss: 0.6643 - val_accuracy: 0.7343\n",
            "Epoch 765/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4326 - accuracy: 0.8265 - val_loss: 0.6682 - val_accuracy: 0.7391\n",
            "Epoch 766/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4493 - accuracy: 0.8192 - val_loss: 0.6764 - val_accuracy: 0.7343\n",
            "Epoch 767/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4492 - accuracy: 0.8265 - val_loss: 0.6950 - val_accuracy: 0.6957\n",
            "Epoch 768/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4570 - accuracy: 0.8265 - val_loss: 0.6785 - val_accuracy: 0.7391\n",
            "Epoch 769/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4369 - accuracy: 0.8283 - val_loss: 0.6669 - val_accuracy: 0.7295\n",
            "Epoch 770/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4385 - accuracy: 0.8434 - val_loss: 0.6813 - val_accuracy: 0.7536\n",
            "Epoch 771/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4305 - accuracy: 0.8283 - val_loss: 0.6716 - val_accuracy: 0.7295\n",
            "Epoch 772/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4183 - accuracy: 0.8349 - val_loss: 0.6680 - val_accuracy: 0.7585\n",
            "Epoch 773/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4395 - accuracy: 0.8331 - val_loss: 0.6840 - val_accuracy: 0.6860\n",
            "Epoch 774/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4099 - accuracy: 0.8470 - val_loss: 0.6636 - val_accuracy: 0.7488\n",
            "Epoch 775/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4296 - accuracy: 0.8331 - val_loss: 0.6721 - val_accuracy: 0.7295\n",
            "Epoch 776/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4320 - accuracy: 0.8380 - val_loss: 0.6970 - val_accuracy: 0.7101\n",
            "Epoch 777/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4482 - accuracy: 0.8277 - val_loss: 0.6825 - val_accuracy: 0.7295\n",
            "Epoch 778/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4312 - accuracy: 0.8392 - val_loss: 0.6915 - val_accuracy: 0.7488\n",
            "Epoch 779/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4269 - accuracy: 0.8295 - val_loss: 0.6881 - val_accuracy: 0.7101\n",
            "Epoch 780/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4053 - accuracy: 0.8416 - val_loss: 0.6775 - val_accuracy: 0.7343\n",
            "Epoch 781/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4352 - accuracy: 0.8362 - val_loss: 0.6766 - val_accuracy: 0.7343\n",
            "Epoch 782/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4292 - accuracy: 0.8307 - val_loss: 0.6619 - val_accuracy: 0.7198\n",
            "Epoch 783/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4391 - accuracy: 0.8265 - val_loss: 0.6844 - val_accuracy: 0.7391\n",
            "Epoch 784/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4167 - accuracy: 0.8458 - val_loss: 0.6695 - val_accuracy: 0.7150\n",
            "Epoch 785/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4438 - accuracy: 0.8331 - val_loss: 0.6855 - val_accuracy: 0.7343\n",
            "Epoch 786/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4362 - accuracy: 0.8247 - val_loss: 0.6814 - val_accuracy: 0.7295\n",
            "Epoch 787/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4271 - accuracy: 0.8356 - val_loss: 0.6955 - val_accuracy: 0.7246\n",
            "Epoch 788/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4264 - accuracy: 0.8434 - val_loss: 0.6856 - val_accuracy: 0.7198\n",
            "Epoch 789/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4181 - accuracy: 0.8464 - val_loss: 0.6599 - val_accuracy: 0.7440\n",
            "Epoch 790/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4217 - accuracy: 0.8452 - val_loss: 0.6723 - val_accuracy: 0.7246\n",
            "Epoch 791/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4128 - accuracy: 0.8440 - val_loss: 0.6701 - val_accuracy: 0.7246\n",
            "Epoch 792/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4423 - accuracy: 0.8277 - val_loss: 0.6750 - val_accuracy: 0.7391\n",
            "Epoch 793/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4285 - accuracy: 0.8440 - val_loss: 0.6565 - val_accuracy: 0.7391\n",
            "Epoch 794/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4097 - accuracy: 0.8380 - val_loss: 0.6593 - val_accuracy: 0.7391\n",
            "Epoch 795/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4502 - accuracy: 0.8313 - val_loss: 0.6782 - val_accuracy: 0.7488\n",
            "Epoch 796/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4285 - accuracy: 0.8410 - val_loss: 0.6611 - val_accuracy: 0.7391\n",
            "Epoch 797/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4195 - accuracy: 0.8470 - val_loss: 0.6566 - val_accuracy: 0.7585\n",
            "Epoch 798/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4124 - accuracy: 0.8398 - val_loss: 0.6697 - val_accuracy: 0.7585\n",
            "Epoch 799/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3958 - accuracy: 0.8507 - val_loss: 0.6876 - val_accuracy: 0.7150\n",
            "Epoch 800/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4102 - accuracy: 0.8428 - val_loss: 0.6743 - val_accuracy: 0.7391\n",
            "Epoch 801/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4076 - accuracy: 0.8398 - val_loss: 0.6642 - val_accuracy: 0.7295\n",
            "Epoch 802/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4234 - accuracy: 0.8452 - val_loss: 0.6653 - val_accuracy: 0.7246\n",
            "Epoch 803/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4103 - accuracy: 0.8422 - val_loss: 0.6810 - val_accuracy: 0.7246\n",
            "Epoch 804/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4221 - accuracy: 0.8416 - val_loss: 0.6894 - val_accuracy: 0.7198\n",
            "Epoch 805/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4147 - accuracy: 0.8452 - val_loss: 0.6851 - val_accuracy: 0.7246\n",
            "Epoch 806/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3947 - accuracy: 0.8470 - val_loss: 0.6651 - val_accuracy: 0.7536\n",
            "Epoch 807/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4173 - accuracy: 0.8458 - val_loss: 0.6619 - val_accuracy: 0.7295\n",
            "Epoch 808/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4241 - accuracy: 0.8368 - val_loss: 0.6855 - val_accuracy: 0.7101\n",
            "Epoch 809/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4088 - accuracy: 0.8428 - val_loss: 0.6816 - val_accuracy: 0.7198\n",
            "Epoch 810/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3932 - accuracy: 0.8561 - val_loss: 0.6549 - val_accuracy: 0.7488\n",
            "Epoch 811/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4193 - accuracy: 0.8464 - val_loss: 0.6542 - val_accuracy: 0.7440\n",
            "Epoch 812/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3997 - accuracy: 0.8356 - val_loss: 0.6615 - val_accuracy: 0.7391\n",
            "Epoch 813/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4075 - accuracy: 0.8446 - val_loss: 0.6510 - val_accuracy: 0.7440\n",
            "Epoch 814/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3973 - accuracy: 0.8374 - val_loss: 0.6631 - val_accuracy: 0.7440\n",
            "Epoch 815/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4091 - accuracy: 0.8470 - val_loss: 0.6676 - val_accuracy: 0.7391\n",
            "Epoch 816/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4081 - accuracy: 0.8476 - val_loss: 0.6558 - val_accuracy: 0.7295\n",
            "Epoch 817/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3991 - accuracy: 0.8398 - val_loss: 0.6597 - val_accuracy: 0.7585\n",
            "Epoch 818/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4118 - accuracy: 0.8392 - val_loss: 0.6659 - val_accuracy: 0.7343\n",
            "Epoch 819/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4098 - accuracy: 0.8428 - val_loss: 0.6595 - val_accuracy: 0.7729\n",
            "Epoch 820/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3894 - accuracy: 0.8597 - val_loss: 0.6450 - val_accuracy: 0.7681\n",
            "Epoch 821/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4044 - accuracy: 0.8470 - val_loss: 0.6719 - val_accuracy: 0.7536\n",
            "Epoch 822/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3895 - accuracy: 0.8501 - val_loss: 0.6581 - val_accuracy: 0.7488\n",
            "Epoch 823/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4171 - accuracy: 0.8440 - val_loss: 0.6577 - val_accuracy: 0.7198\n",
            "Epoch 824/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3933 - accuracy: 0.8489 - val_loss: 0.6738 - val_accuracy: 0.7295\n",
            "Epoch 825/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4115 - accuracy: 0.8368 - val_loss: 0.6626 - val_accuracy: 0.7440\n",
            "Epoch 826/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4098 - accuracy: 0.8392 - val_loss: 0.6607 - val_accuracy: 0.7295\n",
            "Epoch 827/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3850 - accuracy: 0.8495 - val_loss: 0.6529 - val_accuracy: 0.7391\n",
            "Epoch 828/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3944 - accuracy: 0.8476 - val_loss: 0.6745 - val_accuracy: 0.7391\n",
            "Epoch 829/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4035 - accuracy: 0.8507 - val_loss: 0.6650 - val_accuracy: 0.7391\n",
            "Epoch 830/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3912 - accuracy: 0.8531 - val_loss: 0.6533 - val_accuracy: 0.7488\n",
            "Epoch 831/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3844 - accuracy: 0.8555 - val_loss: 0.6543 - val_accuracy: 0.7391\n",
            "Epoch 832/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3805 - accuracy: 0.8573 - val_loss: 0.6934 - val_accuracy: 0.7101\n",
            "Epoch 833/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3978 - accuracy: 0.8428 - val_loss: 0.6758 - val_accuracy: 0.7391\n",
            "Epoch 834/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4148 - accuracy: 0.8374 - val_loss: 0.6566 - val_accuracy: 0.7585\n",
            "Epoch 835/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3948 - accuracy: 0.8513 - val_loss: 0.6658 - val_accuracy: 0.7246\n",
            "Epoch 836/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4020 - accuracy: 0.8476 - val_loss: 0.6543 - val_accuracy: 0.7391\n",
            "Epoch 837/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3835 - accuracy: 0.8519 - val_loss: 0.6717 - val_accuracy: 0.7343\n",
            "Epoch 838/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3822 - accuracy: 0.8507 - val_loss: 0.6508 - val_accuracy: 0.7246\n",
            "Epoch 839/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3981 - accuracy: 0.8446 - val_loss: 0.6677 - val_accuracy: 0.7295\n",
            "Epoch 840/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3699 - accuracy: 0.8495 - val_loss: 0.6873 - val_accuracy: 0.7198\n",
            "Epoch 841/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4071 - accuracy: 0.8458 - val_loss: 0.6729 - val_accuracy: 0.7246\n",
            "Epoch 842/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3799 - accuracy: 0.8664 - val_loss: 0.6652 - val_accuracy: 0.7633\n",
            "Epoch 843/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3742 - accuracy: 0.8549 - val_loss: 0.6546 - val_accuracy: 0.7343\n",
            "Epoch 844/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3928 - accuracy: 0.8507 - val_loss: 0.6678 - val_accuracy: 0.7246\n",
            "Epoch 845/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3914 - accuracy: 0.8422 - val_loss: 0.6677 - val_accuracy: 0.7391\n",
            "Epoch 846/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3935 - accuracy: 0.8458 - val_loss: 0.6497 - val_accuracy: 0.7295\n",
            "Epoch 847/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3922 - accuracy: 0.8495 - val_loss: 0.6348 - val_accuracy: 0.7295\n",
            "Epoch 848/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3864 - accuracy: 0.8555 - val_loss: 0.6601 - val_accuracy: 0.7536\n",
            "Epoch 849/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3820 - accuracy: 0.8537 - val_loss: 0.6626 - val_accuracy: 0.7246\n",
            "Epoch 850/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3905 - accuracy: 0.8495 - val_loss: 0.6743 - val_accuracy: 0.6957\n",
            "Epoch 851/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3952 - accuracy: 0.8452 - val_loss: 0.6558 - val_accuracy: 0.7536\n",
            "Epoch 852/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3747 - accuracy: 0.8609 - val_loss: 0.6737 - val_accuracy: 0.7198\n",
            "Epoch 853/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3858 - accuracy: 0.8482 - val_loss: 0.6795 - val_accuracy: 0.7440\n",
            "Epoch 854/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4027 - accuracy: 0.8458 - val_loss: 0.6770 - val_accuracy: 0.7343\n",
            "Epoch 855/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3953 - accuracy: 0.8440 - val_loss: 0.7006 - val_accuracy: 0.7246\n",
            "Epoch 856/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4073 - accuracy: 0.8458 - val_loss: 0.6413 - val_accuracy: 0.7391\n",
            "Epoch 857/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3632 - accuracy: 0.8694 - val_loss: 0.6665 - val_accuracy: 0.7536\n",
            "Epoch 858/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3850 - accuracy: 0.8555 - val_loss: 0.6730 - val_accuracy: 0.7391\n",
            "Epoch 859/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3760 - accuracy: 0.8597 - val_loss: 0.6652 - val_accuracy: 0.7198\n",
            "Epoch 860/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3805 - accuracy: 0.8561 - val_loss: 0.6502 - val_accuracy: 0.7295\n",
            "Epoch 861/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3910 - accuracy: 0.8543 - val_loss: 0.6687 - val_accuracy: 0.7391\n",
            "Epoch 862/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3614 - accuracy: 0.8628 - val_loss: 0.6734 - val_accuracy: 0.7053\n",
            "Epoch 863/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3954 - accuracy: 0.8531 - val_loss: 0.6850 - val_accuracy: 0.7391\n",
            "Epoch 864/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3546 - accuracy: 0.8712 - val_loss: 0.6739 - val_accuracy: 0.7295\n",
            "Epoch 865/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3933 - accuracy: 0.8597 - val_loss: 0.6764 - val_accuracy: 0.7343\n",
            "Epoch 866/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3675 - accuracy: 0.8622 - val_loss: 0.6550 - val_accuracy: 0.7343\n",
            "Epoch 867/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3730 - accuracy: 0.8640 - val_loss: 0.6594 - val_accuracy: 0.7440\n",
            "Epoch 868/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3727 - accuracy: 0.8567 - val_loss: 0.6510 - val_accuracy: 0.7488\n",
            "Epoch 869/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3586 - accuracy: 0.8615 - val_loss: 0.6706 - val_accuracy: 0.7246\n",
            "Epoch 870/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3826 - accuracy: 0.8482 - val_loss: 0.6497 - val_accuracy: 0.7536\n",
            "Epoch 871/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3831 - accuracy: 0.8470 - val_loss: 0.6678 - val_accuracy: 0.7391\n",
            "Epoch 872/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3630 - accuracy: 0.8670 - val_loss: 0.6690 - val_accuracy: 0.7488\n",
            "Epoch 873/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3610 - accuracy: 0.8597 - val_loss: 0.6457 - val_accuracy: 0.7488\n",
            "Epoch 874/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3684 - accuracy: 0.8597 - val_loss: 0.6550 - val_accuracy: 0.7440\n",
            "Epoch 875/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3626 - accuracy: 0.8579 - val_loss: 0.6604 - val_accuracy: 0.7536\n",
            "Epoch 876/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3713 - accuracy: 0.8603 - val_loss: 0.6546 - val_accuracy: 0.7198\n",
            "Epoch 877/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3771 - accuracy: 0.8628 - val_loss: 0.6720 - val_accuracy: 0.7391\n",
            "Epoch 878/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3557 - accuracy: 0.8706 - val_loss: 0.6373 - val_accuracy: 0.7633\n",
            "Epoch 879/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 0.3816 - accuracy: 0.8573 - val_loss: 0.6589 - val_accuracy: 0.7343\n",
            "Epoch 880/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 0.3686 - accuracy: 0.8567 - val_loss: 0.6662 - val_accuracy: 0.7488\n",
            "Epoch 881/1000\n",
            "104/104 [==============================] - 1s 8ms/step - loss: 0.3635 - accuracy: 0.8603 - val_loss: 0.6630 - val_accuracy: 0.7391\n",
            "Epoch 882/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 0.3744 - accuracy: 0.8519 - val_loss: 0.6362 - val_accuracy: 0.7391\n",
            "Epoch 883/1000\n",
            "104/104 [==============================] - 1s 8ms/step - loss: 0.3707 - accuracy: 0.8597 - val_loss: 0.6708 - val_accuracy: 0.7343\n",
            "Epoch 884/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3669 - accuracy: 0.8615 - val_loss: 0.6549 - val_accuracy: 0.7633\n",
            "Epoch 885/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3616 - accuracy: 0.8664 - val_loss: 0.6796 - val_accuracy: 0.7198\n",
            "Epoch 886/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3593 - accuracy: 0.8603 - val_loss: 0.6532 - val_accuracy: 0.7391\n",
            "Epoch 887/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3692 - accuracy: 0.8640 - val_loss: 0.6492 - val_accuracy: 0.7633\n",
            "Epoch 888/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3870 - accuracy: 0.8549 - val_loss: 0.6539 - val_accuracy: 0.7536\n",
            "Epoch 889/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3579 - accuracy: 0.8706 - val_loss: 0.6482 - val_accuracy: 0.7536\n",
            "Epoch 890/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3492 - accuracy: 0.8664 - val_loss: 0.6168 - val_accuracy: 0.7874\n",
            "Epoch 891/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3656 - accuracy: 0.8579 - val_loss: 0.6817 - val_accuracy: 0.7005\n",
            "Epoch 892/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3693 - accuracy: 0.8634 - val_loss: 0.6727 - val_accuracy: 0.7295\n",
            "Epoch 893/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3635 - accuracy: 0.8682 - val_loss: 0.6427 - val_accuracy: 0.7391\n",
            "Epoch 894/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3438 - accuracy: 0.8706 - val_loss: 0.6600 - val_accuracy: 0.7391\n",
            "Epoch 895/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3625 - accuracy: 0.8724 - val_loss: 0.6569 - val_accuracy: 0.7536\n",
            "Epoch 896/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3624 - accuracy: 0.8646 - val_loss: 0.6570 - val_accuracy: 0.7440\n",
            "Epoch 897/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3643 - accuracy: 0.8634 - val_loss: 0.6464 - val_accuracy: 0.7343\n",
            "Epoch 898/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3645 - accuracy: 0.8688 - val_loss: 0.6435 - val_accuracy: 0.7729\n",
            "Epoch 899/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3544 - accuracy: 0.8634 - val_loss: 0.6668 - val_accuracy: 0.7391\n",
            "Epoch 900/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3703 - accuracy: 0.8531 - val_loss: 0.6640 - val_accuracy: 0.7343\n",
            "Epoch 901/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3678 - accuracy: 0.8579 - val_loss: 0.6595 - val_accuracy: 0.7295\n",
            "Epoch 902/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3528 - accuracy: 0.8567 - val_loss: 0.6609 - val_accuracy: 0.7295\n",
            "Epoch 903/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3746 - accuracy: 0.8567 - val_loss: 0.6572 - val_accuracy: 0.7440\n",
            "Epoch 904/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 0.3473 - accuracy: 0.8688 - val_loss: 0.6333 - val_accuracy: 0.7874\n",
            "Epoch 905/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3450 - accuracy: 0.8724 - val_loss: 0.6625 - val_accuracy: 0.7633\n",
            "Epoch 906/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3354 - accuracy: 0.8658 - val_loss: 0.6650 - val_accuracy: 0.7391\n",
            "Epoch 907/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3423 - accuracy: 0.8700 - val_loss: 0.6334 - val_accuracy: 0.7729\n",
            "Epoch 908/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3548 - accuracy: 0.8628 - val_loss: 0.6398 - val_accuracy: 0.7826\n",
            "Epoch 909/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3586 - accuracy: 0.8628 - val_loss: 0.6684 - val_accuracy: 0.7391\n",
            "Epoch 910/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3521 - accuracy: 0.8730 - val_loss: 0.6573 - val_accuracy: 0.7536\n",
            "Epoch 911/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3637 - accuracy: 0.8567 - val_loss: 0.6720 - val_accuracy: 0.7585\n",
            "Epoch 912/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3554 - accuracy: 0.8664 - val_loss: 0.6640 - val_accuracy: 0.7440\n",
            "Epoch 913/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3421 - accuracy: 0.8694 - val_loss: 0.6475 - val_accuracy: 0.7633\n",
            "Epoch 914/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3540 - accuracy: 0.8742 - val_loss: 0.6467 - val_accuracy: 0.7488\n",
            "Epoch 915/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3350 - accuracy: 0.8821 - val_loss: 0.6541 - val_accuracy: 0.7681\n",
            "Epoch 916/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3453 - accuracy: 0.8688 - val_loss: 0.6626 - val_accuracy: 0.7440\n",
            "Epoch 917/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3246 - accuracy: 0.8797 - val_loss: 0.6438 - val_accuracy: 0.7440\n",
            "Epoch 918/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3371 - accuracy: 0.8664 - val_loss: 0.6334 - val_accuracy: 0.7874\n",
            "Epoch 919/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3449 - accuracy: 0.8694 - val_loss: 0.6369 - val_accuracy: 0.7681\n",
            "Epoch 920/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3653 - accuracy: 0.8634 - val_loss: 0.6265 - val_accuracy: 0.7488\n",
            "Epoch 921/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3387 - accuracy: 0.8815 - val_loss: 0.6238 - val_accuracy: 0.7585\n",
            "Epoch 922/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3613 - accuracy: 0.8670 - val_loss: 0.6268 - val_accuracy: 0.7536\n",
            "Epoch 923/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3554 - accuracy: 0.8615 - val_loss: 0.6345 - val_accuracy: 0.7488\n",
            "Epoch 924/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3553 - accuracy: 0.8537 - val_loss: 0.6420 - val_accuracy: 0.7729\n",
            "Epoch 925/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3332 - accuracy: 0.8706 - val_loss: 0.6393 - val_accuracy: 0.7633\n",
            "Epoch 926/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3483 - accuracy: 0.8615 - val_loss: 0.6535 - val_accuracy: 0.7585\n",
            "Epoch 927/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3472 - accuracy: 0.8682 - val_loss: 0.6743 - val_accuracy: 0.7343\n",
            "Epoch 928/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3501 - accuracy: 0.8543 - val_loss: 0.6565 - val_accuracy: 0.7391\n",
            "Epoch 929/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3659 - accuracy: 0.8646 - val_loss: 0.6443 - val_accuracy: 0.7536\n",
            "Epoch 930/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3603 - accuracy: 0.8585 - val_loss: 0.6441 - val_accuracy: 0.7536\n",
            "Epoch 931/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3360 - accuracy: 0.8682 - val_loss: 0.6399 - val_accuracy: 0.7585\n",
            "Epoch 932/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3507 - accuracy: 0.8615 - val_loss: 0.6469 - val_accuracy: 0.7343\n",
            "Epoch 933/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3285 - accuracy: 0.8821 - val_loss: 0.6336 - val_accuracy: 0.7488\n",
            "Epoch 934/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3094 - accuracy: 0.8906 - val_loss: 0.6589 - val_accuracy: 0.7343\n",
            "Epoch 935/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3518 - accuracy: 0.8700 - val_loss: 0.6494 - val_accuracy: 0.7343\n",
            "Epoch 936/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3406 - accuracy: 0.8785 - val_loss: 0.6539 - val_accuracy: 0.7536\n",
            "Epoch 937/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3358 - accuracy: 0.8700 - val_loss: 0.6515 - val_accuracy: 0.7681\n",
            "Epoch 938/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3403 - accuracy: 0.8730 - val_loss: 0.6767 - val_accuracy: 0.7343\n",
            "Epoch 939/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3485 - accuracy: 0.8676 - val_loss: 0.6709 - val_accuracy: 0.7488\n",
            "Epoch 940/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3314 - accuracy: 0.8821 - val_loss: 0.6425 - val_accuracy: 0.7633\n",
            "Epoch 941/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3441 - accuracy: 0.8646 - val_loss: 0.6620 - val_accuracy: 0.7343\n",
            "Epoch 942/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3435 - accuracy: 0.8730 - val_loss: 0.6446 - val_accuracy: 0.7488\n",
            "Epoch 943/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3408 - accuracy: 0.8742 - val_loss: 0.6348 - val_accuracy: 0.7729\n",
            "Epoch 944/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3339 - accuracy: 0.8767 - val_loss: 0.6653 - val_accuracy: 0.7488\n",
            "Epoch 945/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3185 - accuracy: 0.8827 - val_loss: 0.6567 - val_accuracy: 0.7585\n",
            "Epoch 946/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3505 - accuracy: 0.8706 - val_loss: 0.6671 - val_accuracy: 0.7198\n",
            "Epoch 947/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3331 - accuracy: 0.8706 - val_loss: 0.6671 - val_accuracy: 0.7391\n",
            "Epoch 948/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3418 - accuracy: 0.8597 - val_loss: 0.6520 - val_accuracy: 0.7295\n",
            "Epoch 949/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3473 - accuracy: 0.8688 - val_loss: 0.6724 - val_accuracy: 0.7391\n",
            "Epoch 950/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3375 - accuracy: 0.8773 - val_loss: 0.6366 - val_accuracy: 0.7633\n",
            "Epoch 951/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3548 - accuracy: 0.8640 - val_loss: 0.6454 - val_accuracy: 0.7585\n",
            "Epoch 952/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3290 - accuracy: 0.8851 - val_loss: 0.6251 - val_accuracy: 0.7778\n",
            "Epoch 953/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3154 - accuracy: 0.8803 - val_loss: 0.6185 - val_accuracy: 0.7874\n",
            "Epoch 954/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3176 - accuracy: 0.8736 - val_loss: 0.6431 - val_accuracy: 0.7391\n",
            "Epoch 955/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3372 - accuracy: 0.8767 - val_loss: 0.6528 - val_accuracy: 0.7488\n",
            "Epoch 956/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3189 - accuracy: 0.8869 - val_loss: 0.6502 - val_accuracy: 0.7585\n",
            "Epoch 957/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3425 - accuracy: 0.8748 - val_loss: 0.6659 - val_accuracy: 0.7391\n",
            "Epoch 958/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3451 - accuracy: 0.8664 - val_loss: 0.6609 - val_accuracy: 0.7488\n",
            "Epoch 959/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3489 - accuracy: 0.8773 - val_loss: 0.6481 - val_accuracy: 0.7440\n",
            "Epoch 960/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3342 - accuracy: 0.8755 - val_loss: 0.6477 - val_accuracy: 0.7488\n",
            "Epoch 961/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3100 - accuracy: 0.8809 - val_loss: 0.6237 - val_accuracy: 0.7585\n",
            "Epoch 962/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3498 - accuracy: 0.8730 - val_loss: 0.6778 - val_accuracy: 0.7246\n",
            "Epoch 963/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3223 - accuracy: 0.8881 - val_loss: 0.6395 - val_accuracy: 0.7585\n",
            "Epoch 964/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3053 - accuracy: 0.8827 - val_loss: 0.6250 - val_accuracy: 0.7585\n",
            "Epoch 965/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3212 - accuracy: 0.8755 - val_loss: 0.6442 - val_accuracy: 0.7536\n",
            "Epoch 966/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3100 - accuracy: 0.8809 - val_loss: 0.6556 - val_accuracy: 0.7391\n",
            "Epoch 967/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3289 - accuracy: 0.8712 - val_loss: 0.6547 - val_accuracy: 0.7536\n",
            "Epoch 968/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3308 - accuracy: 0.8736 - val_loss: 0.6683 - val_accuracy: 0.7536\n",
            "Epoch 969/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3127 - accuracy: 0.8791 - val_loss: 0.6315 - val_accuracy: 0.7633\n",
            "Epoch 970/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3411 - accuracy: 0.8688 - val_loss: 0.6479 - val_accuracy: 0.7440\n",
            "Epoch 971/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3091 - accuracy: 0.8857 - val_loss: 0.6116 - val_accuracy: 0.7633\n",
            "Epoch 972/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3153 - accuracy: 0.8815 - val_loss: 0.6693 - val_accuracy: 0.7295\n",
            "Epoch 973/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3161 - accuracy: 0.8869 - val_loss: 0.6400 - val_accuracy: 0.7633\n",
            "Epoch 974/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3301 - accuracy: 0.8839 - val_loss: 0.6421 - val_accuracy: 0.7633\n",
            "Epoch 975/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3159 - accuracy: 0.8845 - val_loss: 0.6881 - val_accuracy: 0.7295\n",
            "Epoch 976/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3132 - accuracy: 0.8839 - val_loss: 0.6403 - val_accuracy: 0.7440\n",
            "Epoch 977/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3309 - accuracy: 0.8785 - val_loss: 0.6290 - val_accuracy: 0.7874\n",
            "Epoch 978/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3206 - accuracy: 0.8851 - val_loss: 0.6463 - val_accuracy: 0.7440\n",
            "Epoch 979/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3109 - accuracy: 0.8924 - val_loss: 0.6371 - val_accuracy: 0.7633\n",
            "Epoch 980/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3210 - accuracy: 0.8742 - val_loss: 0.6508 - val_accuracy: 0.7488\n",
            "Epoch 981/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3087 - accuracy: 0.8851 - val_loss: 0.6589 - val_accuracy: 0.7391\n",
            "Epoch 982/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3310 - accuracy: 0.8718 - val_loss: 0.6604 - val_accuracy: 0.7440\n",
            "Epoch 983/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3167 - accuracy: 0.8869 - val_loss: 0.6820 - val_accuracy: 0.7391\n",
            "Epoch 984/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3225 - accuracy: 0.8773 - val_loss: 0.6460 - val_accuracy: 0.7488\n",
            "Epoch 985/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3261 - accuracy: 0.8803 - val_loss: 0.6424 - val_accuracy: 0.7536\n",
            "Epoch 986/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3203 - accuracy: 0.8827 - val_loss: 0.6442 - val_accuracy: 0.7440\n",
            "Epoch 987/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.2911 - accuracy: 0.8875 - val_loss: 0.6502 - val_accuracy: 0.7295\n",
            "Epoch 988/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3091 - accuracy: 0.8773 - val_loss: 0.6183 - val_accuracy: 0.7729\n",
            "Epoch 989/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3140 - accuracy: 0.8815 - val_loss: 0.6756 - val_accuracy: 0.7295\n",
            "Epoch 990/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3190 - accuracy: 0.8821 - val_loss: 0.6418 - val_accuracy: 0.7681\n",
            "Epoch 991/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3216 - accuracy: 0.8827 - val_loss: 0.6451 - val_accuracy: 0.7343\n",
            "Epoch 992/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 0.3213 - accuracy: 0.8742 - val_loss: 0.6221 - val_accuracy: 0.7729\n",
            "Epoch 993/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3131 - accuracy: 0.8767 - val_loss: 0.6302 - val_accuracy: 0.8019\n",
            "Epoch 994/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3184 - accuracy: 0.8845 - val_loss: 0.6316 - val_accuracy: 0.7488\n",
            "Epoch 995/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3148 - accuracy: 0.8779 - val_loss: 0.6349 - val_accuracy: 0.7585\n",
            "Epoch 996/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3188 - accuracy: 0.8833 - val_loss: 0.6394 - val_accuracy: 0.7440\n",
            "Epoch 997/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3087 - accuracy: 0.8875 - val_loss: 0.6346 - val_accuracy: 0.7633\n",
            "Epoch 998/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3122 - accuracy: 0.8803 - val_loss: 0.6534 - val_accuracy: 0.7536\n",
            "Epoch 999/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3064 - accuracy: 0.8748 - val_loss: 0.6783 - val_accuracy: 0.7440\n",
            "Epoch 1000/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3139 - accuracy: 0.8821 - val_loss: 0.6595 - val_accuracy: 0.7488\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(cnnhistory.history['loss'])\n",
        "plt.plot(cnnhistory.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train','test'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "oQYnuaCrDH_A",
        "outputId": "46d47fe2-a444-44d8-a6cc-fec5cc1eb312"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(cnnhistory.history['accuracy'])\n",
        "plt.plot(cnnhistory.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train','test'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "IFkTuO8nDNdq",
        "outputId": "173f7edb-4fb7-4c89-8111-e07e540c469f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#predictions = model.predict_classes(x_testcnn)\n",
        "predictions = model.predict(x_testcnn)"
      ],
      "metadata": {
        "id": "A2xtrN5wDQgY"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_f_ohjXKDUpY",
        "outputId": "bc93793b-e692-4db9-824d-6384f2e1d494"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.01598031e-04, 2.32896014e-06, 1.61480848e-02, 1.25398464e-03,\n",
              "        2.49843951e-02, 9.57509518e-01],\n",
              "       [9.25292075e-02, 8.67258310e-01, 2.40417686e-03, 1.66398324e-02,\n",
              "        1.50635615e-02, 6.10497408e-03],\n",
              "       [2.46454565e-05, 1.32039457e-03, 9.52492833e-01, 8.26473406e-04,\n",
              "        3.82754020e-02, 7.06024375e-03],\n",
              "       ...,\n",
              "       [1.46739534e-04, 2.45461865e-06, 2.51715741e-04, 8.49417993e-04,\n",
              "        9.73344505e-01, 2.54051592e-02],\n",
              "       [8.16058088e-03, 1.86737604e-03, 2.61757663e-03, 7.39974439e-01,\n",
              "        7.47216912e-03, 2.39907876e-01],\n",
              "       [9.68122482e-02, 3.13083753e-02, 6.53715339e-03, 1.60482116e-02,\n",
              "        8.49196970e-01, 9.70494220e-05]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmA2bFgsDW3D",
        "outputId": "85a2ddb2-1246-4257-f372-6c123fb02a69"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 1, 2, 5, 3, 4, 2, 0, 3, 1, 3, 2, 1, 4, 3, 0, 5, 0, 2, 5, 0, 0,\n",
              "       1, 3, 2, 3, 2, 5, 4, 2, 2, 3, 1, 2, 5, 5, 3, 5, 3, 3, 4, 1, 3, 2,\n",
              "       3, 1, 5, 3, 1, 2, 4, 4, 5, 5, 5, 1, 2, 3, 2, 5, 0, 2, 4, 2, 5, 4,\n",
              "       3, 0, 1, 2, 5, 2, 1, 2, 4, 5, 4, 1, 5, 5, 3, 1, 5, 3, 2, 4, 2, 5,\n",
              "       0, 5, 0, 5, 4, 0, 2, 4, 0, 2, 2, 0, 2, 3, 5, 3, 5, 5, 3, 1, 2, 2,\n",
              "       2, 1, 3, 2, 1, 3, 4, 4, 1, 1, 2, 4, 4, 1, 4, 2, 0, 4, 5, 5, 3, 1,\n",
              "       2, 1, 5, 5, 1, 2, 3, 3, 0, 3, 3, 0, 1, 0, 1, 2, 1, 5, 1, 2, 2, 0,\n",
              "       1, 1, 1, 1, 1, 4, 0, 1, 5, 1, 4, 5, 5, 4, 1, 0, 3, 5, 2, 3, 0, 4,\n",
              "       1, 3, 1, 0, 5, 5, 3, 5, 4, 5, 4, 0, 4, 1, 2, 1, 5, 0, 4, 3, 2, 1,\n",
              "       0, 3, 5, 0, 4, 5, 4, 5, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_Ytest = y_test.astype(int)"
      ],
      "metadata": {
        "id": "0PHDKWJWDY3g"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_Ytest"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k68v2i-pDbZE",
        "outputId": "75a1e83b-2bd2-4ada-a917-48127caa3c50"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 1, 2, 5, 3, 4, 2, 0, 3, 1, 3, 2, 1, 4, 3, 0, 5, 0, 2, 5, 0, 0,\n",
              "       1, 3, 2, 3, 2, 5, 4, 2, 2, 3, 1, 2, 5, 5, 3, 5, 3, 3, 4, 1, 3, 2,\n",
              "       3, 1, 5, 3, 1, 2, 4, 4, 5, 5, 5, 1, 2, 3, 2, 5, 0, 2, 4, 2, 5, 4,\n",
              "       3, 0, 1, 2, 5, 2, 1, 2, 4, 5, 4, 1, 5, 5, 3, 1, 5, 3, 2, 4, 2, 5,\n",
              "       0, 5, 0, 5, 4, 0, 2, 4, 0, 2, 2, 0, 2, 3, 5, 3, 5, 5, 3, 1, 2, 2,\n",
              "       2, 1, 3, 2, 1, 3, 4, 4, 1, 1, 2, 4, 4, 1, 4, 2, 0, 4, 5, 5, 3, 1,\n",
              "       2, 1, 5, 5, 1, 2, 3, 3, 0, 3, 3, 0, 1, 0, 1, 2, 1, 5, 1, 2, 2, 0,\n",
              "       1, 1, 1, 1, 1, 4, 0, 1, 5, 1, 4, 5, 5, 4, 1, 0, 3, 5, 2, 3, 0, 4,\n",
              "       1, 3, 1, 0, 5, 5, 3, 5, 4, 5, 4, 0, 4, 1, 2, 1, 5, 0, 4, 3, 2, 1,\n",
              "       0, 3, 5, 0, 4, 5, 4, 5, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds1=predictions.argmax(axis=1)\n",
        "preds1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_cAI39VFhtp",
        "outputId": "e7d8c24e-33ec-41e1-9133-413625e53700"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 1, 2, 5, 3, 4, 2, 0, 3, 0, 3, 2, 1, 4, 3, 0, 5, 1, 2, 5, 0, 0,\n",
              "       1, 3, 4, 3, 2, 5, 4, 4, 2, 3, 1, 2, 5, 5, 1, 5, 3, 3, 4, 1, 0, 2,\n",
              "       3, 1, 3, 3, 1, 4, 4, 4, 5, 4, 4, 1, 2, 3, 2, 5, 0, 2, 4, 4, 5, 4,\n",
              "       0, 1, 1, 2, 5, 2, 1, 5, 4, 5, 4, 1, 5, 5, 3, 1, 5, 3, 2, 4, 2, 3,\n",
              "       0, 4, 0, 5, 4, 0, 3, 4, 3, 1, 2, 0, 2, 3, 5, 3, 5, 5, 5, 1, 2, 2,\n",
              "       2, 1, 3, 2, 1, 4, 4, 4, 0, 5, 2, 4, 4, 1, 4, 2, 2, 4, 4, 5, 5, 1,\n",
              "       4, 1, 5, 5, 1, 3, 3, 3, 0, 3, 5, 0, 1, 0, 1, 2, 1, 5, 0, 2, 4, 0,\n",
              "       1, 1, 1, 1, 0, 4, 0, 1, 5, 0, 4, 5, 5, 2, 1, 0, 5, 0, 2, 3, 3, 4,\n",
              "       1, 3, 1, 5, 5, 2, 3, 5, 4, 5, 4, 0, 4, 0, 2, 1, 5, 0, 4, 1, 2, 0,\n",
              "       1, 5, 5, 0, 4, 5, 4, 3, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "abc = preds1.astype(int).flatten()"
      ],
      "metadata": {
        "id": "H4WEmM60NfQn"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "c = confusion_matrix(new_Ytest, abc) \n",
        "c"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKEBeEBlFjB8",
        "outputId": "1bd4d9b7-8ee0-4d5e-cc95-f5bbd5bd8563"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[18,  3,  1,  2,  0,  1],\n",
              "       [ 7, 31,  0,  0,  0,  1],\n",
              "       [ 0,  1, 28,  2,  6,  1],\n",
              "       [ 2,  2,  0, 23,  1,  5],\n",
              "       [ 0,  0,  1,  0, 29,  0],\n",
              "       [ 1,  0,  1,  3,  4, 33]])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4RjG7LWLSQx",
        "outputId": "f10553ac-f201-41d3-a220-b7a9aebf7cb1"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 1, 2, 5, 3, 4, 2, 0, 3, 1, 3, 2, 1, 4, 3, 0, 5, 0, 2, 5, 0, 0,\n",
              "       1, 3, 2, 3, 2, 5, 4, 2, 2, 3, 1, 2, 5, 5, 3, 5, 3, 3, 4, 1, 3, 2,\n",
              "       3, 1, 5, 3, 1, 2, 4, 4, 5, 5, 5, 1, 2, 3, 2, 5, 0, 2, 4, 2, 5, 4,\n",
              "       3, 0, 1, 2, 5, 2, 1, 2, 4, 5, 4, 1, 5, 5, 3, 1, 5, 3, 2, 4, 2, 5,\n",
              "       0, 5, 0, 5, 4, 0, 2, 4, 0, 2, 2, 0, 2, 3, 5, 3, 5, 5, 3, 1, 2, 2,\n",
              "       2, 1, 3, 2, 1, 3, 4, 4, 1, 1, 2, 4, 4, 1, 4, 2, 0, 4, 5, 5, 3, 1,\n",
              "       2, 1, 5, 5, 1, 2, 3, 3, 0, 3, 3, 0, 1, 0, 1, 2, 1, 5, 1, 2, 2, 0,\n",
              "       1, 1, 1, 1, 1, 4, 0, 1, 5, 1, 4, 5, 5, 4, 1, 0, 3, 5, 2, 3, 0, 4,\n",
              "       1, 3, 1, 0, 5, 5, 3, 5, 4, 5, 4, 0, 4, 1, 2, 1, 5, 0, 4, 3, 2, 1,\n",
              "       0, 3, 5, 0, 4, 5, 4, 5, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/drive/MyDrive/model2')\n",
        "print(\"MODEL SAVED\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsJBNeMWDl6Z",
        "outputId": "2a62da6c-3339-4362-9609-d1333a55cd3f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/model2/assets\n",
            "MODEL SAVED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_model=keras.models.load_model('/content/drive/MyDrive/model2')\n",
        "new_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3KjaO9GDoNl",
        "outputId": "6932b693-ffec-41c7-e2e4-3c54b49b2fe8"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d (Conv1D)             (None, 40, 128)           1152      \n",
            "                                                                 \n",
            " activation (Activation)     (None, 40, 128)           0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 40, 128)           0         \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, 8, 128)           0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 8, 256)            262400    \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 8, 256)            0         \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 8, 256)            0         \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPooling  (None, 1, 256)           0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               65792     \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 256)               65792     \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 256)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 6)                 1542      \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 6)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 396,678\n",
            "Trainable params: 396,678\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, acc =model.evaluate(x_testcnn, y_test)\n",
        "print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3kzoqPkDqqS",
        "outputId": "9c6378ca-135e-4884-bd95-417b31e742b7"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7/7 [==============================] - 0s 3ms/step - loss: 0.6157 - accuracy: 0.7826\n",
            "Restored model, accuracy: 78.26%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, acc =model.evaluate(X_train, y_train)\n",
        "print(\"Restored model train, accuracy: {:5.2f}%\".format(100*acc))"
      ],
      "metadata": {
        "id": "fJDTGH_OUX0z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31985afb-27b8-4a9b-c13e-fb9b7ba25727"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "52/52 [==============================] - 0s 4ms/step - loss: 0.2311 - accuracy: 0.9438\n",
            "Restored model train, accuracy: 94.38%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix,accuracy_score\n",
        "import seaborn as sn\n",
        "\n",
        "print(classification_report(new_Ytest,abc))\n",
        "\n",
        "acc = float(accuracy_score(new_Ytest,abc))*100\n",
        "print(\"----accuracy score %s ----\" % acc)\n",
        "\n",
        "cm = confusion_matrix(new_Ytest,abc)\n",
        "#df_cm = pd.DataFrame(cm)\n",
        "class_names = ['neutral','calm', 'happy','sad','angry', 'fearful' ]\n",
        "df_cm = pd.DataFrame(cm, index=class_names, columns=class_names,)\n",
        "sn.heatmap(df_cm, annot=True, fmt='')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "id": "SfSC3El94LZg",
        "outputId": "13bfa5a8-76be-45cf-91a5-3f170c175547"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.72      0.68        25\n",
            "           1       0.84      0.79      0.82        39\n",
            "           2       0.90      0.74      0.81        38\n",
            "           3       0.77      0.70      0.73        33\n",
            "           4       0.72      0.97      0.83        30\n",
            "           5       0.80      0.79      0.80        42\n",
            "\n",
            "    accuracy                           0.78       207\n",
            "   macro avg       0.78      0.78      0.78       207\n",
            "weighted avg       0.79      0.78      0.78       207\n",
            "\n",
            "----accuracy score 78.26086956521739 ----\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}