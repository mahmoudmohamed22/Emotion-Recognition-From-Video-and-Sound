{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "original_Adam_lr=0_00002_ 1000 epoch_try2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SRCO-a_AlH5",
        "outputId": "3a557a4a-025c-426e-a27e-d0a81db51dbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install librosa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oi8pWV3yBVjO",
        "outputId": "b6007001-5c26-4ea9-9d98-c34239921f31"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.7/dist-packages (0.8.1)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (21.3)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.0.2)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.6.0)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.2.2)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.51.2)\n",
            "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.10.3.post1)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.21.6)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (2.1.9)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa) (0.34.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->librosa) (3.0.9)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa) (2.23.0)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa) (1.4.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.10)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.7/dist-packages (from resampy>=0.2.2->librosa) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.10.2->librosa) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa) (2.21)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJjcbxwy46bG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "aaa7685f-a938-4a15-eb6b-30a01850638b"
      },
      "source": [
        "# Orignial Notebook: https://github.com/MITESHPUTHRANNEU/Speech-Emotion-Analyzer/blob/master/final_results_gender_test.ipynb\n",
        "# This notebook author: Reza Chu\n",
        "# Last Editing Date: 31st May 2019\n",
        "\n",
        "## Python\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "\n",
        "import IPython\n",
        "from IPython.display import Audio\n",
        "from IPython.display import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "## Package\n",
        "import glob \n",
        "import keras\n",
        "import IPython.display as ipd\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.graph_objs as go\n",
        "import plotly.offline as py\n",
        "import plotly.tools as tls\n",
        "import seaborn as sns\n",
        "import scipy.io.wavfile\n",
        "import tensorflow as tf\n",
        "py.init_notebook_mode(connected=True)\n",
        "\n",
        "## Keras\n",
        "from keras import regularizers\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping\n",
        "from keras.callbacks import  History, ReduceLROnPlateau, CSVLogger\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM\n",
        "from keras.layers import Input, Flatten, Dropout, Activation, BatchNormalization\n",
        "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import np_utils\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "## Sklearn\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "## Rest\n",
        "from scipy.fftpack import fft\n",
        "from scipy import signal\n",
        "from scipy.io import wavfile\n",
        "from tqdm import tqdm\n",
        "\n",
        "input_duration=3\n",
        "# % pylab inline"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "        <script type=\"text/javascript\">\n",
              "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
              "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
              "        if (typeof require !== 'undefined') {\n",
              "        require.undef(\"plotly\");\n",
              "        requirejs.config({\n",
              "            paths: {\n",
              "                'plotly': ['https://cdn.plot.ly/plotly-2.8.3.min']\n",
              "            }\n",
              "        });\n",
              "        require(['plotly'], function(Plotly) {\n",
              "            window._Plotly = Plotly;\n",
              "        });\n",
              "        }\n",
              "        </script>\n",
              "        "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#radvass speech and song\n",
        "import time\n",
        "import os\n",
        "import librosa\n",
        "from librosa import display\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "count=0\n",
        "path1 = '/content/drive/MyDrive/RAVDESS_speech'\n",
        "path2 = '/content/drive/MyDrive/RAVDESS_song'\n",
        "\n",
        "lst = []\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for subdir, dirs, files in os.walk(path2):\n",
        "  print(dirs)\n",
        "  print(subdir)\n",
        "  #print(files)\n",
        "  for file in files:\n",
        "      try:\n",
        "        count +=1\n",
        "        #Load librosa array, obtain mfcss, store the file and the mcss information in a new array\n",
        "        X, sample_rate = librosa.load(os.path.join(subdir,file), res_type='kaiser_fast')\n",
        "        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0) \n",
        "        # The instruction below converts the labels (from 1 to 8) to a series from 0 to 7\n",
        "        # This is because our predictor needs to start from 0 otherwise it will try to predict also 0.\n",
        "        file = int(file[7:8]) - 1 \n",
        "        arr = mfccs, file\n",
        "        lst.append(arr)\n",
        "      # If the file is not valid, skip it\n",
        "      except ValueError:\n",
        "        continue\n",
        "\n",
        "for subdir, dirs, files in os.walk(path1):\n",
        "  print(dirs)\n",
        "  print(subdir)\n",
        " # print(files)\n",
        "  for file in files:\n",
        "      try:\n",
        "        #Load librosa array, obtain mfcss, store the file and the mcss information in a new array\n",
        "        X, sample_rate = librosa.load(os.path.join(subdir,file), res_type='kaiser_fast')\n",
        "        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0) \n",
        "        # The instruction below converts the labels (from 1 to 8) to a series from 0 to 7\n",
        "        # This is because our predictor needs to start from 0 otherwise it will try to predict also 0.\n",
        "        file = int(file[7:8]) - 1 \n",
        "        if(file ==6 or file==7):\n",
        "          continue\n",
        "        count +=1\n",
        "        arr = mfccs, file\n",
        "        lst.append(arr)\n",
        "      # If the file is not valid, skip it\n",
        "      except ValueError:\n",
        "        continue\n",
        "\n",
        "\n",
        "\n",
        "print(\"--- Data loaded. Loading time: %s seconds ---\" % (time.time() - start_time))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfuyRZl42ccZ",
        "outputId": "ad975cd8-03fa-4ffa-8d61-90bde1b46872"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Actor_10', 'Actor_11', 'Actor_13', 'Actor_12', 'Actor_14', 'Actor_15', 'Actor_16', 'Actor_17', 'Actor_18', 'Actor_19', 'Actor_21', 'Actor_22', 'Actor_23', 'Actor_09', 'Actor_24', 'Actor_04', 'Actor_08', 'Actor_03', 'Actor_02', 'Actor_06', 'Actor_05', 'Actor_01', 'Actor_07', 'Actor_20']\n",
            "/content/drive/MyDrive/RAVDESS_song\n",
            "[]\n",
            "/content/drive/MyDrive/RAVDESS_song/Actor_10\n",
            "[]\n",
            "/content/drive/MyDrive/RAVDESS_song/Actor_11\n",
            "[]\n",
            "/content/drive/MyDrive/RAVDESS_song/Actor_13\n",
            "[]\n",
            "/content/drive/MyDrive/RAVDESS_song/Actor_12\n",
            "[]\n",
            "/content/drive/MyDrive/RAVDESS_song/Actor_14\n",
            "[]\n",
            "/content/drive/MyDrive/RAVDESS_song/Actor_15\n",
            "[]\n",
            "/content/drive/MyDrive/RAVDESS_song/Actor_16\n",
            "[]\n",
            "/content/drive/MyDrive/RAVDESS_song/Actor_17\n",
            "[]\n",
            "/content/drive/MyDrive/RAVDESS_song/Actor_18\n",
            "[]\n",
            "/content/drive/MyDrive/RAVDESS_song/Actor_19\n",
            "[]\n",
            "/content/drive/MyDrive/RAVDESS_song/Actor_21\n",
            "[]\n",
            "/content/drive/MyDrive/RAVDESS_song/Actor_22\n",
            "[]\n",
            "/content/drive/MyDrive/RAVDESS_song/Actor_23\n",
            "[]\n",
            "/content/drive/MyDrive/RAVDESS_song/Actor_09\n",
            "[]\n",
            "/content/drive/MyDrive/RAVDESS_song/Actor_24\n",
            "[]\n",
            "/content/drive/MyDrive/RAVDESS_song/Actor_04\n",
            "[]\n",
            "/content/drive/MyDrive/RAVDESS_song/Actor_08\n",
            "[]\n",
            "/content/drive/MyDrive/RAVDESS_song/Actor_03\n",
            "[]\n",
            "/content/drive/MyDrive/RAVDESS_song/Actor_02\n",
            "[]\n",
            "/content/drive/MyDrive/RAVDESS_song/Actor_06\n",
            "[]\n",
            "/content/drive/MyDrive/RAVDESS_song/Actor_05\n",
            "[]\n",
            "/content/drive/MyDrive/RAVDESS_song/Actor_01\n",
            "[]\n",
            "/content/drive/MyDrive/RAVDESS_song/Actor_07\n",
            "[]\n",
            "/content/drive/MyDrive/RAVDESS_song/Actor_20\n",
            "['Actor_05', 'Actor_08', 'Actor_10', 'Actor_04', 'Actor_09', 'Actor_14', 'Actor_12', 'Actor_13', 'Actor_11', 'Actor_06', 'Actor_24', 'Actor_07', 'Actor_23', 'Actor_22', 'Actor_21', 'Actor_20', 'Actor_02', 'Actor_19', 'Actor_18', 'Actor_17', 'Actor_16', 'Actor_15', 'Actor_03', 'Actor_01']\n",
            "/content/drive/MyDrive/RAVDESS_speech\n",
            "[]\n",
            "/content/drive/MyDrive/RAVDESS_speech/Actor_05\n",
            "[]\n",
            "/content/drive/MyDrive/RAVDESS_speech/Actor_08\n",
            "[]\n",
            "/content/drive/MyDrive/RAVDESS_speech/Actor_10\n",
            "[]\n",
            "/content/drive/MyDrive/RAVDESS_speech/Actor_04\n",
            "[]\n",
            "/content/drive/MyDrive/RAVDESS_speech/Actor_09\n",
            "[]\n",
            "/content/drive/MyDrive/RAVDESS_speech/Actor_14\n",
            "[]\n",
            "/content/drive/MyDrive/RAVDESS_speech/Actor_12\n",
            "[]\n",
            "/content/drive/MyDrive/RAVDESS_speech/Actor_13\n",
            "[]\n",
            "/content/drive/MyDrive/RAVDESS_speech/Actor_11\n",
            "[]\n",
            "/content/drive/MyDrive/RAVDESS_speech/Actor_06\n",
            "[]\n",
            "/content/drive/MyDrive/RAVDESS_speech/Actor_24\n",
            "[]\n",
            "/content/drive/MyDrive/RAVDESS_speech/Actor_07\n",
            "[]\n",
            "/content/drive/MyDrive/RAVDESS_speech/Actor_23\n",
            "[]\n",
            "/content/drive/MyDrive/RAVDESS_speech/Actor_22\n",
            "[]\n",
            "/content/drive/MyDrive/RAVDESS_speech/Actor_21\n",
            "[]\n",
            "/content/drive/MyDrive/RAVDESS_speech/Actor_20\n",
            "[]\n",
            "/content/drive/MyDrive/RAVDESS_speech/Actor_02\n",
            "[]\n",
            "/content/drive/MyDrive/RAVDESS_speech/Actor_19\n",
            "[]\n",
            "/content/drive/MyDrive/RAVDESS_speech/Actor_18\n",
            "[]\n",
            "/content/drive/MyDrive/RAVDESS_speech/Actor_17\n",
            "[]\n",
            "/content/drive/MyDrive/RAVDESS_speech/Actor_16\n",
            "[]\n",
            "/content/drive/MyDrive/RAVDESS_speech/Actor_15\n",
            "[]\n",
            "/content/drive/MyDrive/RAVDESS_speech/Actor_03\n",
            "[]\n",
            "/content/drive/MyDrive/RAVDESS_speech/Actor_01\n",
            "--- Data loaded. Loading time: 216.17487144470215 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count"
      ],
      "metadata": {
        "id": "Ltlhi25L2AI5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f8aa92d-a7fa-4c1d-fd7c-af0b41d47a44"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2068"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.core.fromnumeric import size\n",
        "size(lst)"
      ],
      "metadata": {
        "id": "HMOH7d7mzHB7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f77bc22-8874-4ef2-fd60-c76ea34527fd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3208: VisibleDeprecationWarning:\n",
            "\n",
            "Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4136"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating X and y: zip makes a list of all the first elements, and a list of all the second elements.\n",
        "X, y = zip(*lst)"
      ],
      "metadata": {
        "id": "ocdL7tSTBoMt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "X = np.asarray(X)\n",
        "y = np.asarray(y)\n",
        "X.shape, y.shape"
      ],
      "metadata": {
        "id": "vHKwUGTbBtnF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07415cbb-bb31-4522-e37a-4c844eff8346"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2068, 40), (2068,))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "id": "2sUCtbfMztE5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0428f74f-70c4-490b-bc8e-cd4ff0427775"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2068, 40)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Saving joblib files to not load them again with the loop above\n",
        "\n",
        "# import joblib\n",
        "\n",
        "# X_name = 'x.joblib'\n",
        "# y_name = 'y.joblib'\n",
        "# save_dir = '/content/drive/My Drive/graduation project/audio/paper_code/features'\n",
        "\n",
        "# savedX = joblib.dump(X, os.path.join(save_dir, X_name))\n",
        "# savedy = joblib.dump(y, os.path.join(save_dir, y_name))"
      ],
      "metadata": {
        "id": "UCzic8rlDcuk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Loading saved models\n",
        "# import joblib\n",
        "# X = joblib.load('/content/drive/My Drive/graduation project/audio/paper_code/features/x.joblib')\n",
        "# y = joblib.load('/content/drive/My Drive/graduation project/audio/paper_code/features/y.joblib')"
      ],
      "metadata": {
        "id": "Q35CN6zDrzg1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PSTurzjCo5K",
        "outputId": "63e2387a-a687-45da-d4b2-530ec5dce486"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2068, 40)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2,train_size=0.8, random_state=42)\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(X_test,y_test, test_size=0.5,train_size=0.5, random_state=42)"
      ],
      "metadata": {
        "id": "Ai4Fy5cPCiq1"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "x_traincnn = np.expand_dims(X_train, axis=2)\n",
        "x_testcnn = np.expand_dims(X_test, axis=2)\n",
        "X_valid= np.expand_dims(X_valid, axis=2)"
      ],
      "metadata": {
        "id": "tp1Fm5K3CEXu"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_traincnn.shape, x_testcnn.shape , X_valid.shape"
      ],
      "metadata": {
        "id": "RI0MxoIPBws5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbdb752d-4d66-4dfb-8e2d-cf23b1c18ffd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1654, 40, 1), (207, 40, 1), (207, 40, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "goxL4Y3jKICC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8cfed8a-6da8-4f84-9fbc-a57cc9410808"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.8.2+zzzcolab20220527125636)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (14.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.46.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.26.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.21.6)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (4.2.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.0.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.11.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras"
      ],
      "metadata": {
        "id": "EkcmN9UrKSxj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0179053-5d53-4d97-b4e2-4c4a63e2b014"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.8.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.layers import Input, Flatten, Dropout, Activation\n",
        "from keras.layers import Conv1D, MaxPooling1D\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras import optimizers\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv1D(128, 8,padding='same', #classifier.add(Convolution2D(64, (3, 3), padding = 'same', input_shape = (128, 128, 3), activation = 'relu'))\n",
        "                 input_shape=(40,1)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(MaxPooling1D(pool_size=(5)))\n",
        "\n",
        "\n",
        "model.add(Conv1D(256,8,padding='same',))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(MaxPooling1D(pool_size=(5)))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(256))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(256))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Dense(6))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "opt = tf.keras.optimizers.Adam(lr=0.00002)\n",
        "#opt = tf.keras.optimizers.RMSprop(lr=0.0001, rho=0.9, epsilon=1e-07, decay=0.0)\n",
        "#opt = tf.keras.optimizers.SGD(lr=0.001, momentum=0.0, decay=0.0, nesterov=False)"
      ],
      "metadata": {
        "id": "g74fXWVAC4Cr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4ee119b-c6cc-477f-e80a-00083c67e837"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning:\n",
            "\n",
            "The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6ukOxAGC_I4",
        "outputId": "554bb29b-2fe4-4676-a293-1fd0f634e31e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d (Conv1D)             (None, 40, 128)           1152      \n",
            "                                                                 \n",
            " activation (Activation)     (None, 40, 128)           0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 40, 128)           0         \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, 8, 128)           0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 8, 256)            262400    \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 8, 256)            0         \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 8, 256)            0         \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPooling  (None, 1, 256)           0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               65792     \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 256)               65792     \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 256)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 6)                 1542      \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 6)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 396,678\n",
            "Trainable params: 396,678\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "AbMlLNk4DCBM"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cnnhistory=model.fit(x_traincnn, y_train, batch_size=16, epochs=700, validation_data=(x_testcnn, y_test))\n",
        "cnnhistory=model.fit(x_traincnn, y_train, batch_size=16, epochs=1000 , validation_data=(X_valid, y_valid))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RI1v2AuADFhy",
        "outputId": "f3af6266-467b-46eb-ea1a-ecb24c55a014"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "104/104 [==============================] - 4s 12ms/step - loss: 6.5815 - accuracy: 0.1729 - val_loss: 2.3232 - val_accuracy: 0.1884\n",
            "Epoch 2/1000\n",
            "104/104 [==============================] - 1s 8ms/step - loss: 4.8128 - accuracy: 0.1590 - val_loss: 2.1867 - val_accuracy: 0.1691\n",
            "Epoch 3/1000\n",
            "104/104 [==============================] - 1s 8ms/step - loss: 4.0852 - accuracy: 0.1765 - val_loss: 1.8909 - val_accuracy: 0.2126\n",
            "Epoch 4/1000\n",
            "104/104 [==============================] - 1s 8ms/step - loss: 3.6137 - accuracy: 0.1959 - val_loss: 1.9231 - val_accuracy: 0.2271\n",
            "Epoch 5/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 3.3885 - accuracy: 0.1844 - val_loss: 1.8207 - val_accuracy: 0.2415\n",
            "Epoch 6/1000\n",
            "104/104 [==============================] - 1s 8ms/step - loss: 3.0633 - accuracy: 0.1977 - val_loss: 1.9441 - val_accuracy: 0.1836\n",
            "Epoch 7/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 2.9286 - accuracy: 0.2037 - val_loss: 1.8906 - val_accuracy: 0.2271\n",
            "Epoch 8/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 2.8389 - accuracy: 0.1989 - val_loss: 1.7841 - val_accuracy: 0.1836\n",
            "Epoch 9/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 2.6785 - accuracy: 0.2080 - val_loss: 1.7031 - val_accuracy: 0.2367\n",
            "Epoch 10/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 2.6100 - accuracy: 0.1892 - val_loss: 1.7602 - val_accuracy: 0.2222\n",
            "Epoch 11/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 2.4954 - accuracy: 0.1941 - val_loss: 1.6662 - val_accuracy: 0.3043\n",
            "Epoch 12/1000\n",
            "104/104 [==============================] - 1s 8ms/step - loss: 2.4778 - accuracy: 0.1880 - val_loss: 1.6989 - val_accuracy: 0.2367\n",
            "Epoch 13/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 2.4035 - accuracy: 0.2134 - val_loss: 1.7094 - val_accuracy: 0.1981\n",
            "Epoch 14/1000\n",
            "104/104 [==============================] - 1s 8ms/step - loss: 2.2744 - accuracy: 0.2128 - val_loss: 1.6969 - val_accuracy: 0.2609\n",
            "Epoch 15/1000\n",
            "104/104 [==============================] - 1s 8ms/step - loss: 2.2509 - accuracy: 0.2086 - val_loss: 1.6606 - val_accuracy: 0.2754\n",
            "Epoch 16/1000\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 2.2079 - accuracy: 0.2225 - val_loss: 1.6713 - val_accuracy: 0.2995\n",
            "Epoch 17/1000\n",
            "104/104 [==============================] - 1s 9ms/step - loss: 2.2085 - accuracy: 0.2128 - val_loss: 1.6829 - val_accuracy: 0.2271\n",
            "Epoch 18/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 2.1439 - accuracy: 0.2261 - val_loss: 1.6691 - val_accuracy: 0.2415\n",
            "Epoch 19/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 2.1343 - accuracy: 0.2297 - val_loss: 1.6417 - val_accuracy: 0.3382\n",
            "Epoch 20/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 2.1021 - accuracy: 0.2116 - val_loss: 1.6407 - val_accuracy: 0.3333\n",
            "Epoch 21/1000\n",
            "104/104 [==============================] - 1s 8ms/step - loss: 2.0940 - accuracy: 0.2098 - val_loss: 1.6269 - val_accuracy: 0.3768\n",
            "Epoch 22/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 2.0566 - accuracy: 0.2322 - val_loss: 1.6364 - val_accuracy: 0.3478\n",
            "Epoch 23/1000\n",
            "104/104 [==============================] - 1s 8ms/step - loss: 2.0515 - accuracy: 0.2152 - val_loss: 1.6319 - val_accuracy: 0.3188\n",
            "Epoch 24/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 2.0214 - accuracy: 0.2207 - val_loss: 1.6287 - val_accuracy: 0.2947\n",
            "Epoch 25/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 1.9875 - accuracy: 0.2370 - val_loss: 1.6195 - val_accuracy: 0.3527\n",
            "Epoch 26/1000\n",
            "104/104 [==============================] - 1s 8ms/step - loss: 2.0045 - accuracy: 0.2316 - val_loss: 1.6271 - val_accuracy: 0.3430\n",
            "Epoch 27/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 1.9545 - accuracy: 0.2183 - val_loss: 1.6264 - val_accuracy: 0.3671\n",
            "Epoch 28/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 1.9348 - accuracy: 0.2249 - val_loss: 1.6276 - val_accuracy: 0.3720\n",
            "Epoch 29/1000\n",
            "104/104 [==============================] - 1s 8ms/step - loss: 1.9360 - accuracy: 0.2400 - val_loss: 1.6215 - val_accuracy: 0.3527\n",
            "Epoch 30/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 1.9228 - accuracy: 0.2394 - val_loss: 1.6290 - val_accuracy: 0.3865\n",
            "Epoch 31/1000\n",
            "104/104 [==============================] - 1s 8ms/step - loss: 1.9126 - accuracy: 0.2352 - val_loss: 1.6296 - val_accuracy: 0.3430\n",
            "Epoch 32/1000\n",
            "104/104 [==============================] - 1s 10ms/step - loss: 1.8880 - accuracy: 0.2485 - val_loss: 1.6203 - val_accuracy: 0.3816\n",
            "Epoch 33/1000\n",
            "104/104 [==============================] - 1s 8ms/step - loss: 1.8458 - accuracy: 0.2545 - val_loss: 1.6210 - val_accuracy: 0.3575\n",
            "Epoch 34/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 1.8589 - accuracy: 0.2618 - val_loss: 1.6298 - val_accuracy: 0.2947\n",
            "Epoch 35/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 1.8606 - accuracy: 0.2545 - val_loss: 1.6231 - val_accuracy: 0.3865\n",
            "Epoch 36/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 1.8424 - accuracy: 0.2696 - val_loss: 1.6052 - val_accuracy: 0.4010\n",
            "Epoch 37/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.8096 - accuracy: 0.2775 - val_loss: 1.6037 - val_accuracy: 0.3865\n",
            "Epoch 38/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.8107 - accuracy: 0.2727 - val_loss: 1.6121 - val_accuracy: 0.3623\n",
            "Epoch 39/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.8340 - accuracy: 0.2491 - val_loss: 1.6076 - val_accuracy: 0.3527\n",
            "Epoch 40/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.8221 - accuracy: 0.2461 - val_loss: 1.6112 - val_accuracy: 0.3623\n",
            "Epoch 41/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.7864 - accuracy: 0.2527 - val_loss: 1.6162 - val_accuracy: 0.3527\n",
            "Epoch 42/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.8088 - accuracy: 0.2594 - val_loss: 1.5824 - val_accuracy: 0.3961\n",
            "Epoch 43/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.7959 - accuracy: 0.2636 - val_loss: 1.6196 - val_accuracy: 0.3092\n",
            "Epoch 44/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.7639 - accuracy: 0.2739 - val_loss: 1.6333 - val_accuracy: 0.3382\n",
            "Epoch 45/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.7739 - accuracy: 0.2696 - val_loss: 1.5909 - val_accuracy: 0.3768\n",
            "Epoch 46/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.7718 - accuracy: 0.2751 - val_loss: 1.5999 - val_accuracy: 0.4010\n",
            "Epoch 47/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.7259 - accuracy: 0.2823 - val_loss: 1.5962 - val_accuracy: 0.3478\n",
            "Epoch 48/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.7677 - accuracy: 0.2745 - val_loss: 1.5887 - val_accuracy: 0.3382\n",
            "Epoch 49/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.7131 - accuracy: 0.2963 - val_loss: 1.5999 - val_accuracy: 0.4203\n",
            "Epoch 50/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.7300 - accuracy: 0.2866 - val_loss: 1.5770 - val_accuracy: 0.3768\n",
            "Epoch 51/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.7331 - accuracy: 0.2842 - val_loss: 1.5731 - val_accuracy: 0.3865\n",
            "Epoch 52/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.6958 - accuracy: 0.3011 - val_loss: 1.5731 - val_accuracy: 0.3720\n",
            "Epoch 53/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.7268 - accuracy: 0.2956 - val_loss: 1.5560 - val_accuracy: 0.4396\n",
            "Epoch 54/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.6844 - accuracy: 0.3053 - val_loss: 1.5748 - val_accuracy: 0.4010\n",
            "Epoch 55/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.6816 - accuracy: 0.3089 - val_loss: 1.5959 - val_accuracy: 0.3623\n",
            "Epoch 56/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.6800 - accuracy: 0.2938 - val_loss: 1.5411 - val_accuracy: 0.4300\n",
            "Epoch 57/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.6885 - accuracy: 0.3126 - val_loss: 1.5513 - val_accuracy: 0.4155\n",
            "Epoch 58/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.6801 - accuracy: 0.2987 - val_loss: 1.5453 - val_accuracy: 0.3961\n",
            "Epoch 59/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.6715 - accuracy: 0.3120 - val_loss: 1.5415 - val_accuracy: 0.3575\n",
            "Epoch 60/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.6903 - accuracy: 0.3053 - val_loss: 1.5667 - val_accuracy: 0.4058\n",
            "Epoch 61/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.6550 - accuracy: 0.3343 - val_loss: 1.5497 - val_accuracy: 0.4444\n",
            "Epoch 62/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.6704 - accuracy: 0.3071 - val_loss: 1.5326 - val_accuracy: 0.4444\n",
            "Epoch 63/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.6516 - accuracy: 0.3077 - val_loss: 1.5332 - val_accuracy: 0.4541\n",
            "Epoch 64/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.6441 - accuracy: 0.3210 - val_loss: 1.5339 - val_accuracy: 0.4010\n",
            "Epoch 65/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.6400 - accuracy: 0.3132 - val_loss: 1.5487 - val_accuracy: 0.3623\n",
            "Epoch 66/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.6125 - accuracy: 0.3410 - val_loss: 1.5542 - val_accuracy: 0.4300\n",
            "Epoch 67/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.6369 - accuracy: 0.3241 - val_loss: 1.5471 - val_accuracy: 0.3671\n",
            "Epoch 68/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.6147 - accuracy: 0.3343 - val_loss: 1.5387 - val_accuracy: 0.4203\n",
            "Epoch 69/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.6080 - accuracy: 0.3325 - val_loss: 1.5133 - val_accuracy: 0.4203\n",
            "Epoch 70/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.6392 - accuracy: 0.3065 - val_loss: 1.5413 - val_accuracy: 0.4203\n",
            "Epoch 71/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.6186 - accuracy: 0.3289 - val_loss: 1.5056 - val_accuracy: 0.4589\n",
            "Epoch 72/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.5747 - accuracy: 0.3482 - val_loss: 1.5104 - val_accuracy: 0.4734\n",
            "Epoch 73/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.6066 - accuracy: 0.3356 - val_loss: 1.5028 - val_accuracy: 0.4493\n",
            "Epoch 74/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.6038 - accuracy: 0.3404 - val_loss: 1.4974 - val_accuracy: 0.4686\n",
            "Epoch 75/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.6096 - accuracy: 0.3307 - val_loss: 1.4998 - val_accuracy: 0.4638\n",
            "Epoch 76/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.5668 - accuracy: 0.3458 - val_loss: 1.4985 - val_accuracy: 0.4589\n",
            "Epoch 77/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.5927 - accuracy: 0.3501 - val_loss: 1.4769 - val_accuracy: 0.4493\n",
            "Epoch 78/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.5753 - accuracy: 0.3537 - val_loss: 1.4961 - val_accuracy: 0.4734\n",
            "Epoch 79/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.5790 - accuracy: 0.3386 - val_loss: 1.4948 - val_accuracy: 0.4348\n",
            "Epoch 80/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.5605 - accuracy: 0.3730 - val_loss: 1.4810 - val_accuracy: 0.4444\n",
            "Epoch 81/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.5760 - accuracy: 0.3549 - val_loss: 1.4710 - val_accuracy: 0.4686\n",
            "Epoch 82/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.5502 - accuracy: 0.3748 - val_loss: 1.4783 - val_accuracy: 0.4831\n",
            "Epoch 83/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.5343 - accuracy: 0.3670 - val_loss: 1.4699 - val_accuracy: 0.4444\n",
            "Epoch 84/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.5426 - accuracy: 0.3585 - val_loss: 1.4950 - val_accuracy: 0.4300\n",
            "Epoch 85/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.5411 - accuracy: 0.3658 - val_loss: 1.4707 - val_accuracy: 0.4541\n",
            "Epoch 86/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.5450 - accuracy: 0.3622 - val_loss: 1.4536 - val_accuracy: 0.5024\n",
            "Epoch 87/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.5381 - accuracy: 0.3809 - val_loss: 1.4490 - val_accuracy: 0.4638\n",
            "Epoch 88/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.5186 - accuracy: 0.3821 - val_loss: 1.4635 - val_accuracy: 0.4638\n",
            "Epoch 89/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.5097 - accuracy: 0.3688 - val_loss: 1.4614 - val_accuracy: 0.4686\n",
            "Epoch 90/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.5050 - accuracy: 0.3942 - val_loss: 1.4497 - val_accuracy: 0.4444\n",
            "Epoch 91/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.4901 - accuracy: 0.4045 - val_loss: 1.4381 - val_accuracy: 0.4300\n",
            "Epoch 92/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.5184 - accuracy: 0.3936 - val_loss: 1.4572 - val_accuracy: 0.4155\n",
            "Epoch 93/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.5111 - accuracy: 0.3767 - val_loss: 1.4410 - val_accuracy: 0.4686\n",
            "Epoch 94/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.5055 - accuracy: 0.3869 - val_loss: 1.4376 - val_accuracy: 0.4541\n",
            "Epoch 95/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.4666 - accuracy: 0.4039 - val_loss: 1.4232 - val_accuracy: 0.4686\n",
            "Epoch 96/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.4722 - accuracy: 0.4135 - val_loss: 1.4234 - val_accuracy: 0.5072\n",
            "Epoch 97/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.4783 - accuracy: 0.4099 - val_loss: 1.4365 - val_accuracy: 0.4783\n",
            "Epoch 98/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.4633 - accuracy: 0.3978 - val_loss: 1.4248 - val_accuracy: 0.4396\n",
            "Epoch 99/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.4776 - accuracy: 0.4002 - val_loss: 1.4079 - val_accuracy: 0.4831\n",
            "Epoch 100/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.4766 - accuracy: 0.3894 - val_loss: 1.4025 - val_accuracy: 0.4734\n",
            "Epoch 101/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.4566 - accuracy: 0.3966 - val_loss: 1.4012 - val_accuracy: 0.4879\n",
            "Epoch 102/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.4509 - accuracy: 0.4057 - val_loss: 1.3838 - val_accuracy: 0.5266\n",
            "Epoch 103/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.4508 - accuracy: 0.4008 - val_loss: 1.3856 - val_accuracy: 0.5169\n",
            "Epoch 104/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.4541 - accuracy: 0.4117 - val_loss: 1.3860 - val_accuracy: 0.4783\n",
            "Epoch 105/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.4241 - accuracy: 0.4075 - val_loss: 1.4007 - val_accuracy: 0.4783\n",
            "Epoch 106/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.4348 - accuracy: 0.4148 - val_loss: 1.3738 - val_accuracy: 0.5072\n",
            "Epoch 107/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.4256 - accuracy: 0.4317 - val_loss: 1.3946 - val_accuracy: 0.4589\n",
            "Epoch 108/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.4470 - accuracy: 0.4214 - val_loss: 1.3932 - val_accuracy: 0.4589\n",
            "Epoch 109/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.4209 - accuracy: 0.4281 - val_loss: 1.3834 - val_accuracy: 0.4638\n",
            "Epoch 110/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.4042 - accuracy: 0.4438 - val_loss: 1.3763 - val_accuracy: 0.4831\n",
            "Epoch 111/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.4044 - accuracy: 0.4232 - val_loss: 1.3619 - val_accuracy: 0.4783\n",
            "Epoch 112/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.4210 - accuracy: 0.4220 - val_loss: 1.3504 - val_accuracy: 0.5024\n",
            "Epoch 113/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.4065 - accuracy: 0.4353 - val_loss: 1.3503 - val_accuracy: 0.5169\n",
            "Epoch 114/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.4062 - accuracy: 0.4335 - val_loss: 1.3402 - val_accuracy: 0.5072\n",
            "Epoch 115/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.3960 - accuracy: 0.4299 - val_loss: 1.3477 - val_accuracy: 0.5362\n",
            "Epoch 116/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.3675 - accuracy: 0.4534 - val_loss: 1.3249 - val_accuracy: 0.5072\n",
            "Epoch 117/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.3863 - accuracy: 0.4287 - val_loss: 1.3317 - val_accuracy: 0.5121\n",
            "Epoch 118/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.3950 - accuracy: 0.4407 - val_loss: 1.3635 - val_accuracy: 0.4976\n",
            "Epoch 119/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.3628 - accuracy: 0.4534 - val_loss: 1.3217 - val_accuracy: 0.5314\n",
            "Epoch 120/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.3777 - accuracy: 0.4341 - val_loss: 1.3131 - val_accuracy: 0.5556\n",
            "Epoch 121/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.3719 - accuracy: 0.4516 - val_loss: 1.3117 - val_accuracy: 0.5411\n",
            "Epoch 122/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.3590 - accuracy: 0.4601 - val_loss: 1.3153 - val_accuracy: 0.4879\n",
            "Epoch 123/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.3907 - accuracy: 0.4371 - val_loss: 1.3105 - val_accuracy: 0.5266\n",
            "Epoch 124/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.3477 - accuracy: 0.4553 - val_loss: 1.3038 - val_accuracy: 0.5459\n",
            "Epoch 125/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.3866 - accuracy: 0.4444 - val_loss: 1.2972 - val_accuracy: 0.5507\n",
            "Epoch 126/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.3347 - accuracy: 0.4601 - val_loss: 1.2929 - val_accuracy: 0.5700\n",
            "Epoch 127/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.3541 - accuracy: 0.4583 - val_loss: 1.2786 - val_accuracy: 0.5459\n",
            "Epoch 128/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.3434 - accuracy: 0.4577 - val_loss: 1.2728 - val_accuracy: 0.5556\n",
            "Epoch 129/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.3246 - accuracy: 0.4661 - val_loss: 1.2680 - val_accuracy: 0.5749\n",
            "Epoch 130/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.3257 - accuracy: 0.4704 - val_loss: 1.2543 - val_accuracy: 0.5652\n",
            "Epoch 131/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.3481 - accuracy: 0.4571 - val_loss: 1.2818 - val_accuracy: 0.5362\n",
            "Epoch 132/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.3610 - accuracy: 0.4456 - val_loss: 1.2748 - val_accuracy: 0.5266\n",
            "Epoch 133/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.3232 - accuracy: 0.4631 - val_loss: 1.2644 - val_accuracy: 0.5556\n",
            "Epoch 134/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.3276 - accuracy: 0.4498 - val_loss: 1.2696 - val_accuracy: 0.5266\n",
            "Epoch 135/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.3231 - accuracy: 0.4655 - val_loss: 1.2505 - val_accuracy: 0.5652\n",
            "Epoch 136/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.3252 - accuracy: 0.4710 - val_loss: 1.2473 - val_accuracy: 0.5652\n",
            "Epoch 137/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.3031 - accuracy: 0.4655 - val_loss: 1.2390 - val_accuracy: 0.5556\n",
            "Epoch 138/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.3072 - accuracy: 0.4819 - val_loss: 1.2387 - val_accuracy: 0.5652\n",
            "Epoch 139/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.3129 - accuracy: 0.4770 - val_loss: 1.2659 - val_accuracy: 0.5217\n",
            "Epoch 140/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.2853 - accuracy: 0.4837 - val_loss: 1.2338 - val_accuracy: 0.5507\n",
            "Epoch 141/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.3058 - accuracy: 0.4752 - val_loss: 1.2260 - val_accuracy: 0.5652\n",
            "Epoch 142/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.2895 - accuracy: 0.4837 - val_loss: 1.2344 - val_accuracy: 0.5556\n",
            "Epoch 143/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.2811 - accuracy: 0.4758 - val_loss: 1.2395 - val_accuracy: 0.5556\n",
            "Epoch 144/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.2725 - accuracy: 0.4867 - val_loss: 1.2181 - val_accuracy: 0.5604\n",
            "Epoch 145/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.2481 - accuracy: 0.5206 - val_loss: 1.2058 - val_accuracy: 0.5749\n",
            "Epoch 146/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.2816 - accuracy: 0.4728 - val_loss: 1.2112 - val_accuracy: 0.5749\n",
            "Epoch 147/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.3095 - accuracy: 0.4595 - val_loss: 1.2015 - val_accuracy: 0.5749\n",
            "Epoch 148/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.2793 - accuracy: 0.4958 - val_loss: 1.2036 - val_accuracy: 0.5700\n",
            "Epoch 149/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.2668 - accuracy: 0.4958 - val_loss: 1.2177 - val_accuracy: 0.5556\n",
            "Epoch 150/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.2585 - accuracy: 0.4946 - val_loss: 1.2056 - val_accuracy: 0.5411\n",
            "Epoch 151/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.2544 - accuracy: 0.5000 - val_loss: 1.1910 - val_accuracy: 0.5700\n",
            "Epoch 152/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.2334 - accuracy: 0.5109 - val_loss: 1.1995 - val_accuracy: 0.5700\n",
            "Epoch 153/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.2501 - accuracy: 0.4909 - val_loss: 1.1824 - val_accuracy: 0.5894\n",
            "Epoch 154/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.2510 - accuracy: 0.4927 - val_loss: 1.1928 - val_accuracy: 0.5556\n",
            "Epoch 155/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.2489 - accuracy: 0.4976 - val_loss: 1.1885 - val_accuracy: 0.5797\n",
            "Epoch 156/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.2289 - accuracy: 0.5012 - val_loss: 1.1831 - val_accuracy: 0.5797\n",
            "Epoch 157/1000\n",
            "104/104 [==============================] - 1s 8ms/step - loss: 1.2539 - accuracy: 0.5085 - val_loss: 1.1920 - val_accuracy: 0.5556\n",
            "Epoch 158/1000\n",
            "104/104 [==============================] - 1s 8ms/step - loss: 1.2534 - accuracy: 0.4879 - val_loss: 1.1741 - val_accuracy: 0.5845\n",
            "Epoch 159/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 1.2264 - accuracy: 0.5042 - val_loss: 1.1698 - val_accuracy: 0.5894\n",
            "Epoch 160/1000\n",
            "104/104 [==============================] - 1s 8ms/step - loss: 1.2258 - accuracy: 0.5284 - val_loss: 1.1704 - val_accuracy: 0.5894\n",
            "Epoch 161/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 1.2365 - accuracy: 0.5042 - val_loss: 1.1611 - val_accuracy: 0.5990\n",
            "Epoch 162/1000\n",
            "104/104 [==============================] - 1s 9ms/step - loss: 1.2412 - accuracy: 0.4952 - val_loss: 1.1570 - val_accuracy: 0.5845\n",
            "Epoch 163/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.2161 - accuracy: 0.5115 - val_loss: 1.1675 - val_accuracy: 0.5700\n",
            "Epoch 164/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.2191 - accuracy: 0.5302 - val_loss: 1.1661 - val_accuracy: 0.5556\n",
            "Epoch 165/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.1897 - accuracy: 0.5218 - val_loss: 1.1452 - val_accuracy: 0.5604\n",
            "Epoch 166/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.1935 - accuracy: 0.5302 - val_loss: 1.1495 - val_accuracy: 0.5652\n",
            "Epoch 167/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.2065 - accuracy: 0.5121 - val_loss: 1.1498 - val_accuracy: 0.5749\n",
            "Epoch 168/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.2145 - accuracy: 0.5169 - val_loss: 1.1541 - val_accuracy: 0.5797\n",
            "Epoch 169/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.1986 - accuracy: 0.5151 - val_loss: 1.1587 - val_accuracy: 0.5845\n",
            "Epoch 170/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.1937 - accuracy: 0.5206 - val_loss: 1.1427 - val_accuracy: 0.6039\n",
            "Epoch 171/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.1828 - accuracy: 0.5435 - val_loss: 1.1593 - val_accuracy: 0.5894\n",
            "Epoch 172/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.1972 - accuracy: 0.5175 - val_loss: 1.1358 - val_accuracy: 0.5894\n",
            "Epoch 173/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.1794 - accuracy: 0.5375 - val_loss: 1.1311 - val_accuracy: 0.5845\n",
            "Epoch 174/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.1780 - accuracy: 0.5254 - val_loss: 1.1205 - val_accuracy: 0.5942\n",
            "Epoch 175/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.1739 - accuracy: 0.5369 - val_loss: 1.1244 - val_accuracy: 0.5894\n",
            "Epoch 176/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.1645 - accuracy: 0.5399 - val_loss: 1.1152 - val_accuracy: 0.6280\n",
            "Epoch 177/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.1540 - accuracy: 0.5435 - val_loss: 1.1231 - val_accuracy: 0.5797\n",
            "Epoch 178/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.1559 - accuracy: 0.5302 - val_loss: 1.1067 - val_accuracy: 0.6232\n",
            "Epoch 179/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.1819 - accuracy: 0.5224 - val_loss: 1.1133 - val_accuracy: 0.6039\n",
            "Epoch 180/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.1703 - accuracy: 0.5435 - val_loss: 1.1003 - val_accuracy: 0.6135\n",
            "Epoch 181/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.1476 - accuracy: 0.5502 - val_loss: 1.1111 - val_accuracy: 0.6184\n",
            "Epoch 182/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.1478 - accuracy: 0.5490 - val_loss: 1.1126 - val_accuracy: 0.5700\n",
            "Epoch 183/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.1676 - accuracy: 0.5375 - val_loss: 1.0929 - val_accuracy: 0.6135\n",
            "Epoch 184/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.1653 - accuracy: 0.5399 - val_loss: 1.1131 - val_accuracy: 0.5894\n",
            "Epoch 185/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.1368 - accuracy: 0.5586 - val_loss: 1.0968 - val_accuracy: 0.6039\n",
            "Epoch 186/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.1426 - accuracy: 0.5453 - val_loss: 1.0835 - val_accuracy: 0.6232\n",
            "Epoch 187/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.1372 - accuracy: 0.5484 - val_loss: 1.0877 - val_accuracy: 0.6135\n",
            "Epoch 188/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.1392 - accuracy: 0.5453 - val_loss: 1.0817 - val_accuracy: 0.5990\n",
            "Epoch 189/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.1220 - accuracy: 0.5677 - val_loss: 1.0858 - val_accuracy: 0.6184\n",
            "Epoch 190/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.1341 - accuracy: 0.5605 - val_loss: 1.0832 - val_accuracy: 0.6232\n",
            "Epoch 191/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.1155 - accuracy: 0.5586 - val_loss: 1.0776 - val_accuracy: 0.6377\n",
            "Epoch 192/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.1147 - accuracy: 0.5502 - val_loss: 1.0959 - val_accuracy: 0.5990\n",
            "Epoch 193/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.1209 - accuracy: 0.5677 - val_loss: 1.0819 - val_accuracy: 0.6329\n",
            "Epoch 194/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.1263 - accuracy: 0.5562 - val_loss: 1.0730 - val_accuracy: 0.6522\n",
            "Epoch 195/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.1148 - accuracy: 0.5689 - val_loss: 1.0740 - val_accuracy: 0.6184\n",
            "Epoch 196/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.1064 - accuracy: 0.5713 - val_loss: 1.0786 - val_accuracy: 0.6135\n",
            "Epoch 197/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.1293 - accuracy: 0.5453 - val_loss: 1.0699 - val_accuracy: 0.6280\n",
            "Epoch 198/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.1092 - accuracy: 0.5605 - val_loss: 1.0632 - val_accuracy: 0.6570\n",
            "Epoch 199/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.1232 - accuracy: 0.5623 - val_loss: 1.0596 - val_accuracy: 0.6425\n",
            "Epoch 200/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.1015 - accuracy: 0.5629 - val_loss: 1.0546 - val_accuracy: 0.6329\n",
            "Epoch 201/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.1186 - accuracy: 0.5532 - val_loss: 1.0626 - val_accuracy: 0.6329\n",
            "Epoch 202/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0963 - accuracy: 0.5635 - val_loss: 1.0512 - val_accuracy: 0.6184\n",
            "Epoch 203/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.1015 - accuracy: 0.5593 - val_loss: 1.0575 - val_accuracy: 0.6135\n",
            "Epoch 204/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0861 - accuracy: 0.5750 - val_loss: 1.0491 - val_accuracy: 0.6425\n",
            "Epoch 205/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.0949 - accuracy: 0.5659 - val_loss: 1.0556 - val_accuracy: 0.6425\n",
            "Epoch 206/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.0732 - accuracy: 0.5774 - val_loss: 1.0618 - val_accuracy: 0.6280\n",
            "Epoch 207/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.0924 - accuracy: 0.5713 - val_loss: 1.0581 - val_accuracy: 0.6184\n",
            "Epoch 208/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.0804 - accuracy: 0.5695 - val_loss: 1.0443 - val_accuracy: 0.6280\n",
            "Epoch 209/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.0883 - accuracy: 0.5756 - val_loss: 1.0515 - val_accuracy: 0.6377\n",
            "Epoch 210/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.0906 - accuracy: 0.5629 - val_loss: 1.0544 - val_accuracy: 0.6473\n",
            "Epoch 211/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.0859 - accuracy: 0.5629 - val_loss: 1.0520 - val_accuracy: 0.6377\n",
            "Epoch 212/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.0801 - accuracy: 0.5798 - val_loss: 1.0451 - val_accuracy: 0.6280\n",
            "Epoch 213/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.0463 - accuracy: 0.5895 - val_loss: 1.0277 - val_accuracy: 0.6377\n",
            "Epoch 214/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.0683 - accuracy: 0.5593 - val_loss: 1.0228 - val_accuracy: 0.6522\n",
            "Epoch 215/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.0741 - accuracy: 0.5762 - val_loss: 1.0313 - val_accuracy: 0.6377\n",
            "Epoch 216/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.0527 - accuracy: 0.5707 - val_loss: 1.0333 - val_accuracy: 0.6232\n",
            "Epoch 217/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0773 - accuracy: 0.5865 - val_loss: 1.0136 - val_accuracy: 0.6377\n",
            "Epoch 218/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0638 - accuracy: 0.5786 - val_loss: 1.0143 - val_accuracy: 0.6377\n",
            "Epoch 219/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0449 - accuracy: 0.5865 - val_loss: 1.0258 - val_accuracy: 0.6232\n",
            "Epoch 220/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0720 - accuracy: 0.5738 - val_loss: 1.0120 - val_accuracy: 0.6329\n",
            "Epoch 221/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.0579 - accuracy: 0.5883 - val_loss: 1.0362 - val_accuracy: 0.6473\n",
            "Epoch 222/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.0608 - accuracy: 0.5780 - val_loss: 1.0189 - val_accuracy: 0.6280\n",
            "Epoch 223/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.0404 - accuracy: 0.5949 - val_loss: 1.0120 - val_accuracy: 0.6329\n",
            "Epoch 224/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.0689 - accuracy: 0.5635 - val_loss: 1.0224 - val_accuracy: 0.6184\n",
            "Epoch 225/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0484 - accuracy: 0.5852 - val_loss: 1.0047 - val_accuracy: 0.6570\n",
            "Epoch 226/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.0314 - accuracy: 0.5919 - val_loss: 1.0400 - val_accuracy: 0.6135\n",
            "Epoch 227/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0560 - accuracy: 0.5792 - val_loss: 1.0170 - val_accuracy: 0.6425\n",
            "Epoch 228/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.0166 - accuracy: 0.6010 - val_loss: 1.0192 - val_accuracy: 0.6135\n",
            "Epoch 229/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.0452 - accuracy: 0.5810 - val_loss: 0.9951 - val_accuracy: 0.6522\n",
            "Epoch 230/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0180 - accuracy: 0.6064 - val_loss: 0.9839 - val_accuracy: 0.6425\n",
            "Epoch 231/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.0072 - accuracy: 0.6052 - val_loss: 1.0053 - val_accuracy: 0.6522\n",
            "Epoch 232/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.0124 - accuracy: 0.6119 - val_loss: 1.0074 - val_accuracy: 0.6280\n",
            "Epoch 233/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.0341 - accuracy: 0.6034 - val_loss: 1.0046 - val_accuracy: 0.6329\n",
            "Epoch 234/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.0133 - accuracy: 0.6058 - val_loss: 1.0094 - val_accuracy: 0.6329\n",
            "Epoch 235/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0171 - accuracy: 0.6034 - val_loss: 1.0286 - val_accuracy: 0.6039\n",
            "Epoch 236/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.0321 - accuracy: 0.5852 - val_loss: 0.9908 - val_accuracy: 0.6570\n",
            "Epoch 237/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.0325 - accuracy: 0.5961 - val_loss: 0.9843 - val_accuracy: 0.6715\n",
            "Epoch 238/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.0043 - accuracy: 0.6070 - val_loss: 1.0000 - val_accuracy: 0.6329\n",
            "Epoch 239/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 1.0161 - accuracy: 0.5840 - val_loss: 0.9893 - val_accuracy: 0.6473\n",
            "Epoch 240/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 1.0172 - accuracy: 0.6112 - val_loss: 1.0126 - val_accuracy: 0.6377\n",
            "Epoch 241/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.9962 - accuracy: 0.6161 - val_loss: 0.9813 - val_accuracy: 0.6184\n",
            "Epoch 242/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.0190 - accuracy: 0.6022 - val_loss: 1.0014 - val_accuracy: 0.6377\n",
            "Epoch 243/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9998 - accuracy: 0.6022 - val_loss: 0.9893 - val_accuracy: 0.6329\n",
            "Epoch 244/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.9987 - accuracy: 0.6064 - val_loss: 0.9953 - val_accuracy: 0.6280\n",
            "Epoch 245/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 1.0200 - accuracy: 0.5828 - val_loss: 0.9904 - val_accuracy: 0.6667\n",
            "Epoch 246/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.9918 - accuracy: 0.6076 - val_loss: 0.9804 - val_accuracy: 0.6522\n",
            "Epoch 247/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.9986 - accuracy: 0.6106 - val_loss: 0.9823 - val_accuracy: 0.6473\n",
            "Epoch 248/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.9802 - accuracy: 0.6155 - val_loss: 0.9778 - val_accuracy: 0.6667\n",
            "Epoch 249/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.9921 - accuracy: 0.6185 - val_loss: 0.9855 - val_accuracy: 0.6618\n",
            "Epoch 250/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.9993 - accuracy: 0.5973 - val_loss: 0.9703 - val_accuracy: 0.6667\n",
            "Epoch 251/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.9943 - accuracy: 0.6106 - val_loss: 0.9734 - val_accuracy: 0.6763\n",
            "Epoch 252/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.9919 - accuracy: 0.6016 - val_loss: 0.9597 - val_accuracy: 0.6908\n",
            "Epoch 253/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.9975 - accuracy: 0.6058 - val_loss: 0.9542 - val_accuracy: 0.6522\n",
            "Epoch 254/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9947 - accuracy: 0.6112 - val_loss: 0.9677 - val_accuracy: 0.6618\n",
            "Epoch 255/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.9682 - accuracy: 0.6215 - val_loss: 0.9436 - val_accuracy: 0.6667\n",
            "Epoch 256/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.9868 - accuracy: 0.6209 - val_loss: 0.9663 - val_accuracy: 0.6473\n",
            "Epoch 257/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9850 - accuracy: 0.6185 - val_loss: 0.9727 - val_accuracy: 0.6184\n",
            "Epoch 258/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9657 - accuracy: 0.6161 - val_loss: 0.9424 - val_accuracy: 0.6860\n",
            "Epoch 259/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.9638 - accuracy: 0.6179 - val_loss: 0.9569 - val_accuracy: 0.6763\n",
            "Epoch 260/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9738 - accuracy: 0.6221 - val_loss: 0.9520 - val_accuracy: 0.6522\n",
            "Epoch 261/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.9635 - accuracy: 0.6209 - val_loss: 0.9544 - val_accuracy: 0.6763\n",
            "Epoch 262/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9427 - accuracy: 0.6342 - val_loss: 0.9648 - val_accuracy: 0.6763\n",
            "Epoch 263/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.9552 - accuracy: 0.6312 - val_loss: 0.9620 - val_accuracy: 0.6570\n",
            "Epoch 264/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9801 - accuracy: 0.6070 - val_loss: 0.9465 - val_accuracy: 0.6860\n",
            "Epoch 265/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.9874 - accuracy: 0.6058 - val_loss: 0.9538 - val_accuracy: 0.6812\n",
            "Epoch 266/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.9447 - accuracy: 0.6233 - val_loss: 0.9621 - val_accuracy: 0.6667\n",
            "Epoch 267/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.9503 - accuracy: 0.6294 - val_loss: 0.9477 - val_accuracy: 0.6812\n",
            "Epoch 268/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.9601 - accuracy: 0.6288 - val_loss: 0.9542 - val_accuracy: 0.6232\n",
            "Epoch 269/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.9449 - accuracy: 0.6264 - val_loss: 0.9465 - val_accuracy: 0.6667\n",
            "Epoch 270/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9580 - accuracy: 0.6306 - val_loss: 0.9507 - val_accuracy: 0.6763\n",
            "Epoch 271/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9640 - accuracy: 0.6100 - val_loss: 0.9420 - val_accuracy: 0.6667\n",
            "Epoch 272/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.9612 - accuracy: 0.6179 - val_loss: 0.9491 - val_accuracy: 0.6570\n",
            "Epoch 273/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9329 - accuracy: 0.6318 - val_loss: 0.9451 - val_accuracy: 0.6667\n",
            "Epoch 274/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9302 - accuracy: 0.6239 - val_loss: 0.9346 - val_accuracy: 0.6425\n",
            "Epoch 275/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.9488 - accuracy: 0.6131 - val_loss: 0.9399 - val_accuracy: 0.6618\n",
            "Epoch 276/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9254 - accuracy: 0.6330 - val_loss: 0.9193 - val_accuracy: 0.6908\n",
            "Epoch 277/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.9330 - accuracy: 0.6245 - val_loss: 0.9335 - val_accuracy: 0.6860\n",
            "Epoch 278/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9472 - accuracy: 0.6221 - val_loss: 0.9229 - val_accuracy: 0.6957\n",
            "Epoch 279/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9390 - accuracy: 0.6330 - val_loss: 0.9342 - val_accuracy: 0.6570\n",
            "Epoch 280/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.9292 - accuracy: 0.6385 - val_loss: 0.9245 - val_accuracy: 0.6522\n",
            "Epoch 281/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8935 - accuracy: 0.6445 - val_loss: 0.9377 - val_accuracy: 0.6425\n",
            "Epoch 282/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.9350 - accuracy: 0.6288 - val_loss: 0.9351 - val_accuracy: 0.6715\n",
            "Epoch 283/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9131 - accuracy: 0.6518 - val_loss: 0.9181 - val_accuracy: 0.6618\n",
            "Epoch 284/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.9366 - accuracy: 0.6221 - val_loss: 0.9362 - val_accuracy: 0.6570\n",
            "Epoch 285/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9281 - accuracy: 0.6415 - val_loss: 0.9262 - val_accuracy: 0.6667\n",
            "Epoch 286/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.9277 - accuracy: 0.6191 - val_loss: 0.9242 - val_accuracy: 0.6618\n",
            "Epoch 287/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.9332 - accuracy: 0.6348 - val_loss: 0.9361 - val_accuracy: 0.6522\n",
            "Epoch 288/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.9019 - accuracy: 0.6463 - val_loss: 0.9239 - val_accuracy: 0.6715\n",
            "Epoch 289/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.9145 - accuracy: 0.6252 - val_loss: 0.9408 - val_accuracy: 0.6377\n",
            "Epoch 290/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.9015 - accuracy: 0.6354 - val_loss: 0.9259 - val_accuracy: 0.6570\n",
            "Epoch 291/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8868 - accuracy: 0.6524 - val_loss: 0.9257 - val_accuracy: 0.6473\n",
            "Epoch 292/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9007 - accuracy: 0.6439 - val_loss: 0.9169 - val_accuracy: 0.6570\n",
            "Epoch 293/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8902 - accuracy: 0.6536 - val_loss: 0.9370 - val_accuracy: 0.6522\n",
            "Epoch 294/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8896 - accuracy: 0.6511 - val_loss: 0.9197 - val_accuracy: 0.6473\n",
            "Epoch 295/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9138 - accuracy: 0.6312 - val_loss: 0.9134 - val_accuracy: 0.6667\n",
            "Epoch 296/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.9079 - accuracy: 0.6439 - val_loss: 0.9032 - val_accuracy: 0.6570\n",
            "Epoch 297/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.8926 - accuracy: 0.6524 - val_loss: 0.9115 - val_accuracy: 0.6860\n",
            "Epoch 298/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8989 - accuracy: 0.6439 - val_loss: 0.9222 - val_accuracy: 0.6522\n",
            "Epoch 299/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.9174 - accuracy: 0.6258 - val_loss: 0.9274 - val_accuracy: 0.6425\n",
            "Epoch 300/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8946 - accuracy: 0.6511 - val_loss: 0.9086 - val_accuracy: 0.6715\n",
            "Epoch 301/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8848 - accuracy: 0.6469 - val_loss: 0.9154 - val_accuracy: 0.6618\n",
            "Epoch 302/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8923 - accuracy: 0.6542 - val_loss: 0.9024 - val_accuracy: 0.6715\n",
            "Epoch 303/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8855 - accuracy: 0.6433 - val_loss: 0.9191 - val_accuracy: 0.6570\n",
            "Epoch 304/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8836 - accuracy: 0.6632 - val_loss: 0.9126 - val_accuracy: 0.6667\n",
            "Epoch 305/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8886 - accuracy: 0.6421 - val_loss: 0.9267 - val_accuracy: 0.6425\n",
            "Epoch 306/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8846 - accuracy: 0.6566 - val_loss: 0.8938 - val_accuracy: 0.6957\n",
            "Epoch 307/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8776 - accuracy: 0.6735 - val_loss: 0.8989 - val_accuracy: 0.6860\n",
            "Epoch 308/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8957 - accuracy: 0.6445 - val_loss: 0.9026 - val_accuracy: 0.6473\n",
            "Epoch 309/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8757 - accuracy: 0.6602 - val_loss: 0.9044 - val_accuracy: 0.6618\n",
            "Epoch 310/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8777 - accuracy: 0.6536 - val_loss: 0.8876 - val_accuracy: 0.6812\n",
            "Epoch 311/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8693 - accuracy: 0.6644 - val_loss: 0.9083 - val_accuracy: 0.6618\n",
            "Epoch 312/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8717 - accuracy: 0.6699 - val_loss: 0.9025 - val_accuracy: 0.6570\n",
            "Epoch 313/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8664 - accuracy: 0.6693 - val_loss: 0.9040 - val_accuracy: 0.6715\n",
            "Epoch 314/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8694 - accuracy: 0.6584 - val_loss: 0.8940 - val_accuracy: 0.6860\n",
            "Epoch 315/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8596 - accuracy: 0.6644 - val_loss: 0.8856 - val_accuracy: 0.6667\n",
            "Epoch 316/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8733 - accuracy: 0.6518 - val_loss: 0.8849 - val_accuracy: 0.6957\n",
            "Epoch 317/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8767 - accuracy: 0.6511 - val_loss: 0.8849 - val_accuracy: 0.6908\n",
            "Epoch 318/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8583 - accuracy: 0.6530 - val_loss: 0.8877 - val_accuracy: 0.6618\n",
            "Epoch 319/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8658 - accuracy: 0.6614 - val_loss: 0.8883 - val_accuracy: 0.7005\n",
            "Epoch 320/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.8905 - accuracy: 0.6445 - val_loss: 0.9021 - val_accuracy: 0.6618\n",
            "Epoch 321/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8428 - accuracy: 0.6699 - val_loss: 0.8796 - val_accuracy: 0.6908\n",
            "Epoch 322/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8485 - accuracy: 0.6626 - val_loss: 0.8762 - val_accuracy: 0.6667\n",
            "Epoch 323/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8526 - accuracy: 0.6487 - val_loss: 0.8817 - val_accuracy: 0.6522\n",
            "Epoch 324/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8588 - accuracy: 0.6693 - val_loss: 0.8670 - val_accuracy: 0.7005\n",
            "Epoch 325/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8578 - accuracy: 0.6729 - val_loss: 0.8745 - val_accuracy: 0.6812\n",
            "Epoch 326/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8527 - accuracy: 0.6753 - val_loss: 0.8955 - val_accuracy: 0.6618\n",
            "Epoch 327/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8538 - accuracy: 0.6681 - val_loss: 0.8762 - val_accuracy: 0.6667\n",
            "Epoch 328/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.8420 - accuracy: 0.6711 - val_loss: 0.8937 - val_accuracy: 0.6860\n",
            "Epoch 329/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8594 - accuracy: 0.6657 - val_loss: 0.8909 - val_accuracy: 0.6763\n",
            "Epoch 330/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8428 - accuracy: 0.6572 - val_loss: 0.8818 - val_accuracy: 0.6908\n",
            "Epoch 331/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8627 - accuracy: 0.6596 - val_loss: 0.8809 - val_accuracy: 0.7053\n",
            "Epoch 332/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8328 - accuracy: 0.6711 - val_loss: 0.8694 - val_accuracy: 0.6618\n",
            "Epoch 333/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8501 - accuracy: 0.6584 - val_loss: 0.8709 - val_accuracy: 0.6860\n",
            "Epoch 334/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8539 - accuracy: 0.6644 - val_loss: 0.8637 - val_accuracy: 0.6812\n",
            "Epoch 335/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8364 - accuracy: 0.6705 - val_loss: 0.8728 - val_accuracy: 0.6763\n",
            "Epoch 336/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8486 - accuracy: 0.6602 - val_loss: 0.8609 - val_accuracy: 0.6715\n",
            "Epoch 337/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8354 - accuracy: 0.6723 - val_loss: 0.8848 - val_accuracy: 0.6715\n",
            "Epoch 338/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8391 - accuracy: 0.6590 - val_loss: 0.8731 - val_accuracy: 0.6618\n",
            "Epoch 339/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8426 - accuracy: 0.6626 - val_loss: 0.8602 - val_accuracy: 0.7005\n",
            "Epoch 340/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8413 - accuracy: 0.6729 - val_loss: 0.8817 - val_accuracy: 0.6715\n",
            "Epoch 341/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8415 - accuracy: 0.6548 - val_loss: 0.8744 - val_accuracy: 0.6908\n",
            "Epoch 342/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8295 - accuracy: 0.6638 - val_loss: 0.8615 - val_accuracy: 0.6812\n",
            "Epoch 343/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8614 - accuracy: 0.6711 - val_loss: 0.8804 - val_accuracy: 0.6618\n",
            "Epoch 344/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8368 - accuracy: 0.6723 - val_loss: 0.8736 - val_accuracy: 0.7005\n",
            "Epoch 345/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8252 - accuracy: 0.6657 - val_loss: 0.8586 - val_accuracy: 0.6763\n",
            "Epoch 346/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8230 - accuracy: 0.6675 - val_loss: 0.8660 - val_accuracy: 0.6860\n",
            "Epoch 347/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.8363 - accuracy: 0.6614 - val_loss: 0.8736 - val_accuracy: 0.7005\n",
            "Epoch 348/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8219 - accuracy: 0.6735 - val_loss: 0.8637 - val_accuracy: 0.6763\n",
            "Epoch 349/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8264 - accuracy: 0.6651 - val_loss: 0.8715 - val_accuracy: 0.6570\n",
            "Epoch 350/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8160 - accuracy: 0.6868 - val_loss: 0.8607 - val_accuracy: 0.6812\n",
            "Epoch 351/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8189 - accuracy: 0.6741 - val_loss: 0.8627 - val_accuracy: 0.6715\n",
            "Epoch 352/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8183 - accuracy: 0.6778 - val_loss: 0.8549 - val_accuracy: 0.6715\n",
            "Epoch 353/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8060 - accuracy: 0.6826 - val_loss: 0.8550 - val_accuracy: 0.6715\n",
            "Epoch 354/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8353 - accuracy: 0.6657 - val_loss: 0.8677 - val_accuracy: 0.6425\n",
            "Epoch 355/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7945 - accuracy: 0.6941 - val_loss: 0.8615 - val_accuracy: 0.6715\n",
            "Epoch 356/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8474 - accuracy: 0.6626 - val_loss: 0.8600 - val_accuracy: 0.6860\n",
            "Epoch 357/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7961 - accuracy: 0.6917 - val_loss: 0.8536 - val_accuracy: 0.6763\n",
            "Epoch 358/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8015 - accuracy: 0.6917 - val_loss: 0.8608 - val_accuracy: 0.6763\n",
            "Epoch 359/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7964 - accuracy: 0.7013 - val_loss: 0.8758 - val_accuracy: 0.6667\n",
            "Epoch 360/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7963 - accuracy: 0.6856 - val_loss: 0.8515 - val_accuracy: 0.6908\n",
            "Epoch 361/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8166 - accuracy: 0.6826 - val_loss: 0.8316 - val_accuracy: 0.6860\n",
            "Epoch 362/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7990 - accuracy: 0.6826 - val_loss: 0.8615 - val_accuracy: 0.6715\n",
            "Epoch 363/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8031 - accuracy: 0.6826 - val_loss: 0.8532 - val_accuracy: 0.6860\n",
            "Epoch 364/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8037 - accuracy: 0.6784 - val_loss: 0.8506 - val_accuracy: 0.6812\n",
            "Epoch 365/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8122 - accuracy: 0.6856 - val_loss: 0.8648 - val_accuracy: 0.6618\n",
            "Epoch 366/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7957 - accuracy: 0.6814 - val_loss: 0.8485 - val_accuracy: 0.6957\n",
            "Epoch 367/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8164 - accuracy: 0.6862 - val_loss: 0.8499 - val_accuracy: 0.6812\n",
            "Epoch 368/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8088 - accuracy: 0.6826 - val_loss: 0.8390 - val_accuracy: 0.6715\n",
            "Epoch 369/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7930 - accuracy: 0.7037 - val_loss: 0.8348 - val_accuracy: 0.6860\n",
            "Epoch 370/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7900 - accuracy: 0.6989 - val_loss: 0.8305 - val_accuracy: 0.7005\n",
            "Epoch 371/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8167 - accuracy: 0.6723 - val_loss: 0.8472 - val_accuracy: 0.6715\n",
            "Epoch 372/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8132 - accuracy: 0.6832 - val_loss: 0.8333 - val_accuracy: 0.7005\n",
            "Epoch 373/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.8042 - accuracy: 0.6814 - val_loss: 0.8300 - val_accuracy: 0.6860\n",
            "Epoch 374/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7933 - accuracy: 0.6874 - val_loss: 0.8290 - val_accuracy: 0.7053\n",
            "Epoch 375/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7925 - accuracy: 0.6898 - val_loss: 0.8577 - val_accuracy: 0.6618\n",
            "Epoch 376/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8000 - accuracy: 0.6808 - val_loss: 0.8472 - val_accuracy: 0.6812\n",
            "Epoch 377/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7926 - accuracy: 0.6929 - val_loss: 0.8577 - val_accuracy: 0.6715\n",
            "Epoch 378/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8121 - accuracy: 0.6844 - val_loss: 0.8632 - val_accuracy: 0.6667\n",
            "Epoch 379/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.8045 - accuracy: 0.6868 - val_loss: 0.8371 - val_accuracy: 0.6763\n",
            "Epoch 380/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.7902 - accuracy: 0.6935 - val_loss: 0.8266 - val_accuracy: 0.6957\n",
            "Epoch 381/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7820 - accuracy: 0.6911 - val_loss: 0.8190 - val_accuracy: 0.7101\n",
            "Epoch 382/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7856 - accuracy: 0.6898 - val_loss: 0.8460 - val_accuracy: 0.6763\n",
            "Epoch 383/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7982 - accuracy: 0.6898 - val_loss: 0.8376 - val_accuracy: 0.6860\n",
            "Epoch 384/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7906 - accuracy: 0.6947 - val_loss: 0.8307 - val_accuracy: 0.7005\n",
            "Epoch 385/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7724 - accuracy: 0.6989 - val_loss: 0.8237 - val_accuracy: 0.6715\n",
            "Epoch 386/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7576 - accuracy: 0.7074 - val_loss: 0.8409 - val_accuracy: 0.6812\n",
            "Epoch 387/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7718 - accuracy: 0.6977 - val_loss: 0.8359 - val_accuracy: 0.6763\n",
            "Epoch 388/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7784 - accuracy: 0.6886 - val_loss: 0.8327 - val_accuracy: 0.7053\n",
            "Epoch 389/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7815 - accuracy: 0.6977 - val_loss: 0.8218 - val_accuracy: 0.6957\n",
            "Epoch 390/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7812 - accuracy: 0.6947 - val_loss: 0.8235 - val_accuracy: 0.6812\n",
            "Epoch 391/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7629 - accuracy: 0.7098 - val_loss: 0.8483 - val_accuracy: 0.6618\n",
            "Epoch 392/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7854 - accuracy: 0.6947 - val_loss: 0.8398 - val_accuracy: 0.6812\n",
            "Epoch 393/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7899 - accuracy: 0.6923 - val_loss: 0.8230 - val_accuracy: 0.6860\n",
            "Epoch 394/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7597 - accuracy: 0.6856 - val_loss: 0.8255 - val_accuracy: 0.6908\n",
            "Epoch 395/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7516 - accuracy: 0.7025 - val_loss: 0.8202 - val_accuracy: 0.7005\n",
            "Epoch 396/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7635 - accuracy: 0.6977 - val_loss: 0.8226 - val_accuracy: 0.6812\n",
            "Epoch 397/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7735 - accuracy: 0.6935 - val_loss: 0.8586 - val_accuracy: 0.6667\n",
            "Epoch 398/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7531 - accuracy: 0.7068 - val_loss: 0.8309 - val_accuracy: 0.6618\n",
            "Epoch 399/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7405 - accuracy: 0.7158 - val_loss: 0.8058 - val_accuracy: 0.7246\n",
            "Epoch 400/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7647 - accuracy: 0.7122 - val_loss: 0.8215 - val_accuracy: 0.6908\n",
            "Epoch 401/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7544 - accuracy: 0.6941 - val_loss: 0.8207 - val_accuracy: 0.6957\n",
            "Epoch 402/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7594 - accuracy: 0.7134 - val_loss: 0.8327 - val_accuracy: 0.6763\n",
            "Epoch 403/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7405 - accuracy: 0.7122 - val_loss: 0.8239 - val_accuracy: 0.6908\n",
            "Epoch 404/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7681 - accuracy: 0.7098 - val_loss: 0.8181 - val_accuracy: 0.6715\n",
            "Epoch 405/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7457 - accuracy: 0.7050 - val_loss: 0.8338 - val_accuracy: 0.6715\n",
            "Epoch 406/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7495 - accuracy: 0.7044 - val_loss: 0.8498 - val_accuracy: 0.6715\n",
            "Epoch 407/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7583 - accuracy: 0.7037 - val_loss: 0.8464 - val_accuracy: 0.6763\n",
            "Epoch 408/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7641 - accuracy: 0.6904 - val_loss: 0.8312 - val_accuracy: 0.6763\n",
            "Epoch 409/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7363 - accuracy: 0.7104 - val_loss: 0.8406 - val_accuracy: 0.6763\n",
            "Epoch 410/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7521 - accuracy: 0.7134 - val_loss: 0.8300 - val_accuracy: 0.6667\n",
            "Epoch 411/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7423 - accuracy: 0.7019 - val_loss: 0.8486 - val_accuracy: 0.6667\n",
            "Epoch 412/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7465 - accuracy: 0.7037 - val_loss: 0.8081 - val_accuracy: 0.6957\n",
            "Epoch 413/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7268 - accuracy: 0.7177 - val_loss: 0.8002 - val_accuracy: 0.6957\n",
            "Epoch 414/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7570 - accuracy: 0.7013 - val_loss: 0.8197 - val_accuracy: 0.6908\n",
            "Epoch 415/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7275 - accuracy: 0.7140 - val_loss: 0.8001 - val_accuracy: 0.7150\n",
            "Epoch 416/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7369 - accuracy: 0.7044 - val_loss: 0.8090 - val_accuracy: 0.6860\n",
            "Epoch 417/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7481 - accuracy: 0.7074 - val_loss: 0.8216 - val_accuracy: 0.6715\n",
            "Epoch 418/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7497 - accuracy: 0.7013 - val_loss: 0.8286 - val_accuracy: 0.6667\n",
            "Epoch 419/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7398 - accuracy: 0.7086 - val_loss: 0.8101 - val_accuracy: 0.6957\n",
            "Epoch 420/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7398 - accuracy: 0.7170 - val_loss: 0.8056 - val_accuracy: 0.7005\n",
            "Epoch 421/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7377 - accuracy: 0.7110 - val_loss: 0.8230 - val_accuracy: 0.6763\n",
            "Epoch 422/1000\n",
            "104/104 [==============================] - 0s 4ms/step - loss: 0.7126 - accuracy: 0.7189 - val_loss: 0.8188 - val_accuracy: 0.6473\n",
            "Epoch 423/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7254 - accuracy: 0.7170 - val_loss: 0.8265 - val_accuracy: 0.6667\n",
            "Epoch 424/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7310 - accuracy: 0.7164 - val_loss: 0.8075 - val_accuracy: 0.6957\n",
            "Epoch 425/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7482 - accuracy: 0.7013 - val_loss: 0.8117 - val_accuracy: 0.6715\n",
            "Epoch 426/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7329 - accuracy: 0.7237 - val_loss: 0.8088 - val_accuracy: 0.6860\n",
            "Epoch 427/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7296 - accuracy: 0.7231 - val_loss: 0.7981 - val_accuracy: 0.7005\n",
            "Epoch 428/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7454 - accuracy: 0.7007 - val_loss: 0.7945 - val_accuracy: 0.7101\n",
            "Epoch 429/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7234 - accuracy: 0.7195 - val_loss: 0.8380 - val_accuracy: 0.6570\n",
            "Epoch 430/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7212 - accuracy: 0.7110 - val_loss: 0.8192 - val_accuracy: 0.7005\n",
            "Epoch 431/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7146 - accuracy: 0.7243 - val_loss: 0.8148 - val_accuracy: 0.6667\n",
            "Epoch 432/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7337 - accuracy: 0.7201 - val_loss: 0.8008 - val_accuracy: 0.7053\n",
            "Epoch 433/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7348 - accuracy: 0.7207 - val_loss: 0.8456 - val_accuracy: 0.6715\n",
            "Epoch 434/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7349 - accuracy: 0.7122 - val_loss: 0.7947 - val_accuracy: 0.7198\n",
            "Epoch 435/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7098 - accuracy: 0.7285 - val_loss: 0.8093 - val_accuracy: 0.7053\n",
            "Epoch 436/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7138 - accuracy: 0.7134 - val_loss: 0.8144 - val_accuracy: 0.6860\n",
            "Epoch 437/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7247 - accuracy: 0.7207 - val_loss: 0.7986 - val_accuracy: 0.7101\n",
            "Epoch 438/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7099 - accuracy: 0.7152 - val_loss: 0.8067 - val_accuracy: 0.6957\n",
            "Epoch 439/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7278 - accuracy: 0.7273 - val_loss: 0.8069 - val_accuracy: 0.7005\n",
            "Epoch 440/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7142 - accuracy: 0.7116 - val_loss: 0.8079 - val_accuracy: 0.7005\n",
            "Epoch 441/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7179 - accuracy: 0.7177 - val_loss: 0.7912 - val_accuracy: 0.7198\n",
            "Epoch 442/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7029 - accuracy: 0.7116 - val_loss: 0.7944 - val_accuracy: 0.7005\n",
            "Epoch 443/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7230 - accuracy: 0.7104 - val_loss: 0.7947 - val_accuracy: 0.7053\n",
            "Epoch 444/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7049 - accuracy: 0.7279 - val_loss: 0.7958 - val_accuracy: 0.7053\n",
            "Epoch 445/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.6879 - accuracy: 0.7358 - val_loss: 0.8059 - val_accuracy: 0.6957\n",
            "Epoch 446/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7117 - accuracy: 0.7225 - val_loss: 0.7927 - val_accuracy: 0.7101\n",
            "Epoch 447/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7146 - accuracy: 0.7219 - val_loss: 0.7947 - val_accuracy: 0.7005\n",
            "Epoch 448/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6967 - accuracy: 0.7219 - val_loss: 0.7815 - val_accuracy: 0.7005\n",
            "Epoch 449/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 0.6994 - accuracy: 0.7255 - val_loss: 0.7914 - val_accuracy: 0.7005\n",
            "Epoch 450/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 0.7109 - accuracy: 0.7243 - val_loss: 0.8028 - val_accuracy: 0.7005\n",
            "Epoch 451/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.7135 - accuracy: 0.7207 - val_loss: 0.8052 - val_accuracy: 0.6618\n",
            "Epoch 452/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 0.7014 - accuracy: 0.7310 - val_loss: 0.7823 - val_accuracy: 0.6812\n",
            "Epoch 453/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.6662 - accuracy: 0.7449 - val_loss: 0.8186 - val_accuracy: 0.6522\n",
            "Epoch 454/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7029 - accuracy: 0.7243 - val_loss: 0.8034 - val_accuracy: 0.6908\n",
            "Epoch 455/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6964 - accuracy: 0.7279 - val_loss: 0.7940 - val_accuracy: 0.7101\n",
            "Epoch 456/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7065 - accuracy: 0.7219 - val_loss: 0.7896 - val_accuracy: 0.7053\n",
            "Epoch 457/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6918 - accuracy: 0.7406 - val_loss: 0.7893 - val_accuracy: 0.6763\n",
            "Epoch 458/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7194 - accuracy: 0.7189 - val_loss: 0.7747 - val_accuracy: 0.7053\n",
            "Epoch 459/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6904 - accuracy: 0.7304 - val_loss: 0.7906 - val_accuracy: 0.6957\n",
            "Epoch 460/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6784 - accuracy: 0.7322 - val_loss: 0.7726 - val_accuracy: 0.7246\n",
            "Epoch 461/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6996 - accuracy: 0.7219 - val_loss: 0.7902 - val_accuracy: 0.7101\n",
            "Epoch 462/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6882 - accuracy: 0.7352 - val_loss: 0.7763 - val_accuracy: 0.7198\n",
            "Epoch 463/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7091 - accuracy: 0.7219 - val_loss: 0.7839 - val_accuracy: 0.7005\n",
            "Epoch 464/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7055 - accuracy: 0.7267 - val_loss: 0.7880 - val_accuracy: 0.6812\n",
            "Epoch 465/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.7020 - accuracy: 0.7237 - val_loss: 0.8142 - val_accuracy: 0.6473\n",
            "Epoch 466/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.6887 - accuracy: 0.7328 - val_loss: 0.8125 - val_accuracy: 0.6812\n",
            "Epoch 467/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6870 - accuracy: 0.7310 - val_loss: 0.7879 - val_accuracy: 0.6812\n",
            "Epoch 468/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6815 - accuracy: 0.7334 - val_loss: 0.7961 - val_accuracy: 0.6473\n",
            "Epoch 469/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6751 - accuracy: 0.7297 - val_loss: 0.7826 - val_accuracy: 0.6860\n",
            "Epoch 470/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6753 - accuracy: 0.7322 - val_loss: 0.7750 - val_accuracy: 0.7005\n",
            "Epoch 471/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.7002 - accuracy: 0.7279 - val_loss: 0.8075 - val_accuracy: 0.6812\n",
            "Epoch 472/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6902 - accuracy: 0.7255 - val_loss: 0.7899 - val_accuracy: 0.6860\n",
            "Epoch 473/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6522 - accuracy: 0.7473 - val_loss: 0.7917 - val_accuracy: 0.6763\n",
            "Epoch 474/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6825 - accuracy: 0.7328 - val_loss: 0.7856 - val_accuracy: 0.6715\n",
            "Epoch 475/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.6795 - accuracy: 0.7346 - val_loss: 0.7823 - val_accuracy: 0.7053\n",
            "Epoch 476/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6805 - accuracy: 0.7334 - val_loss: 0.7747 - val_accuracy: 0.7101\n",
            "Epoch 477/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6901 - accuracy: 0.7304 - val_loss: 0.7845 - val_accuracy: 0.7101\n",
            "Epoch 478/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6776 - accuracy: 0.7310 - val_loss: 0.7732 - val_accuracy: 0.7198\n",
            "Epoch 479/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6684 - accuracy: 0.7437 - val_loss: 0.7986 - val_accuracy: 0.6763\n",
            "Epoch 480/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6806 - accuracy: 0.7400 - val_loss: 0.7939 - val_accuracy: 0.6715\n",
            "Epoch 481/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.6509 - accuracy: 0.7376 - val_loss: 0.7814 - val_accuracy: 0.6860\n",
            "Epoch 482/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.6852 - accuracy: 0.7304 - val_loss: 0.7709 - val_accuracy: 0.6908\n",
            "Epoch 483/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6507 - accuracy: 0.7437 - val_loss: 0.7900 - val_accuracy: 0.6667\n",
            "Epoch 484/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6950 - accuracy: 0.7418 - val_loss: 0.7862 - val_accuracy: 0.6860\n",
            "Epoch 485/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6696 - accuracy: 0.7491 - val_loss: 0.7760 - val_accuracy: 0.6908\n",
            "Epoch 486/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6684 - accuracy: 0.7418 - val_loss: 0.7745 - val_accuracy: 0.6957\n",
            "Epoch 487/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6740 - accuracy: 0.7304 - val_loss: 0.7853 - val_accuracy: 0.6715\n",
            "Epoch 488/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6774 - accuracy: 0.7334 - val_loss: 0.7772 - val_accuracy: 0.7150\n",
            "Epoch 489/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6590 - accuracy: 0.7443 - val_loss: 0.7645 - val_accuracy: 0.7053\n",
            "Epoch 490/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6450 - accuracy: 0.7497 - val_loss: 0.7822 - val_accuracy: 0.6957\n",
            "Epoch 491/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6741 - accuracy: 0.7340 - val_loss: 0.7675 - val_accuracy: 0.7005\n",
            "Epoch 492/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6463 - accuracy: 0.7551 - val_loss: 0.7562 - val_accuracy: 0.7246\n",
            "Epoch 493/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.6628 - accuracy: 0.7340 - val_loss: 0.7537 - val_accuracy: 0.7053\n",
            "Epoch 494/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6603 - accuracy: 0.7485 - val_loss: 0.7606 - val_accuracy: 0.7101\n",
            "Epoch 495/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6471 - accuracy: 0.7515 - val_loss: 0.7816 - val_accuracy: 0.7005\n",
            "Epoch 496/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6369 - accuracy: 0.7473 - val_loss: 0.7770 - val_accuracy: 0.6908\n",
            "Epoch 497/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.6431 - accuracy: 0.7479 - val_loss: 0.7790 - val_accuracy: 0.6763\n",
            "Epoch 498/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6463 - accuracy: 0.7352 - val_loss: 0.7659 - val_accuracy: 0.7150\n",
            "Epoch 499/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.6367 - accuracy: 0.7588 - val_loss: 0.7600 - val_accuracy: 0.7246\n",
            "Epoch 500/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6414 - accuracy: 0.7576 - val_loss: 0.7781 - val_accuracy: 0.6957\n",
            "Epoch 501/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6483 - accuracy: 0.7418 - val_loss: 0.7696 - val_accuracy: 0.6957\n",
            "Epoch 502/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.6590 - accuracy: 0.7551 - val_loss: 0.7852 - val_accuracy: 0.7053\n",
            "Epoch 503/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.6349 - accuracy: 0.7533 - val_loss: 0.7551 - val_accuracy: 0.7246\n",
            "Epoch 504/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6604 - accuracy: 0.7418 - val_loss: 0.7800 - val_accuracy: 0.6812\n",
            "Epoch 505/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.6367 - accuracy: 0.7527 - val_loss: 0.7608 - val_accuracy: 0.7150\n",
            "Epoch 506/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.6387 - accuracy: 0.7455 - val_loss: 0.7586 - val_accuracy: 0.7150\n",
            "Epoch 507/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6441 - accuracy: 0.7527 - val_loss: 0.7680 - val_accuracy: 0.7246\n",
            "Epoch 508/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.6328 - accuracy: 0.7479 - val_loss: 0.7633 - val_accuracy: 0.7246\n",
            "Epoch 509/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.6496 - accuracy: 0.7370 - val_loss: 0.7617 - val_accuracy: 0.7150\n",
            "Epoch 510/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.6530 - accuracy: 0.7515 - val_loss: 0.7895 - val_accuracy: 0.6715\n",
            "Epoch 511/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6403 - accuracy: 0.7551 - val_loss: 0.7567 - val_accuracy: 0.7391\n",
            "Epoch 512/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6408 - accuracy: 0.7515 - val_loss: 0.7615 - val_accuracy: 0.7101\n",
            "Epoch 513/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6258 - accuracy: 0.7521 - val_loss: 0.7715 - val_accuracy: 0.7053\n",
            "Epoch 514/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6156 - accuracy: 0.7563 - val_loss: 0.7713 - val_accuracy: 0.7101\n",
            "Epoch 515/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6311 - accuracy: 0.7437 - val_loss: 0.7594 - val_accuracy: 0.7053\n",
            "Epoch 516/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6209 - accuracy: 0.7551 - val_loss: 0.7553 - val_accuracy: 0.7005\n",
            "Epoch 517/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6241 - accuracy: 0.7479 - val_loss: 0.7590 - val_accuracy: 0.7053\n",
            "Epoch 518/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6253 - accuracy: 0.7539 - val_loss: 0.7557 - val_accuracy: 0.7101\n",
            "Epoch 519/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6471 - accuracy: 0.7382 - val_loss: 0.7306 - val_accuracy: 0.7246\n",
            "Epoch 520/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6360 - accuracy: 0.7509 - val_loss: 0.7378 - val_accuracy: 0.7198\n",
            "Epoch 521/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6252 - accuracy: 0.7654 - val_loss: 0.7376 - val_accuracy: 0.7295\n",
            "Epoch 522/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6182 - accuracy: 0.7678 - val_loss: 0.7443 - val_accuracy: 0.7295\n",
            "Epoch 523/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6367 - accuracy: 0.7424 - val_loss: 0.7373 - val_accuracy: 0.7150\n",
            "Epoch 524/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6341 - accuracy: 0.7527 - val_loss: 0.7307 - val_accuracy: 0.7198\n",
            "Epoch 525/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6104 - accuracy: 0.7654 - val_loss: 0.7460 - val_accuracy: 0.7150\n",
            "Epoch 526/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6088 - accuracy: 0.7678 - val_loss: 0.7437 - val_accuracy: 0.7246\n",
            "Epoch 527/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6144 - accuracy: 0.7594 - val_loss: 0.7459 - val_accuracy: 0.7198\n",
            "Epoch 528/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6204 - accuracy: 0.7582 - val_loss: 0.7695 - val_accuracy: 0.7246\n",
            "Epoch 529/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6177 - accuracy: 0.7648 - val_loss: 0.7683 - val_accuracy: 0.7005\n",
            "Epoch 530/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6339 - accuracy: 0.7533 - val_loss: 0.7807 - val_accuracy: 0.6908\n",
            "Epoch 531/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6298 - accuracy: 0.7467 - val_loss: 0.7623 - val_accuracy: 0.6908\n",
            "Epoch 532/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6305 - accuracy: 0.7557 - val_loss: 0.7525 - val_accuracy: 0.7053\n",
            "Epoch 533/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6146 - accuracy: 0.7618 - val_loss: 0.7506 - val_accuracy: 0.7101\n",
            "Epoch 534/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6425 - accuracy: 0.7455 - val_loss: 0.7654 - val_accuracy: 0.7005\n",
            "Epoch 535/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6194 - accuracy: 0.7563 - val_loss: 0.7466 - val_accuracy: 0.7198\n",
            "Epoch 536/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.6257 - accuracy: 0.7557 - val_loss: 0.7419 - val_accuracy: 0.7343\n",
            "Epoch 537/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.6020 - accuracy: 0.7660 - val_loss: 0.7330 - val_accuracy: 0.7295\n",
            "Epoch 538/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6089 - accuracy: 0.7509 - val_loss: 0.7440 - val_accuracy: 0.7343\n",
            "Epoch 539/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6040 - accuracy: 0.7703 - val_loss: 0.7468 - val_accuracy: 0.7198\n",
            "Epoch 540/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5945 - accuracy: 0.7721 - val_loss: 0.7352 - val_accuracy: 0.7198\n",
            "Epoch 541/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6342 - accuracy: 0.7654 - val_loss: 0.7525 - val_accuracy: 0.7150\n",
            "Epoch 542/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5846 - accuracy: 0.7715 - val_loss: 0.7330 - val_accuracy: 0.7295\n",
            "Epoch 543/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5990 - accuracy: 0.7630 - val_loss: 0.7399 - val_accuracy: 0.7053\n",
            "Epoch 544/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5948 - accuracy: 0.7703 - val_loss: 0.7597 - val_accuracy: 0.7005\n",
            "Epoch 545/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5994 - accuracy: 0.7642 - val_loss: 0.7607 - val_accuracy: 0.7005\n",
            "Epoch 546/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6175 - accuracy: 0.7497 - val_loss: 0.7548 - val_accuracy: 0.6812\n",
            "Epoch 547/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5922 - accuracy: 0.7690 - val_loss: 0.7410 - val_accuracy: 0.7150\n",
            "Epoch 548/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5986 - accuracy: 0.7703 - val_loss: 0.7598 - val_accuracy: 0.7005\n",
            "Epoch 549/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5969 - accuracy: 0.7624 - val_loss: 0.7406 - val_accuracy: 0.7198\n",
            "Epoch 550/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5938 - accuracy: 0.7690 - val_loss: 0.7373 - val_accuracy: 0.7053\n",
            "Epoch 551/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5853 - accuracy: 0.7751 - val_loss: 0.7331 - val_accuracy: 0.7150\n",
            "Epoch 552/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6036 - accuracy: 0.7678 - val_loss: 0.7416 - val_accuracy: 0.7150\n",
            "Epoch 553/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6120 - accuracy: 0.7642 - val_loss: 0.7383 - val_accuracy: 0.7101\n",
            "Epoch 554/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5925 - accuracy: 0.7763 - val_loss: 0.7254 - val_accuracy: 0.7198\n",
            "Epoch 555/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5929 - accuracy: 0.7727 - val_loss: 0.7311 - val_accuracy: 0.7053\n",
            "Epoch 556/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5883 - accuracy: 0.7715 - val_loss: 0.7312 - val_accuracy: 0.7053\n",
            "Epoch 557/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6078 - accuracy: 0.7769 - val_loss: 0.7485 - val_accuracy: 0.7005\n",
            "Epoch 558/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6088 - accuracy: 0.7582 - val_loss: 0.7423 - val_accuracy: 0.7005\n",
            "Epoch 559/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.5829 - accuracy: 0.7769 - val_loss: 0.7288 - val_accuracy: 0.7295\n",
            "Epoch 560/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5817 - accuracy: 0.7823 - val_loss: 0.7227 - val_accuracy: 0.7343\n",
            "Epoch 561/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5820 - accuracy: 0.7860 - val_loss: 0.7484 - val_accuracy: 0.7053\n",
            "Epoch 562/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5858 - accuracy: 0.7703 - val_loss: 0.7526 - val_accuracy: 0.7053\n",
            "Epoch 563/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5604 - accuracy: 0.7751 - val_loss: 0.7381 - val_accuracy: 0.7150\n",
            "Epoch 564/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5898 - accuracy: 0.7636 - val_loss: 0.7461 - val_accuracy: 0.7053\n",
            "Epoch 565/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5827 - accuracy: 0.7751 - val_loss: 0.7433 - val_accuracy: 0.7005\n",
            "Epoch 566/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5740 - accuracy: 0.7769 - val_loss: 0.7460 - val_accuracy: 0.7053\n",
            "Epoch 567/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5642 - accuracy: 0.7805 - val_loss: 0.7331 - val_accuracy: 0.7101\n",
            "Epoch 568/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.5813 - accuracy: 0.7678 - val_loss: 0.7325 - val_accuracy: 0.7150\n",
            "Epoch 569/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.5847 - accuracy: 0.7684 - val_loss: 0.7540 - val_accuracy: 0.7150\n",
            "Epoch 570/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5923 - accuracy: 0.7721 - val_loss: 0.7572 - val_accuracy: 0.6908\n",
            "Epoch 571/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5835 - accuracy: 0.7630 - val_loss: 0.7587 - val_accuracy: 0.6812\n",
            "Epoch 572/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5929 - accuracy: 0.7793 - val_loss: 0.7530 - val_accuracy: 0.7101\n",
            "Epoch 573/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5751 - accuracy: 0.7709 - val_loss: 0.7461 - val_accuracy: 0.6860\n",
            "Epoch 574/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5730 - accuracy: 0.7745 - val_loss: 0.7407 - val_accuracy: 0.7101\n",
            "Epoch 575/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5808 - accuracy: 0.7721 - val_loss: 0.7405 - val_accuracy: 0.7053\n",
            "Epoch 576/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5680 - accuracy: 0.7914 - val_loss: 0.7470 - val_accuracy: 0.7101\n",
            "Epoch 577/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5787 - accuracy: 0.7715 - val_loss: 0.7259 - val_accuracy: 0.7198\n",
            "Epoch 578/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5957 - accuracy: 0.7539 - val_loss: 0.7383 - val_accuracy: 0.7053\n",
            "Epoch 579/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5797 - accuracy: 0.7817 - val_loss: 0.7412 - val_accuracy: 0.7101\n",
            "Epoch 580/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.6121 - accuracy: 0.7654 - val_loss: 0.7360 - val_accuracy: 0.7246\n",
            "Epoch 581/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5829 - accuracy: 0.7733 - val_loss: 0.7292 - val_accuracy: 0.7246\n",
            "Epoch 582/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5556 - accuracy: 0.7860 - val_loss: 0.7415 - val_accuracy: 0.7198\n",
            "Epoch 583/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5732 - accuracy: 0.7860 - val_loss: 0.7348 - val_accuracy: 0.7005\n",
            "Epoch 584/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5639 - accuracy: 0.7775 - val_loss: 0.7223 - val_accuracy: 0.7101\n",
            "Epoch 585/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5767 - accuracy: 0.7606 - val_loss: 0.7109 - val_accuracy: 0.7295\n",
            "Epoch 586/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5497 - accuracy: 0.7751 - val_loss: 0.7139 - val_accuracy: 0.7246\n",
            "Epoch 587/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.5696 - accuracy: 0.7739 - val_loss: 0.7307 - val_accuracy: 0.7005\n",
            "Epoch 588/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5801 - accuracy: 0.7709 - val_loss: 0.7228 - val_accuracy: 0.7440\n",
            "Epoch 589/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5743 - accuracy: 0.7775 - val_loss: 0.7488 - val_accuracy: 0.6957\n",
            "Epoch 590/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5571 - accuracy: 0.7884 - val_loss: 0.7071 - val_accuracy: 0.7343\n",
            "Epoch 591/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.5482 - accuracy: 0.7830 - val_loss: 0.7302 - val_accuracy: 0.7101\n",
            "Epoch 592/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5723 - accuracy: 0.7817 - val_loss: 0.7467 - val_accuracy: 0.7053\n",
            "Epoch 593/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5689 - accuracy: 0.7733 - val_loss: 0.7346 - val_accuracy: 0.7198\n",
            "Epoch 594/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.5575 - accuracy: 0.7908 - val_loss: 0.7325 - val_accuracy: 0.7150\n",
            "Epoch 595/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5472 - accuracy: 0.7902 - val_loss: 0.7438 - val_accuracy: 0.7101\n",
            "Epoch 596/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5469 - accuracy: 0.7969 - val_loss: 0.7334 - val_accuracy: 0.7053\n",
            "Epoch 597/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5706 - accuracy: 0.7854 - val_loss: 0.7520 - val_accuracy: 0.7005\n",
            "Epoch 598/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5288 - accuracy: 0.8011 - val_loss: 0.7295 - val_accuracy: 0.7101\n",
            "Epoch 599/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5543 - accuracy: 0.7860 - val_loss: 0.7220 - val_accuracy: 0.7150\n",
            "Epoch 600/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5528 - accuracy: 0.7703 - val_loss: 0.7620 - val_accuracy: 0.6812\n",
            "Epoch 601/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5493 - accuracy: 0.7926 - val_loss: 0.7133 - val_accuracy: 0.7246\n",
            "Epoch 602/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5412 - accuracy: 0.7878 - val_loss: 0.7427 - val_accuracy: 0.7005\n",
            "Epoch 603/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5336 - accuracy: 0.7932 - val_loss: 0.7418 - val_accuracy: 0.6957\n",
            "Epoch 604/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5294 - accuracy: 0.8065 - val_loss: 0.7100 - val_accuracy: 0.7343\n",
            "Epoch 605/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5783 - accuracy: 0.7745 - val_loss: 0.7312 - val_accuracy: 0.7150\n",
            "Epoch 606/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5536 - accuracy: 0.7836 - val_loss: 0.7322 - val_accuracy: 0.7053\n",
            "Epoch 607/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5258 - accuracy: 0.7908 - val_loss: 0.7208 - val_accuracy: 0.7053\n",
            "Epoch 608/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5536 - accuracy: 0.7775 - val_loss: 0.7280 - val_accuracy: 0.7150\n",
            "Epoch 609/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5343 - accuracy: 0.8029 - val_loss: 0.7249 - val_accuracy: 0.7101\n",
            "Epoch 610/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5215 - accuracy: 0.7944 - val_loss: 0.7044 - val_accuracy: 0.7198\n",
            "Epoch 611/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5441 - accuracy: 0.7860 - val_loss: 0.7000 - val_accuracy: 0.7295\n",
            "Epoch 612/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5395 - accuracy: 0.7914 - val_loss: 0.7256 - val_accuracy: 0.7150\n",
            "Epoch 613/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5301 - accuracy: 0.7902 - val_loss: 0.7125 - val_accuracy: 0.7391\n",
            "Epoch 614/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5113 - accuracy: 0.8083 - val_loss: 0.7084 - val_accuracy: 0.7198\n",
            "Epoch 615/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5371 - accuracy: 0.7890 - val_loss: 0.6925 - val_accuracy: 0.7440\n",
            "Epoch 616/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.5485 - accuracy: 0.8011 - val_loss: 0.7224 - val_accuracy: 0.7150\n",
            "Epoch 617/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5358 - accuracy: 0.8029 - val_loss: 0.7110 - val_accuracy: 0.7198\n",
            "Epoch 618/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5260 - accuracy: 0.8077 - val_loss: 0.7250 - val_accuracy: 0.7150\n",
            "Epoch 619/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5462 - accuracy: 0.7944 - val_loss: 0.7242 - val_accuracy: 0.7005\n",
            "Epoch 620/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5190 - accuracy: 0.7963 - val_loss: 0.7142 - val_accuracy: 0.6957\n",
            "Epoch 621/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5366 - accuracy: 0.7963 - val_loss: 0.7111 - val_accuracy: 0.7053\n",
            "Epoch 622/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5278 - accuracy: 0.8011 - val_loss: 0.7261 - val_accuracy: 0.7150\n",
            "Epoch 623/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5249 - accuracy: 0.7999 - val_loss: 0.7280 - val_accuracy: 0.6860\n",
            "Epoch 624/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5115 - accuracy: 0.8071 - val_loss: 0.7059 - val_accuracy: 0.7198\n",
            "Epoch 625/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5350 - accuracy: 0.7872 - val_loss: 0.7048 - val_accuracy: 0.7198\n",
            "Epoch 626/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5254 - accuracy: 0.7956 - val_loss: 0.7136 - val_accuracy: 0.7198\n",
            "Epoch 627/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5235 - accuracy: 0.7872 - val_loss: 0.7020 - val_accuracy: 0.7391\n",
            "Epoch 628/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5262 - accuracy: 0.8047 - val_loss: 0.7087 - val_accuracy: 0.7150\n",
            "Epoch 629/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.5150 - accuracy: 0.7999 - val_loss: 0.7227 - val_accuracy: 0.7198\n",
            "Epoch 630/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5403 - accuracy: 0.7848 - val_loss: 0.7357 - val_accuracy: 0.6908\n",
            "Epoch 631/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5150 - accuracy: 0.8102 - val_loss: 0.7091 - val_accuracy: 0.7246\n",
            "Epoch 632/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5326 - accuracy: 0.7969 - val_loss: 0.7059 - val_accuracy: 0.7391\n",
            "Epoch 633/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5232 - accuracy: 0.8011 - val_loss: 0.7087 - val_accuracy: 0.7391\n",
            "Epoch 634/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5287 - accuracy: 0.7981 - val_loss: 0.7141 - val_accuracy: 0.7053\n",
            "Epoch 635/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5344 - accuracy: 0.7920 - val_loss: 0.6894 - val_accuracy: 0.7246\n",
            "Epoch 636/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.5253 - accuracy: 0.8047 - val_loss: 0.7162 - val_accuracy: 0.7246\n",
            "Epoch 637/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5327 - accuracy: 0.7950 - val_loss: 0.6770 - val_accuracy: 0.7391\n",
            "Epoch 638/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.5233 - accuracy: 0.8005 - val_loss: 0.7184 - val_accuracy: 0.7053\n",
            "Epoch 639/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.5313 - accuracy: 0.7987 - val_loss: 0.7034 - val_accuracy: 0.7295\n",
            "Epoch 640/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5018 - accuracy: 0.8150 - val_loss: 0.7094 - val_accuracy: 0.7343\n",
            "Epoch 641/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.4942 - accuracy: 0.8029 - val_loss: 0.7182 - val_accuracy: 0.7246\n",
            "Epoch 642/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5019 - accuracy: 0.8071 - val_loss: 0.7267 - val_accuracy: 0.6957\n",
            "Epoch 643/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5064 - accuracy: 0.7981 - val_loss: 0.7176 - val_accuracy: 0.7150\n",
            "Epoch 644/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.5114 - accuracy: 0.7920 - val_loss: 0.7099 - val_accuracy: 0.7440\n",
            "Epoch 645/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5304 - accuracy: 0.7914 - val_loss: 0.6867 - val_accuracy: 0.7391\n",
            "Epoch 646/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.5207 - accuracy: 0.7950 - val_loss: 0.6987 - val_accuracy: 0.7198\n",
            "Epoch 647/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5101 - accuracy: 0.8047 - val_loss: 0.7006 - val_accuracy: 0.7198\n",
            "Epoch 648/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.4955 - accuracy: 0.7969 - val_loss: 0.7149 - val_accuracy: 0.7005\n",
            "Epoch 649/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5085 - accuracy: 0.8065 - val_loss: 0.7148 - val_accuracy: 0.7440\n",
            "Epoch 650/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5081 - accuracy: 0.8059 - val_loss: 0.7005 - val_accuracy: 0.7391\n",
            "Epoch 651/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5445 - accuracy: 0.7890 - val_loss: 0.7060 - val_accuracy: 0.7295\n",
            "Epoch 652/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.5056 - accuracy: 0.7987 - val_loss: 0.7076 - val_accuracy: 0.7150\n",
            "Epoch 653/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5130 - accuracy: 0.8102 - val_loss: 0.6995 - val_accuracy: 0.7246\n",
            "Epoch 654/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5053 - accuracy: 0.8077 - val_loss: 0.7034 - val_accuracy: 0.7391\n",
            "Epoch 655/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4968 - accuracy: 0.8120 - val_loss: 0.6976 - val_accuracy: 0.7440\n",
            "Epoch 656/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5298 - accuracy: 0.7956 - val_loss: 0.7209 - val_accuracy: 0.7053\n",
            "Epoch 657/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5108 - accuracy: 0.7999 - val_loss: 0.7068 - val_accuracy: 0.7053\n",
            "Epoch 658/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4889 - accuracy: 0.8156 - val_loss: 0.7060 - val_accuracy: 0.7246\n",
            "Epoch 659/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4855 - accuracy: 0.8289 - val_loss: 0.6991 - val_accuracy: 0.7150\n",
            "Epoch 660/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5113 - accuracy: 0.8089 - val_loss: 0.6883 - val_accuracy: 0.7440\n",
            "Epoch 661/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4997 - accuracy: 0.8035 - val_loss: 0.6942 - val_accuracy: 0.7246\n",
            "Epoch 662/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4787 - accuracy: 0.8216 - val_loss: 0.6985 - val_accuracy: 0.7391\n",
            "Epoch 663/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4905 - accuracy: 0.8096 - val_loss: 0.6906 - val_accuracy: 0.7198\n",
            "Epoch 664/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4877 - accuracy: 0.8198 - val_loss: 0.6978 - val_accuracy: 0.7053\n",
            "Epoch 665/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.4871 - accuracy: 0.8108 - val_loss: 0.7089 - val_accuracy: 0.7053\n",
            "Epoch 666/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4981 - accuracy: 0.8096 - val_loss: 0.6864 - val_accuracy: 0.7198\n",
            "Epoch 667/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4949 - accuracy: 0.8222 - val_loss: 0.6924 - val_accuracy: 0.7440\n",
            "Epoch 668/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4902 - accuracy: 0.8138 - val_loss: 0.6924 - val_accuracy: 0.7246\n",
            "Epoch 669/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4833 - accuracy: 0.8156 - val_loss: 0.6933 - val_accuracy: 0.7246\n",
            "Epoch 670/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4810 - accuracy: 0.8120 - val_loss: 0.6782 - val_accuracy: 0.7343\n",
            "Epoch 671/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.4769 - accuracy: 0.8156 - val_loss: 0.6654 - val_accuracy: 0.7440\n",
            "Epoch 672/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4794 - accuracy: 0.8120 - val_loss: 0.6878 - val_accuracy: 0.7488\n",
            "Epoch 673/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4820 - accuracy: 0.8132 - val_loss: 0.6953 - val_accuracy: 0.7198\n",
            "Epoch 674/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4967 - accuracy: 0.8047 - val_loss: 0.6870 - val_accuracy: 0.7343\n",
            "Epoch 675/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4821 - accuracy: 0.8247 - val_loss: 0.7030 - val_accuracy: 0.7343\n",
            "Epoch 676/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4851 - accuracy: 0.8253 - val_loss: 0.6872 - val_accuracy: 0.7246\n",
            "Epoch 677/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.5109 - accuracy: 0.8059 - val_loss: 0.6872 - val_accuracy: 0.7198\n",
            "Epoch 678/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4828 - accuracy: 0.8192 - val_loss: 0.6857 - val_accuracy: 0.7101\n",
            "Epoch 679/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4767 - accuracy: 0.8126 - val_loss: 0.6747 - val_accuracy: 0.7246\n",
            "Epoch 680/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4836 - accuracy: 0.8150 - val_loss: 0.6765 - val_accuracy: 0.7198\n",
            "Epoch 681/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4809 - accuracy: 0.8265 - val_loss: 0.6961 - val_accuracy: 0.7150\n",
            "Epoch 682/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4753 - accuracy: 0.8216 - val_loss: 0.6926 - val_accuracy: 0.7246\n",
            "Epoch 683/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4899 - accuracy: 0.8198 - val_loss: 0.6770 - val_accuracy: 0.7150\n",
            "Epoch 684/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4806 - accuracy: 0.8102 - val_loss: 0.6759 - val_accuracy: 0.7295\n",
            "Epoch 685/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4726 - accuracy: 0.8150 - val_loss: 0.6777 - val_accuracy: 0.7198\n",
            "Epoch 686/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4473 - accuracy: 0.8301 - val_loss: 0.6687 - val_accuracy: 0.7246\n",
            "Epoch 687/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4551 - accuracy: 0.8247 - val_loss: 0.6842 - val_accuracy: 0.7246\n",
            "Epoch 688/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4621 - accuracy: 0.8289 - val_loss: 0.6900 - val_accuracy: 0.7101\n",
            "Epoch 689/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4756 - accuracy: 0.8253 - val_loss: 0.7010 - val_accuracy: 0.7150\n",
            "Epoch 690/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4751 - accuracy: 0.8132 - val_loss: 0.7057 - val_accuracy: 0.7198\n",
            "Epoch 691/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4763 - accuracy: 0.8198 - val_loss: 0.6772 - val_accuracy: 0.7295\n",
            "Epoch 692/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4606 - accuracy: 0.8253 - val_loss: 0.6709 - val_accuracy: 0.7488\n",
            "Epoch 693/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4761 - accuracy: 0.8132 - val_loss: 0.7030 - val_accuracy: 0.7005\n",
            "Epoch 694/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4524 - accuracy: 0.8229 - val_loss: 0.7004 - val_accuracy: 0.7246\n",
            "Epoch 695/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4640 - accuracy: 0.8204 - val_loss: 0.6969 - val_accuracy: 0.7101\n",
            "Epoch 696/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4699 - accuracy: 0.8168 - val_loss: 0.7030 - val_accuracy: 0.7198\n",
            "Epoch 697/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4701 - accuracy: 0.8083 - val_loss: 0.6916 - val_accuracy: 0.7198\n",
            "Epoch 698/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4801 - accuracy: 0.8162 - val_loss: 0.6822 - val_accuracy: 0.7343\n",
            "Epoch 699/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4639 - accuracy: 0.8186 - val_loss: 0.6978 - val_accuracy: 0.7440\n",
            "Epoch 700/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4576 - accuracy: 0.8277 - val_loss: 0.6912 - val_accuracy: 0.7536\n",
            "Epoch 701/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4644 - accuracy: 0.8210 - val_loss: 0.7120 - val_accuracy: 0.7101\n",
            "Epoch 702/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4662 - accuracy: 0.8144 - val_loss: 0.6958 - val_accuracy: 0.7488\n",
            "Epoch 703/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4528 - accuracy: 0.8313 - val_loss: 0.6768 - val_accuracy: 0.7536\n",
            "Epoch 704/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4516 - accuracy: 0.8210 - val_loss: 0.6955 - val_accuracy: 0.7246\n",
            "Epoch 705/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4494 - accuracy: 0.8295 - val_loss: 0.6614 - val_accuracy: 0.7440\n",
            "Epoch 706/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4449 - accuracy: 0.8307 - val_loss: 0.6660 - val_accuracy: 0.7391\n",
            "Epoch 707/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4696 - accuracy: 0.8283 - val_loss: 0.6800 - val_accuracy: 0.7391\n",
            "Epoch 708/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4613 - accuracy: 0.8241 - val_loss: 0.6845 - val_accuracy: 0.7150\n",
            "Epoch 709/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4800 - accuracy: 0.8144 - val_loss: 0.6846 - val_accuracy: 0.7343\n",
            "Epoch 710/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4565 - accuracy: 0.8319 - val_loss: 0.6904 - val_accuracy: 0.7343\n",
            "Epoch 711/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4536 - accuracy: 0.8180 - val_loss: 0.6730 - val_accuracy: 0.7391\n",
            "Epoch 712/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4589 - accuracy: 0.8138 - val_loss: 0.6628 - val_accuracy: 0.7536\n",
            "Epoch 713/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4581 - accuracy: 0.8259 - val_loss: 0.6576 - val_accuracy: 0.7536\n",
            "Epoch 714/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4437 - accuracy: 0.8168 - val_loss: 0.6701 - val_accuracy: 0.7295\n",
            "Epoch 715/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4565 - accuracy: 0.8168 - val_loss: 0.6768 - val_accuracy: 0.7488\n",
            "Epoch 716/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4634 - accuracy: 0.8271 - val_loss: 0.6652 - val_accuracy: 0.7440\n",
            "Epoch 717/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4558 - accuracy: 0.8138 - val_loss: 0.6774 - val_accuracy: 0.7246\n",
            "Epoch 718/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4574 - accuracy: 0.8265 - val_loss: 0.6569 - val_accuracy: 0.7343\n",
            "Epoch 719/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4497 - accuracy: 0.8277 - val_loss: 0.6583 - val_accuracy: 0.7633\n",
            "Epoch 720/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4461 - accuracy: 0.8222 - val_loss: 0.6840 - val_accuracy: 0.7150\n",
            "Epoch 721/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4547 - accuracy: 0.8247 - val_loss: 0.6912 - val_accuracy: 0.7295\n",
            "Epoch 722/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4397 - accuracy: 0.8398 - val_loss: 0.6717 - val_accuracy: 0.7295\n",
            "Epoch 723/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4330 - accuracy: 0.8422 - val_loss: 0.6747 - val_accuracy: 0.7005\n",
            "Epoch 724/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4480 - accuracy: 0.8289 - val_loss: 0.6593 - val_accuracy: 0.7343\n",
            "Epoch 725/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4353 - accuracy: 0.8374 - val_loss: 0.6784 - val_accuracy: 0.7391\n",
            "Epoch 726/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.4463 - accuracy: 0.8301 - val_loss: 0.6843 - val_accuracy: 0.7343\n",
            "Epoch 727/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4646 - accuracy: 0.8168 - val_loss: 0.6829 - val_accuracy: 0.7488\n",
            "Epoch 728/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4637 - accuracy: 0.8198 - val_loss: 0.6680 - val_accuracy: 0.7585\n",
            "Epoch 729/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4466 - accuracy: 0.8289 - val_loss: 0.6659 - val_accuracy: 0.7440\n",
            "Epoch 730/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4148 - accuracy: 0.8428 - val_loss: 0.6649 - val_accuracy: 0.7585\n",
            "Epoch 731/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4552 - accuracy: 0.8301 - val_loss: 0.6667 - val_accuracy: 0.7585\n",
            "Epoch 732/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4373 - accuracy: 0.8440 - val_loss: 0.6557 - val_accuracy: 0.7391\n",
            "Epoch 733/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4566 - accuracy: 0.8265 - val_loss: 0.6751 - val_accuracy: 0.7343\n",
            "Epoch 734/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4181 - accuracy: 0.8464 - val_loss: 0.6680 - val_accuracy: 0.7343\n",
            "Epoch 735/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4274 - accuracy: 0.8331 - val_loss: 0.6892 - val_accuracy: 0.7246\n",
            "Epoch 736/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4388 - accuracy: 0.8362 - val_loss: 0.6786 - val_accuracy: 0.7150\n",
            "Epoch 737/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4230 - accuracy: 0.8343 - val_loss: 0.6644 - val_accuracy: 0.7391\n",
            "Epoch 738/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4389 - accuracy: 0.8362 - val_loss: 0.6817 - val_accuracy: 0.7246\n",
            "Epoch 739/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4210 - accuracy: 0.8356 - val_loss: 0.6717 - val_accuracy: 0.7343\n",
            "Epoch 740/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4225 - accuracy: 0.8434 - val_loss: 0.6545 - val_accuracy: 0.7440\n",
            "Epoch 741/1000\n",
            "104/104 [==============================] - 0s 5ms/step - loss: 0.4344 - accuracy: 0.8319 - val_loss: 0.6617 - val_accuracy: 0.7440\n",
            "Epoch 742/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4278 - accuracy: 0.8307 - val_loss: 0.6704 - val_accuracy: 0.7440\n",
            "Epoch 743/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4274 - accuracy: 0.8374 - val_loss: 0.6732 - val_accuracy: 0.7343\n",
            "Epoch 744/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4178 - accuracy: 0.8368 - val_loss: 0.6698 - val_accuracy: 0.7536\n",
            "Epoch 745/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4368 - accuracy: 0.8289 - val_loss: 0.6617 - val_accuracy: 0.7246\n",
            "Epoch 746/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4445 - accuracy: 0.8331 - val_loss: 0.6553 - val_accuracy: 0.7343\n",
            "Epoch 747/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4245 - accuracy: 0.8337 - val_loss: 0.6540 - val_accuracy: 0.7343\n",
            "Epoch 748/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4311 - accuracy: 0.8349 - val_loss: 0.6821 - val_accuracy: 0.7295\n",
            "Epoch 749/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4307 - accuracy: 0.8325 - val_loss: 0.6639 - val_accuracy: 0.7391\n",
            "Epoch 750/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4186 - accuracy: 0.8410 - val_loss: 0.6535 - val_accuracy: 0.7391\n",
            "Epoch 751/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4315 - accuracy: 0.8392 - val_loss: 0.6601 - val_accuracy: 0.7440\n",
            "Epoch 752/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4299 - accuracy: 0.8241 - val_loss: 0.6443 - val_accuracy: 0.7585\n",
            "Epoch 753/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4318 - accuracy: 0.8319 - val_loss: 0.6704 - val_accuracy: 0.7440\n",
            "Epoch 754/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4247 - accuracy: 0.8452 - val_loss: 0.6518 - val_accuracy: 0.7536\n",
            "Epoch 755/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.4211 - accuracy: 0.8513 - val_loss: 0.6939 - val_accuracy: 0.7295\n",
            "Epoch 756/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4211 - accuracy: 0.8277 - val_loss: 0.6589 - val_accuracy: 0.7295\n",
            "Epoch 757/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4114 - accuracy: 0.8464 - val_loss: 0.6561 - val_accuracy: 0.7633\n",
            "Epoch 758/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4185 - accuracy: 0.8398 - val_loss: 0.6648 - val_accuracy: 0.7391\n",
            "Epoch 759/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4136 - accuracy: 0.8440 - val_loss: 0.6538 - val_accuracy: 0.7536\n",
            "Epoch 760/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4253 - accuracy: 0.8307 - val_loss: 0.6485 - val_accuracy: 0.7488\n",
            "Epoch 761/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4161 - accuracy: 0.8434 - val_loss: 0.6508 - val_accuracy: 0.7585\n",
            "Epoch 762/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3882 - accuracy: 0.8646 - val_loss: 0.6693 - val_accuracy: 0.7536\n",
            "Epoch 763/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4238 - accuracy: 0.8374 - val_loss: 0.6567 - val_accuracy: 0.7536\n",
            "Epoch 764/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4179 - accuracy: 0.8325 - val_loss: 0.6385 - val_accuracy: 0.7729\n",
            "Epoch 765/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4206 - accuracy: 0.8470 - val_loss: 0.6466 - val_accuracy: 0.7536\n",
            "Epoch 766/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4048 - accuracy: 0.8452 - val_loss: 0.6567 - val_accuracy: 0.7585\n",
            "Epoch 767/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4019 - accuracy: 0.8428 - val_loss: 0.6412 - val_accuracy: 0.7681\n",
            "Epoch 768/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4014 - accuracy: 0.8525 - val_loss: 0.6569 - val_accuracy: 0.7391\n",
            "Epoch 769/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4023 - accuracy: 0.8458 - val_loss: 0.6470 - val_accuracy: 0.7246\n",
            "Epoch 770/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4161 - accuracy: 0.8416 - val_loss: 0.6480 - val_accuracy: 0.7488\n",
            "Epoch 771/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3966 - accuracy: 0.8495 - val_loss: 0.6439 - val_accuracy: 0.7488\n",
            "Epoch 772/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4217 - accuracy: 0.8289 - val_loss: 0.6590 - val_accuracy: 0.7198\n",
            "Epoch 773/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4151 - accuracy: 0.8489 - val_loss: 0.6427 - val_accuracy: 0.7488\n",
            "Epoch 774/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3978 - accuracy: 0.8507 - val_loss: 0.6404 - val_accuracy: 0.7488\n",
            "Epoch 775/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4208 - accuracy: 0.8404 - val_loss: 0.6478 - val_accuracy: 0.7391\n",
            "Epoch 776/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4062 - accuracy: 0.8392 - val_loss: 0.6351 - val_accuracy: 0.7440\n",
            "Epoch 777/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4041 - accuracy: 0.8555 - val_loss: 0.6714 - val_accuracy: 0.7198\n",
            "Epoch 778/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4006 - accuracy: 0.8482 - val_loss: 0.6499 - val_accuracy: 0.7488\n",
            "Epoch 779/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4142 - accuracy: 0.8428 - val_loss: 0.6473 - val_accuracy: 0.7536\n",
            "Epoch 780/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3955 - accuracy: 0.8404 - val_loss: 0.6313 - val_accuracy: 0.7536\n",
            "Epoch 781/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3947 - accuracy: 0.8476 - val_loss: 0.6410 - val_accuracy: 0.7585\n",
            "Epoch 782/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4289 - accuracy: 0.8325 - val_loss: 0.6481 - val_accuracy: 0.7295\n",
            "Epoch 783/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3901 - accuracy: 0.8537 - val_loss: 0.6194 - val_accuracy: 0.7633\n",
            "Epoch 784/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3817 - accuracy: 0.8652 - val_loss: 0.6641 - val_accuracy: 0.7343\n",
            "Epoch 785/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4163 - accuracy: 0.8507 - val_loss: 0.6262 - val_accuracy: 0.7681\n",
            "Epoch 786/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3977 - accuracy: 0.8434 - val_loss: 0.6409 - val_accuracy: 0.7585\n",
            "Epoch 787/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3928 - accuracy: 0.8440 - val_loss: 0.6501 - val_accuracy: 0.7246\n",
            "Epoch 788/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3892 - accuracy: 0.8464 - val_loss: 0.6395 - val_accuracy: 0.7391\n",
            "Epoch 789/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4126 - accuracy: 0.8398 - val_loss: 0.6348 - val_accuracy: 0.7343\n",
            "Epoch 790/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3845 - accuracy: 0.8452 - val_loss: 0.6545 - val_accuracy: 0.7246\n",
            "Epoch 791/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3990 - accuracy: 0.8501 - val_loss: 0.6360 - val_accuracy: 0.7391\n",
            "Epoch 792/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4050 - accuracy: 0.8519 - val_loss: 0.6232 - val_accuracy: 0.7729\n",
            "Epoch 793/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3944 - accuracy: 0.8567 - val_loss: 0.6440 - val_accuracy: 0.7295\n",
            "Epoch 794/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3840 - accuracy: 0.8567 - val_loss: 0.6240 - val_accuracy: 0.7729\n",
            "Epoch 795/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4076 - accuracy: 0.8398 - val_loss: 0.6357 - val_accuracy: 0.7343\n",
            "Epoch 796/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4086 - accuracy: 0.8476 - val_loss: 0.6353 - val_accuracy: 0.7246\n",
            "Epoch 797/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4055 - accuracy: 0.8476 - val_loss: 0.6273 - val_accuracy: 0.7633\n",
            "Epoch 798/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3802 - accuracy: 0.8628 - val_loss: 0.6450 - val_accuracy: 0.7343\n",
            "Epoch 799/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3719 - accuracy: 0.8543 - val_loss: 0.6477 - val_accuracy: 0.7440\n",
            "Epoch 800/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3625 - accuracy: 0.8670 - val_loss: 0.6442 - val_accuracy: 0.7536\n",
            "Epoch 801/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3957 - accuracy: 0.8537 - val_loss: 0.6447 - val_accuracy: 0.7536\n",
            "Epoch 802/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3910 - accuracy: 0.8476 - val_loss: 0.6244 - val_accuracy: 0.7536\n",
            "Epoch 803/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3847 - accuracy: 0.8531 - val_loss: 0.6466 - val_accuracy: 0.7295\n",
            "Epoch 804/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3900 - accuracy: 0.8452 - val_loss: 0.6488 - val_accuracy: 0.7440\n",
            "Epoch 805/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3657 - accuracy: 0.8567 - val_loss: 0.6201 - val_accuracy: 0.7633\n",
            "Epoch 806/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3817 - accuracy: 0.8458 - val_loss: 0.6338 - val_accuracy: 0.7826\n",
            "Epoch 807/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3912 - accuracy: 0.8549 - val_loss: 0.6163 - val_accuracy: 0.7681\n",
            "Epoch 808/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3770 - accuracy: 0.8603 - val_loss: 0.6292 - val_accuracy: 0.7778\n",
            "Epoch 809/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3700 - accuracy: 0.8640 - val_loss: 0.6444 - val_accuracy: 0.7681\n",
            "Epoch 810/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3956 - accuracy: 0.8464 - val_loss: 0.6286 - val_accuracy: 0.7681\n",
            "Epoch 811/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3723 - accuracy: 0.8567 - val_loss: 0.6219 - val_accuracy: 0.7488\n",
            "Epoch 812/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.4086 - accuracy: 0.8476 - val_loss: 0.6371 - val_accuracy: 0.7778\n",
            "Epoch 813/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3779 - accuracy: 0.8519 - val_loss: 0.6323 - val_accuracy: 0.7585\n",
            "Epoch 814/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3651 - accuracy: 0.8670 - val_loss: 0.6554 - val_accuracy: 0.7343\n",
            "Epoch 815/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3887 - accuracy: 0.8609 - val_loss: 0.6385 - val_accuracy: 0.7536\n",
            "Epoch 816/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3872 - accuracy: 0.8513 - val_loss: 0.6392 - val_accuracy: 0.7391\n",
            "Epoch 817/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3656 - accuracy: 0.8615 - val_loss: 0.6344 - val_accuracy: 0.7633\n",
            "Epoch 818/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3773 - accuracy: 0.8585 - val_loss: 0.6587 - val_accuracy: 0.7343\n",
            "Epoch 819/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3750 - accuracy: 0.8628 - val_loss: 0.6365 - val_accuracy: 0.7391\n",
            "Epoch 820/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3722 - accuracy: 0.8597 - val_loss: 0.6278 - val_accuracy: 0.7729\n",
            "Epoch 821/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3717 - accuracy: 0.8597 - val_loss: 0.6540 - val_accuracy: 0.7536\n",
            "Epoch 822/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3801 - accuracy: 0.8664 - val_loss: 0.6473 - val_accuracy: 0.7585\n",
            "Epoch 823/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3690 - accuracy: 0.8664 - val_loss: 0.6135 - val_accuracy: 0.7826\n",
            "Epoch 824/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3664 - accuracy: 0.8579 - val_loss: 0.6317 - val_accuracy: 0.7778\n",
            "Epoch 825/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3682 - accuracy: 0.8622 - val_loss: 0.6574 - val_accuracy: 0.7343\n",
            "Epoch 826/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3624 - accuracy: 0.8646 - val_loss: 0.6272 - val_accuracy: 0.7585\n",
            "Epoch 827/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3736 - accuracy: 0.8628 - val_loss: 0.6203 - val_accuracy: 0.7681\n",
            "Epoch 828/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3958 - accuracy: 0.8495 - val_loss: 0.6410 - val_accuracy: 0.7488\n",
            "Epoch 829/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3696 - accuracy: 0.8585 - val_loss: 0.6267 - val_accuracy: 0.7585\n",
            "Epoch 830/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3636 - accuracy: 0.8603 - val_loss: 0.6318 - val_accuracy: 0.7729\n",
            "Epoch 831/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3546 - accuracy: 0.8591 - val_loss: 0.6549 - val_accuracy: 0.7391\n",
            "Epoch 832/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3658 - accuracy: 0.8567 - val_loss: 0.6300 - val_accuracy: 0.7585\n",
            "Epoch 833/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3537 - accuracy: 0.8652 - val_loss: 0.6402 - val_accuracy: 0.7343\n",
            "Epoch 834/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3538 - accuracy: 0.8730 - val_loss: 0.6561 - val_accuracy: 0.7391\n",
            "Epoch 835/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3706 - accuracy: 0.8543 - val_loss: 0.6325 - val_accuracy: 0.7488\n",
            "Epoch 836/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3459 - accuracy: 0.8736 - val_loss: 0.6344 - val_accuracy: 0.7585\n",
            "Epoch 837/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3569 - accuracy: 0.8724 - val_loss: 0.6534 - val_accuracy: 0.7246\n",
            "Epoch 838/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3514 - accuracy: 0.8652 - val_loss: 0.6356 - val_accuracy: 0.7585\n",
            "Epoch 839/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3651 - accuracy: 0.8634 - val_loss: 0.6100 - val_accuracy: 0.7874\n",
            "Epoch 840/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3695 - accuracy: 0.8676 - val_loss: 0.6232 - val_accuracy: 0.7681\n",
            "Epoch 841/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3835 - accuracy: 0.8567 - val_loss: 0.6540 - val_accuracy: 0.7391\n",
            "Epoch 842/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3660 - accuracy: 0.8585 - val_loss: 0.6170 - val_accuracy: 0.7778\n",
            "Epoch 843/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3655 - accuracy: 0.8646 - val_loss: 0.6269 - val_accuracy: 0.7681\n",
            "Epoch 844/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3788 - accuracy: 0.8501 - val_loss: 0.6225 - val_accuracy: 0.7778\n",
            "Epoch 845/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3566 - accuracy: 0.8609 - val_loss: 0.6218 - val_accuracy: 0.7681\n",
            "Epoch 846/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3597 - accuracy: 0.8634 - val_loss: 0.6238 - val_accuracy: 0.7440\n",
            "Epoch 847/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3477 - accuracy: 0.8700 - val_loss: 0.6193 - val_accuracy: 0.7585\n",
            "Epoch 848/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3525 - accuracy: 0.8597 - val_loss: 0.6263 - val_accuracy: 0.7440\n",
            "Epoch 849/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3527 - accuracy: 0.8640 - val_loss: 0.6187 - val_accuracy: 0.7585\n",
            "Epoch 850/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3661 - accuracy: 0.8609 - val_loss: 0.6152 - val_accuracy: 0.7681\n",
            "Epoch 851/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3798 - accuracy: 0.8531 - val_loss: 0.6112 - val_accuracy: 0.7536\n",
            "Epoch 852/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3371 - accuracy: 0.8688 - val_loss: 0.6046 - val_accuracy: 0.7585\n",
            "Epoch 853/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3375 - accuracy: 0.8730 - val_loss: 0.5959 - val_accuracy: 0.7874\n",
            "Epoch 854/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3528 - accuracy: 0.8664 - val_loss: 0.6235 - val_accuracy: 0.7729\n",
            "Epoch 855/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3285 - accuracy: 0.8755 - val_loss: 0.6053 - val_accuracy: 0.7923\n",
            "Epoch 856/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3484 - accuracy: 0.8730 - val_loss: 0.6167 - val_accuracy: 0.7536\n",
            "Epoch 857/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3407 - accuracy: 0.8706 - val_loss: 0.6294 - val_accuracy: 0.7729\n",
            "Epoch 858/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3575 - accuracy: 0.8670 - val_loss: 0.6257 - val_accuracy: 0.7585\n",
            "Epoch 859/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3483 - accuracy: 0.8694 - val_loss: 0.6342 - val_accuracy: 0.7729\n",
            "Epoch 860/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3339 - accuracy: 0.8839 - val_loss: 0.6488 - val_accuracy: 0.7536\n",
            "Epoch 861/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3300 - accuracy: 0.8833 - val_loss: 0.6199 - val_accuracy: 0.7681\n",
            "Epoch 862/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3560 - accuracy: 0.8567 - val_loss: 0.6169 - val_accuracy: 0.7729\n",
            "Epoch 863/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3444 - accuracy: 0.8718 - val_loss: 0.6315 - val_accuracy: 0.7536\n",
            "Epoch 864/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3307 - accuracy: 0.8773 - val_loss: 0.6181 - val_accuracy: 0.7585\n",
            "Epoch 865/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3626 - accuracy: 0.8555 - val_loss: 0.6160 - val_accuracy: 0.7826\n",
            "Epoch 866/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3505 - accuracy: 0.8706 - val_loss: 0.6505 - val_accuracy: 0.7488\n",
            "Epoch 867/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3482 - accuracy: 0.8658 - val_loss: 0.6106 - val_accuracy: 0.7778\n",
            "Epoch 868/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3370 - accuracy: 0.8640 - val_loss: 0.6308 - val_accuracy: 0.7488\n",
            "Epoch 869/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3415 - accuracy: 0.8670 - val_loss: 0.6212 - val_accuracy: 0.7681\n",
            "Epoch 870/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3449 - accuracy: 0.8718 - val_loss: 0.6393 - val_accuracy: 0.7488\n",
            "Epoch 871/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3438 - accuracy: 0.8652 - val_loss: 0.6525 - val_accuracy: 0.7343\n",
            "Epoch 872/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3366 - accuracy: 0.8755 - val_loss: 0.6120 - val_accuracy: 0.7826\n",
            "Epoch 873/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3452 - accuracy: 0.8664 - val_loss: 0.5994 - val_accuracy: 0.7971\n",
            "Epoch 874/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3459 - accuracy: 0.8779 - val_loss: 0.6140 - val_accuracy: 0.7633\n",
            "Epoch 875/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3390 - accuracy: 0.8791 - val_loss: 0.6140 - val_accuracy: 0.7585\n",
            "Epoch 876/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3249 - accuracy: 0.8742 - val_loss: 0.6249 - val_accuracy: 0.7536\n",
            "Epoch 877/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3506 - accuracy: 0.8706 - val_loss: 0.6189 - val_accuracy: 0.7681\n",
            "Epoch 878/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3370 - accuracy: 0.8706 - val_loss: 0.6089 - val_accuracy: 0.7778\n",
            "Epoch 879/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3641 - accuracy: 0.8682 - val_loss: 0.5945 - val_accuracy: 0.7633\n",
            "Epoch 880/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3368 - accuracy: 0.8718 - val_loss: 0.6438 - val_accuracy: 0.7246\n",
            "Epoch 881/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3335 - accuracy: 0.8670 - val_loss: 0.6059 - val_accuracy: 0.7536\n",
            "Epoch 882/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3388 - accuracy: 0.8591 - val_loss: 0.6019 - val_accuracy: 0.7874\n",
            "Epoch 883/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3211 - accuracy: 0.8815 - val_loss: 0.6009 - val_accuracy: 0.7971\n",
            "Epoch 884/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3138 - accuracy: 0.8833 - val_loss: 0.6052 - val_accuracy: 0.7874\n",
            "Epoch 885/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3453 - accuracy: 0.8736 - val_loss: 0.6122 - val_accuracy: 0.7585\n",
            "Epoch 886/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3272 - accuracy: 0.8791 - val_loss: 0.5917 - val_accuracy: 0.7874\n",
            "Epoch 887/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3280 - accuracy: 0.8736 - val_loss: 0.6147 - val_accuracy: 0.7778\n",
            "Epoch 888/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3157 - accuracy: 0.8755 - val_loss: 0.6206 - val_accuracy: 0.7633\n",
            "Epoch 889/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3304 - accuracy: 0.8724 - val_loss: 0.6327 - val_accuracy: 0.7681\n",
            "Epoch 890/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3414 - accuracy: 0.8634 - val_loss: 0.6254 - val_accuracy: 0.7391\n",
            "Epoch 891/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3323 - accuracy: 0.8761 - val_loss: 0.6398 - val_accuracy: 0.7391\n",
            "Epoch 892/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3270 - accuracy: 0.8736 - val_loss: 0.6122 - val_accuracy: 0.7633\n",
            "Epoch 893/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3215 - accuracy: 0.8894 - val_loss: 0.6191 - val_accuracy: 0.7391\n",
            "Epoch 894/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3393 - accuracy: 0.8694 - val_loss: 0.6082 - val_accuracy: 0.7778\n",
            "Epoch 895/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3201 - accuracy: 0.8791 - val_loss: 0.5989 - val_accuracy: 0.7585\n",
            "Epoch 896/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3206 - accuracy: 0.8900 - val_loss: 0.6171 - val_accuracy: 0.7633\n",
            "Epoch 897/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3327 - accuracy: 0.8724 - val_loss: 0.6103 - val_accuracy: 0.7633\n",
            "Epoch 898/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3362 - accuracy: 0.8730 - val_loss: 0.6065 - val_accuracy: 0.7633\n",
            "Epoch 899/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3286 - accuracy: 0.8785 - val_loss: 0.6287 - val_accuracy: 0.7536\n",
            "Epoch 900/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3319 - accuracy: 0.8803 - val_loss: 0.6183 - val_accuracy: 0.7488\n",
            "Epoch 901/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3093 - accuracy: 0.8863 - val_loss: 0.6384 - val_accuracy: 0.7343\n",
            "Epoch 902/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3459 - accuracy: 0.8694 - val_loss: 0.6370 - val_accuracy: 0.7246\n",
            "Epoch 903/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3313 - accuracy: 0.8779 - val_loss: 0.5893 - val_accuracy: 0.7971\n",
            "Epoch 904/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3333 - accuracy: 0.8809 - val_loss: 0.6180 - val_accuracy: 0.7536\n",
            "Epoch 905/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3134 - accuracy: 0.8881 - val_loss: 0.6092 - val_accuracy: 0.7440\n",
            "Epoch 906/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3035 - accuracy: 0.8881 - val_loss: 0.6015 - val_accuracy: 0.7633\n",
            "Epoch 907/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3200 - accuracy: 0.8761 - val_loss: 0.6109 - val_accuracy: 0.7681\n",
            "Epoch 908/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3160 - accuracy: 0.8875 - val_loss: 0.5985 - val_accuracy: 0.7826\n",
            "Epoch 909/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3210 - accuracy: 0.8748 - val_loss: 0.6074 - val_accuracy: 0.7729\n",
            "Epoch 910/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3052 - accuracy: 0.8954 - val_loss: 0.6248 - val_accuracy: 0.7488\n",
            "Epoch 911/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3163 - accuracy: 0.8827 - val_loss: 0.6224 - val_accuracy: 0.7343\n",
            "Epoch 912/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3439 - accuracy: 0.8676 - val_loss: 0.6079 - val_accuracy: 0.7536\n",
            "Epoch 913/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3207 - accuracy: 0.8785 - val_loss: 0.6010 - val_accuracy: 0.7681\n",
            "Epoch 914/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3422 - accuracy: 0.8748 - val_loss: 0.6141 - val_accuracy: 0.7585\n",
            "Epoch 915/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3200 - accuracy: 0.8815 - val_loss: 0.6126 - val_accuracy: 0.7778\n",
            "Epoch 916/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.2899 - accuracy: 0.8936 - val_loss: 0.6133 - val_accuracy: 0.7488\n",
            "Epoch 917/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3128 - accuracy: 0.8791 - val_loss: 0.6159 - val_accuracy: 0.7488\n",
            "Epoch 918/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3117 - accuracy: 0.8827 - val_loss: 0.5963 - val_accuracy: 0.7826\n",
            "Epoch 919/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.2957 - accuracy: 0.8906 - val_loss: 0.5979 - val_accuracy: 0.7778\n",
            "Epoch 920/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3043 - accuracy: 0.8863 - val_loss: 0.5940 - val_accuracy: 0.7923\n",
            "Epoch 921/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3185 - accuracy: 0.8857 - val_loss: 0.6133 - val_accuracy: 0.7633\n",
            "Epoch 922/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3312 - accuracy: 0.8742 - val_loss: 0.5831 - val_accuracy: 0.7729\n",
            "Epoch 923/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3104 - accuracy: 0.8821 - val_loss: 0.6024 - val_accuracy: 0.7778\n",
            "Epoch 924/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3136 - accuracy: 0.8791 - val_loss: 0.5993 - val_accuracy: 0.7633\n",
            "Epoch 925/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3051 - accuracy: 0.8839 - val_loss: 0.5839 - val_accuracy: 0.7778\n",
            "Epoch 926/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3067 - accuracy: 0.8791 - val_loss: 0.6009 - val_accuracy: 0.7681\n",
            "Epoch 927/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3041 - accuracy: 0.8894 - val_loss: 0.5882 - val_accuracy: 0.7826\n",
            "Epoch 928/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.2851 - accuracy: 0.8948 - val_loss: 0.6038 - val_accuracy: 0.7874\n",
            "Epoch 929/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.2966 - accuracy: 0.8996 - val_loss: 0.6232 - val_accuracy: 0.7633\n",
            "Epoch 930/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3126 - accuracy: 0.8875 - val_loss: 0.5999 - val_accuracy: 0.7633\n",
            "Epoch 931/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.2982 - accuracy: 0.8912 - val_loss: 0.5923 - val_accuracy: 0.7826\n",
            "Epoch 932/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.3052 - accuracy: 0.8839 - val_loss: 0.5973 - val_accuracy: 0.7729\n",
            "Epoch 933/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3129 - accuracy: 0.8845 - val_loss: 0.5949 - val_accuracy: 0.7778\n",
            "Epoch 934/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3156 - accuracy: 0.8718 - val_loss: 0.5908 - val_accuracy: 0.7778\n",
            "Epoch 935/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.2895 - accuracy: 0.8972 - val_loss: 0.5960 - val_accuracy: 0.7585\n",
            "Epoch 936/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3073 - accuracy: 0.8833 - val_loss: 0.5808 - val_accuracy: 0.8019\n",
            "Epoch 937/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.2943 - accuracy: 0.8845 - val_loss: 0.5819 - val_accuracy: 0.7778\n",
            "Epoch 938/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.2824 - accuracy: 0.8978 - val_loss: 0.6257 - val_accuracy: 0.7536\n",
            "Epoch 939/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.2871 - accuracy: 0.8960 - val_loss: 0.5940 - val_accuracy: 0.7778\n",
            "Epoch 940/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.2990 - accuracy: 0.8803 - val_loss: 0.6069 - val_accuracy: 0.7971\n",
            "Epoch 941/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.2791 - accuracy: 0.8966 - val_loss: 0.6080 - val_accuracy: 0.7440\n",
            "Epoch 942/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3025 - accuracy: 0.8888 - val_loss: 0.5832 - val_accuracy: 0.7971\n",
            "Epoch 943/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 0.2846 - accuracy: 0.8912 - val_loss: 0.6079 - val_accuracy: 0.7440\n",
            "Epoch 944/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 0.2877 - accuracy: 0.8875 - val_loss: 0.6106 - val_accuracy: 0.7633\n",
            "Epoch 945/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 0.3021 - accuracy: 0.8881 - val_loss: 0.5919 - val_accuracy: 0.7923\n",
            "Epoch 946/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 0.2945 - accuracy: 0.8936 - val_loss: 0.6054 - val_accuracy: 0.7633\n",
            "Epoch 947/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 0.2923 - accuracy: 0.8875 - val_loss: 0.6105 - val_accuracy: 0.7681\n",
            "Epoch 948/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 0.2973 - accuracy: 0.8851 - val_loss: 0.5820 - val_accuracy: 0.7778\n",
            "Epoch 949/1000\n",
            "104/104 [==============================] - 1s 7ms/step - loss: 0.3017 - accuracy: 0.8845 - val_loss: 0.6202 - val_accuracy: 0.7681\n",
            "Epoch 950/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.2729 - accuracy: 0.9045 - val_loss: 0.5968 - val_accuracy: 0.7585\n",
            "Epoch 951/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.2999 - accuracy: 0.8894 - val_loss: 0.6156 - val_accuracy: 0.7729\n",
            "Epoch 952/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.2996 - accuracy: 0.8785 - val_loss: 0.5928 - val_accuracy: 0.7536\n",
            "Epoch 953/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.2877 - accuracy: 0.9039 - val_loss: 0.5778 - val_accuracy: 0.7778\n",
            "Epoch 954/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.2869 - accuracy: 0.8978 - val_loss: 0.5923 - val_accuracy: 0.7826\n",
            "Epoch 955/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.3118 - accuracy: 0.8779 - val_loss: 0.6202 - val_accuracy: 0.7536\n",
            "Epoch 956/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.2906 - accuracy: 0.8918 - val_loss: 0.6046 - val_accuracy: 0.7729\n",
            "Epoch 957/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.2877 - accuracy: 0.8930 - val_loss: 0.6053 - val_accuracy: 0.7585\n",
            "Epoch 958/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.2927 - accuracy: 0.8881 - val_loss: 0.5968 - val_accuracy: 0.7633\n",
            "Epoch 959/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.2824 - accuracy: 0.9063 - val_loss: 0.6063 - val_accuracy: 0.7874\n",
            "Epoch 960/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.2926 - accuracy: 0.8894 - val_loss: 0.5960 - val_accuracy: 0.7729\n",
            "Epoch 961/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.2815 - accuracy: 0.8930 - val_loss: 0.5893 - val_accuracy: 0.7923\n",
            "Epoch 962/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.2971 - accuracy: 0.8894 - val_loss: 0.6066 - val_accuracy: 0.7826\n",
            "Epoch 963/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.2904 - accuracy: 0.8930 - val_loss: 0.5981 - val_accuracy: 0.7778\n",
            "Epoch 964/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.2811 - accuracy: 0.8978 - val_loss: 0.6064 - val_accuracy: 0.7923\n",
            "Epoch 965/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.2976 - accuracy: 0.8960 - val_loss: 0.6155 - val_accuracy: 0.7681\n",
            "Epoch 966/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.2781 - accuracy: 0.9069 - val_loss: 0.5995 - val_accuracy: 0.7681\n",
            "Epoch 967/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.2746 - accuracy: 0.8966 - val_loss: 0.6198 - val_accuracy: 0.7874\n",
            "Epoch 968/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.2763 - accuracy: 0.8912 - val_loss: 0.6181 - val_accuracy: 0.7585\n",
            "Epoch 969/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.2873 - accuracy: 0.8996 - val_loss: 0.5716 - val_accuracy: 0.7971\n",
            "Epoch 970/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.2923 - accuracy: 0.8894 - val_loss: 0.5918 - val_accuracy: 0.7874\n",
            "Epoch 971/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.2710 - accuracy: 0.8990 - val_loss: 0.5879 - val_accuracy: 0.7681\n",
            "Epoch 972/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.2572 - accuracy: 0.9093 - val_loss: 0.5849 - val_accuracy: 0.7778\n",
            "Epoch 973/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.2953 - accuracy: 0.8863 - val_loss: 0.5886 - val_accuracy: 0.7826\n",
            "Epoch 974/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.2901 - accuracy: 0.8839 - val_loss: 0.5913 - val_accuracy: 0.7778\n",
            "Epoch 975/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.2652 - accuracy: 0.9033 - val_loss: 0.5885 - val_accuracy: 0.7826\n",
            "Epoch 976/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.2636 - accuracy: 0.9063 - val_loss: 0.5845 - val_accuracy: 0.7923\n",
            "Epoch 977/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.2842 - accuracy: 0.8948 - val_loss: 0.5731 - val_accuracy: 0.7923\n",
            "Epoch 978/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.2731 - accuracy: 0.9008 - val_loss: 0.5797 - val_accuracy: 0.7874\n",
            "Epoch 979/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.2862 - accuracy: 0.8888 - val_loss: 0.6141 - val_accuracy: 0.7585\n",
            "Epoch 980/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.2849 - accuracy: 0.8924 - val_loss: 0.5655 - val_accuracy: 0.8068\n",
            "Epoch 981/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.2687 - accuracy: 0.9027 - val_loss: 0.5797 - val_accuracy: 0.7971\n",
            "Epoch 982/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.2934 - accuracy: 0.8948 - val_loss: 0.5750 - val_accuracy: 0.7874\n",
            "Epoch 983/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.2871 - accuracy: 0.8900 - val_loss: 0.5779 - val_accuracy: 0.7923\n",
            "Epoch 984/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.2716 - accuracy: 0.9051 - val_loss: 0.5921 - val_accuracy: 0.7536\n",
            "Epoch 985/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.2564 - accuracy: 0.9117 - val_loss: 0.5824 - val_accuracy: 0.7923\n",
            "Epoch 986/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.2820 - accuracy: 0.8936 - val_loss: 0.6022 - val_accuracy: 0.7391\n",
            "Epoch 987/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.2695 - accuracy: 0.8990 - val_loss: 0.6040 - val_accuracy: 0.7681\n",
            "Epoch 988/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.2790 - accuracy: 0.8984 - val_loss: 0.5844 - val_accuracy: 0.7585\n",
            "Epoch 989/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.2603 - accuracy: 0.9033 - val_loss: 0.5934 - val_accuracy: 0.7826\n",
            "Epoch 990/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.2755 - accuracy: 0.8990 - val_loss: 0.5990 - val_accuracy: 0.7585\n",
            "Epoch 991/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.2761 - accuracy: 0.9015 - val_loss: 0.6043 - val_accuracy: 0.7729\n",
            "Epoch 992/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.2609 - accuracy: 0.9063 - val_loss: 0.5912 - val_accuracy: 0.7681\n",
            "Epoch 993/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.2735 - accuracy: 0.9033 - val_loss: 0.6016 - val_accuracy: 0.7536\n",
            "Epoch 994/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.2695 - accuracy: 0.8978 - val_loss: 0.5940 - val_accuracy: 0.7681\n",
            "Epoch 995/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.2805 - accuracy: 0.9002 - val_loss: 0.5904 - val_accuracy: 0.7633\n",
            "Epoch 996/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.2681 - accuracy: 0.8984 - val_loss: 0.5857 - val_accuracy: 0.7536\n",
            "Epoch 997/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.2683 - accuracy: 0.9045 - val_loss: 0.6003 - val_accuracy: 0.7778\n",
            "Epoch 998/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.2641 - accuracy: 0.9045 - val_loss: 0.5893 - val_accuracy: 0.7585\n",
            "Epoch 999/1000\n",
            "104/104 [==============================] - 1s 5ms/step - loss: 0.2638 - accuracy: 0.9069 - val_loss: 0.5792 - val_accuracy: 0.7923\n",
            "Epoch 1000/1000\n",
            "104/104 [==============================] - 1s 6ms/step - loss: 0.2698 - accuracy: 0.8978 - val_loss: 0.6059 - val_accuracy: 0.7488\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(cnnhistory.history['loss'])\n",
        "plt.plot(cnnhistory.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train','test'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "oQYnuaCrDH_A",
        "outputId": "2c6c9793-df93-4d29-b831-ba495b735ac2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(cnnhistory.history['accuracy'])\n",
        "plt.plot(cnnhistory.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train','test'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "IFkTuO8nDNdq",
        "outputId": "f39ef837-5ce1-4024-b975-f51e147fa1c3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#predictions = model.predict_classes(x_testcnn)\n",
        "predictions = model.predict(x_testcnn)"
      ],
      "metadata": {
        "id": "A2xtrN5wDQgY"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_f_ohjXKDUpY",
        "outputId": "4ba2a496-a278-4ed7-c413-7c243e614358"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.8646485e-04, 8.2860261e-06, 9.3280748e-02, 5.0019319e-03,\n",
              "        1.8907925e-01, 7.1244329e-01],\n",
              "       [6.3410699e-02, 9.1343153e-01, 3.6946712e-03, 8.5271960e-03,\n",
              "        7.3273042e-03, 3.6085676e-03],\n",
              "       [1.7346000e-06, 2.8157714e-05, 9.9121022e-01, 1.5056925e-05,\n",
              "        6.8813562e-03, 1.8635107e-03],\n",
              "       ...,\n",
              "       [3.2399232e-05, 3.6688388e-07, 3.3637459e-04, 3.7726547e-04,\n",
              "        9.7002274e-01, 2.9230896e-02],\n",
              "       [1.0864719e-02, 3.1233041e-03, 1.8024234e-02, 5.8855397e-01,\n",
              "        3.0086342e-02, 3.4934744e-01],\n",
              "       [1.3154703e-02, 5.8075013e-03, 7.1558595e-02, 1.7980736e-02,\n",
              "        8.9113051e-01, 3.6794713e-04]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmA2bFgsDW3D",
        "outputId": "a926bdc0-cf83-430a-e6ef-79fd2655305f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 1, 2, 5, 3, 4, 2, 0, 3, 1, 3, 2, 1, 4, 3, 0, 5, 0, 2, 5, 0, 0,\n",
              "       1, 3, 2, 3, 2, 5, 4, 2, 2, 3, 1, 2, 5, 5, 3, 5, 3, 3, 4, 1, 3, 2,\n",
              "       3, 1, 5, 3, 1, 2, 4, 4, 5, 5, 5, 1, 2, 3, 2, 5, 0, 2, 4, 2, 5, 4,\n",
              "       3, 0, 1, 2, 5, 2, 1, 2, 4, 5, 4, 1, 5, 5, 3, 1, 5, 3, 2, 4, 2, 5,\n",
              "       0, 5, 0, 5, 4, 0, 2, 4, 0, 2, 2, 0, 2, 3, 5, 3, 5, 5, 3, 1, 2, 2,\n",
              "       2, 1, 3, 2, 1, 3, 4, 4, 1, 1, 2, 4, 4, 1, 4, 2, 0, 4, 5, 5, 3, 1,\n",
              "       2, 1, 5, 5, 1, 2, 3, 3, 0, 3, 3, 0, 1, 0, 1, 2, 1, 5, 1, 2, 2, 0,\n",
              "       1, 1, 1, 1, 1, 4, 0, 1, 5, 1, 4, 5, 5, 4, 1, 0, 3, 5, 2, 3, 0, 4,\n",
              "       1, 3, 1, 0, 5, 5, 3, 5, 4, 5, 4, 0, 4, 1, 2, 1, 5, 0, 4, 3, 2, 1,\n",
              "       0, 3, 5, 0, 4, 5, 4, 5, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_Ytest = y_test.astype(int)"
      ],
      "metadata": {
        "id": "0PHDKWJWDY3g"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_Ytest"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k68v2i-pDbZE",
        "outputId": "0593269d-4912-4fe2-c1fe-0c894e364f6a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 1, 2, 5, 3, 4, 2, 0, 3, 1, 3, 2, 1, 4, 3, 0, 5, 0, 2, 5, 0, 0,\n",
              "       1, 3, 2, 3, 2, 5, 4, 2, 2, 3, 1, 2, 5, 5, 3, 5, 3, 3, 4, 1, 3, 2,\n",
              "       3, 1, 5, 3, 1, 2, 4, 4, 5, 5, 5, 1, 2, 3, 2, 5, 0, 2, 4, 2, 5, 4,\n",
              "       3, 0, 1, 2, 5, 2, 1, 2, 4, 5, 4, 1, 5, 5, 3, 1, 5, 3, 2, 4, 2, 5,\n",
              "       0, 5, 0, 5, 4, 0, 2, 4, 0, 2, 2, 0, 2, 3, 5, 3, 5, 5, 3, 1, 2, 2,\n",
              "       2, 1, 3, 2, 1, 3, 4, 4, 1, 1, 2, 4, 4, 1, 4, 2, 0, 4, 5, 5, 3, 1,\n",
              "       2, 1, 5, 5, 1, 2, 3, 3, 0, 3, 3, 0, 1, 0, 1, 2, 1, 5, 1, 2, 2, 0,\n",
              "       1, 1, 1, 1, 1, 4, 0, 1, 5, 1, 4, 5, 5, 4, 1, 0, 3, 5, 2, 3, 0, 4,\n",
              "       1, 3, 1, 0, 5, 5, 3, 5, 4, 5, 4, 0, 4, 1, 2, 1, 5, 0, 4, 3, 2, 1,\n",
              "       0, 3, 5, 0, 4, 5, 4, 5, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds1=predictions.argmax(axis=1)\n",
        "preds1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_cAI39VFhtp",
        "outputId": "e09aa67f-ba37-48ac-d413-e355842300a2"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 1, 2, 5, 3, 4, 2, 0, 3, 0, 3, 2, 1, 4, 3, 0, 5, 3, 2, 5, 2, 1,\n",
              "       1, 3, 2, 5, 2, 5, 2, 4, 2, 3, 1, 2, 5, 4, 1, 5, 3, 2, 4, 1, 4, 2,\n",
              "       3, 1, 3, 3, 1, 4, 4, 4, 5, 2, 4, 1, 4, 3, 2, 5, 0, 2, 4, 4, 5, 4,\n",
              "       0, 0, 1, 2, 3, 2, 1, 2, 4, 3, 4, 3, 5, 5, 5, 1, 5, 3, 2, 4, 2, 3,\n",
              "       0, 5, 0, 5, 4, 0, 3, 4, 3, 4, 2, 0, 2, 3, 5, 3, 5, 5, 5, 1, 2, 2,\n",
              "       2, 1, 3, 2, 1, 3, 4, 4, 1, 5, 2, 4, 4, 1, 4, 2, 2, 4, 4, 5, 5, 1,\n",
              "       2, 1, 5, 5, 1, 3, 3, 3, 0, 3, 5, 0, 1, 0, 1, 2, 1, 5, 0, 2, 4, 0,\n",
              "       1, 4, 1, 1, 2, 4, 0, 1, 5, 1, 4, 5, 5, 2, 1, 0, 5, 1, 2, 3, 3, 4,\n",
              "       1, 3, 1, 5, 5, 2, 3, 5, 4, 5, 4, 0, 4, 0, 2, 1, 5, 0, 4, 3, 2, 1,\n",
              "       1, 5, 5, 0, 4, 5, 4, 3, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "abc = preds1.astype(int).flatten()"
      ],
      "metadata": {
        "id": "H4WEmM60NfQn"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "c = confusion_matrix(new_Ytest, abc) \n",
        "c"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKEBeEBlFjB8",
        "outputId": "abbb9b8f-97f2-4ff3-e3d0-2b19ff682fa8"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[17,  2,  2,  3,  0,  1],\n",
              "       [ 3, 32,  1,  1,  1,  1],\n",
              "       [ 0,  0, 30,  2,  6,  0],\n",
              "       [ 1,  1,  1, 22,  1,  7],\n",
              "       [ 0,  0,  2,  0, 28,  0],\n",
              "       [ 0,  1,  2,  5,  3, 31]])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4RjG7LWLSQx",
        "outputId": "641dff05-99ee-4c46-96c6-1937c0fc1d5d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 1, 2, 5, 3, 4, 2, 0, 3, 1, 3, 2, 1, 4, 3, 0, 5, 0, 2, 5, 0, 0,\n",
              "       1, 3, 2, 3, 2, 5, 4, 2, 2, 3, 1, 2, 5, 5, 3, 5, 3, 3, 4, 1, 3, 2,\n",
              "       3, 1, 5, 3, 1, 2, 4, 4, 5, 5, 5, 1, 2, 3, 2, 5, 0, 2, 4, 2, 5, 4,\n",
              "       3, 0, 1, 2, 5, 2, 1, 2, 4, 5, 4, 1, 5, 5, 3, 1, 5, 3, 2, 4, 2, 5,\n",
              "       0, 5, 0, 5, 4, 0, 2, 4, 0, 2, 2, 0, 2, 3, 5, 3, 5, 5, 3, 1, 2, 2,\n",
              "       2, 1, 3, 2, 1, 3, 4, 4, 1, 1, 2, 4, 4, 1, 4, 2, 0, 4, 5, 5, 3, 1,\n",
              "       2, 1, 5, 5, 1, 2, 3, 3, 0, 3, 3, 0, 1, 0, 1, 2, 1, 5, 1, 2, 2, 0,\n",
              "       1, 1, 1, 1, 1, 4, 0, 1, 5, 1, 4, 5, 5, 4, 1, 0, 3, 5, 2, 3, 0, 4,\n",
              "       1, 3, 1, 0, 5, 5, 3, 5, 4, 5, 4, 0, 4, 1, 2, 1, 5, 0, 4, 3, 2, 1,\n",
              "       0, 3, 5, 0, 4, 5, 4, 5, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/drive/My Drive/graduation project/audio/paper_code/mariam/original_500 epoch_with_valid_Adam2_3')\n",
        "print(\"MODEL SAVED\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsJBNeMWDl6Z",
        "outputId": "13a6d8b9-b2b9-4ca8-9ff8-967e588e58d1"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/graduation project/audio/paper_code/mariam/original_500 epoch_with_valid_Adam2_3/assets\n",
            "MODEL SAVED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_model=keras.models.load_model('/content/drive/My Drive/graduation project/audio/paper_code/mariam/original_500 epoch_with_valid_Adam2_3')\n",
        "new_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3KjaO9GDoNl",
        "outputId": "1deb3b43-58ac-4a2d-cac5-0cea052e42b9"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d (Conv1D)             (None, 40, 128)           1152      \n",
            "                                                                 \n",
            " activation (Activation)     (None, 40, 128)           0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 40, 128)           0         \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, 8, 128)           0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 8, 256)            262400    \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 8, 256)            0         \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 8, 256)            0         \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPooling  (None, 1, 256)           0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               65792     \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 256)               65792     \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 256)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 6)                 1542      \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 6)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 396,678\n",
            "Trainable params: 396,678\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, acc =model.evaluate(x_testcnn, y_test)\n",
        "print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3kzoqPkDqqS",
        "outputId": "daf34cd3-541e-41a9-a478-5029a2de72c8"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7/7 [==============================] - 0s 3ms/step - loss: 0.6341 - accuracy: 0.7729\n",
            "Restored model, accuracy: 77.29%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, acc =model.evaluate(X_train, y_train)\n",
        "print(\"Restored model train, accuracy: {:5.2f}%\".format(100*acc))"
      ],
      "metadata": {
        "id": "fJDTGH_OUX0z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b10c7d8e-31c9-465c-d977-42e5eae1db19"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "52/52 [==============================] - 0s 4ms/step - loss: 0.2189 - accuracy: 0.9462\n",
            "Restored model train, accuracy: 94.62%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix,accuracy_score\n",
        "import seaborn as sn\n",
        "\n",
        "print(classification_report(new_Ytest,abc))\n",
        "\n",
        "acc = float(accuracy_score(new_Ytest,abc))*100\n",
        "print(\"----accuracy score %s ----\" % acc)\n",
        "\n",
        "cm = confusion_matrix(new_Ytest,abc)\n",
        "#df_cm = pd.DataFrame(cm)\n",
        "class_names = ['neutral','calm', 'happy','sad','angry', 'fearful' ]\n",
        "df_cm = pd.DataFrame(cm, index=class_names, columns=class_names,)\n",
        "sn.heatmap(df_cm, annot=True, fmt='')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        },
        "id": "SfSC3El94LZg",
        "outputId": "c719224a-7fc3-4d24-a089-49987d11d372"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.68      0.74        25\n",
            "           1       0.89      0.82      0.85        39\n",
            "           2       0.79      0.79      0.79        38\n",
            "           3       0.67      0.67      0.67        33\n",
            "           4       0.72      0.93      0.81        30\n",
            "           5       0.78      0.74      0.76        42\n",
            "\n",
            "    accuracy                           0.77       207\n",
            "   macro avg       0.77      0.77      0.77       207\n",
            "weighted avg       0.78      0.77      0.77       207\n",
            "\n",
            "----accuracy score 77.29468599033817 ----\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVoAAAD4CAYAAACt8i4nAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3daZgU1fn38e/dswDDIqvsCgjGDQUExB2MCxoFY5RojEtMwt9EEzUu8TEkcdeQiIIaFQIKAhpEDYtERcSAKAIiyDLCyCIy7Mo6CMx038+LqsEWZqarm66urvH+cNVFd3VV9W9qes6cOXXqHFFVjDHG+CcSdABjjKnurKA1xhifWUFrjDE+s4LWGGN8ZgWtMcb4LNfvNyjscFGoujWcs3l10BG+F+rk1Qo6QlJWbd8QdISkFeTVCDpC0naUrJRDPUbplpWey5y8xu0O+f28sBqtMcb4zPcarTHGZFQsGnSCg1hBa4ypXqJlQSc4SJUFrYjsBCpq7xBAVbWeL6mMMSZFqrGgIxykyoJWVetmKogxxqRFLGQF7YFE5HCgZvlzVV2T9kTGGHMowlajLScifYDHgBbAJuBIoBA43r9oxhiTgiy8GOa1e9cDQA9guaq2BX4IzPYtlTHGpEpj3pcM8dp0UKqqX4lIREQiqjpdRJ7wNZkxxqRAw9brIM42EakDzADGiMgmoMS/WMYYk6IsvBjmtemgL7AbuA14E1gBXOJXKGOMSVmamg5EpKaIzBGRhSKyRETuc9e3FZGPRORzEfm3iOQnipSwoBWRHGCyqsZUtUxVR6rqEFX9yuvXbYwxGROLel+qthc4R1VPAjoBvUWkB/A34HFVbQ9sBX6Z6EAJC1pVjQIxETks0bbGGBO4NNVo1bHLfZrnLgqcA4x3148ELk0UyWsb7S5gkYhMJa5tVlV/73H/Q9L8kVup06s7ZV9tY9WPfgtAyyfuJr9dSwAidesQ27mLVX1+l4k4SWvRshlDnn2EJk0ao6qMHjmOfz07OuhYlQpbXoD8GvmMnTiM/Px8cnNzeHPSNIYMfC7oWAldcH5PBg26n5xIhBHPv8TAvz8ddKQqPf3M3+h9YS82b/6KHt0uDDpOxZK4GCYi/YH+cauGqurQuNdzgI+B9sDTOM2m21S1/E3WAi0TvY/XgvY1d4mXseEPt732DltfnETzv9++f13xrY/uf3z43b8itit7r82VlZVx34CBLFpYSO06Bbz13nhmTP+Q5ctWBB2tQmHLC7Bv7z6uvexGdpd8Q25uLi9PHs6MabNY8PHioKNVKhKJMGTwQ/S+6CrWrl3P7A+nMGny2xQWFgUdrVJjRo9n6HOjeG7YP4KOUrkkLoa5herQKl6PAp1EpD7wOnBMKpG8Xgyr77bN7l+ABqm8YSq+mbuY6Padlb5e76Iz2T7pf5mKk7RNG7ewaGEhACW7dlO0fCXNmh8ecKrKhS1vud0l3wCQm5dLbl4u2T7Bc/dunVmxYjWrVq2htLSUceMm0OeSC4KOVaUPZs1l69fbgo5RJdWo58X7MXUbMB04FagvIuWV1FZAcaL9vRa011Ww7nqP+/qqVrcTKNuyjdIv1gUdxZNWR7SgY8djmf/xp0FH8SRMeSORCBOnj2V24VRmvTebhfOztzYLThPNl2u//dyuLV5PixbNAkxUTaSv10ETtyaLiNQCzsO5I3Y6cLm72XXAhESRqixoReQqEZkEtBWRiXHLdODrKvbrLyLzRGTeuO3+Dodw2MVns2Pye76+R7oU1C5g+KjB/OWeR9i1M3ubOsqFLW8sFqNPr59x5okXcmKXE+hwzFFBRzJBiMW8L1VrDkwXkU+BucBUVZ0M/BH4g4h8DjQChic6UKI22g+A9UBjnLEOyu0EKq3ixLd7+DqVTU6EuuefxqofZ+Sa3CHJzc1l+KgneO2VyUyZ9E7QcRIKW954O3fs4qP353HWOadR9Fn2tiuvK95A61Yt9j9v1bI569aFb8qcrJOmW2tV9VOgcwXrVwLdkzlWomESvwC+wGmXyDq1T+vM3pVrKduQ/V16Bz31AEXLV/Lc0yODjuJJ2PI2bFSf0tIydu7YRY2aNTit5ykMG5Ld2efOW0D79m1p06Y1xcUb6NevL9dce1PQscIvWhp0goN4Hb0rfgDwfJz+ZCWZGvi7xeN3Ubv7ieQ0qEf7maPYPHg028e/Tb2Lz2LH5Oy9CFaue48uXHFlX5YuWcbUmU7njUfuf4J3p84IOFnFwpYXoEnTxgx86j4ikRwiEeG/E95h+tSZQceqUjQa5ZZbBzDljbHkRCK8MPLfLF26POhYVRrxwmDOOPMUGjVqQOHyWTz84GBeHDUu6FjflYW34IomeWlWRATnltweqnp3ou1tFlxTEZsF13/f11lw93z4kucyp+apV2XnLLju3RL/AbK7H4ox5vspfRfD0sZr08FlcU8jQFdgjy+JjDHmUGRh04HXO8PiR+oqA1bjNB8YY0xW0bBeDFPVX/gdxBhj0iIL5wzz1EYrIkeLyDQRWew+P1FEBvgbzRhjUpCFbbReL4YNA/4fUAr7O/Je6VcoY4xJWYjnDCtQ1TlOz679sm9iHmOMCfHFsC0ichTuTQsicjnOrbnGGJNdsrCN1mtBexPO2AXHiEgxsAq42rdUxhiTqrLs+2Pba0FbDDyPMzxYQ2AHzvBg9/uUyxhjUhPiGu0EYBswHwjHwK/GmO+nELfRtlLV3r4mMcaYdMjCGq3X7l0fiEhHX5MYY0w6ZGE/Wq812jOA60VkFc5c54IzvsyJiXbss23jIcTLvDWfTw46QtIaHXlu0BGSVlIarqEywjgS1vdWFtZovRa0WTqvsDHGHCCsvQ7cmRaMMSb7ZeH0x15rtMYYEw4h7nVgjDHhYAWtMcb4LMQXw4wxJhyi0aATHMQKWmNM9WJNB8YY47MsLGiTngXXGGOyWpoG/haR1iIyXUSWisgSEbnFXX+viBSLyAJ3uShRJM81WhE5EWgTv4+qvuZ1f2OMyQSNpa0fbRlwu6rOF5G6wMciMtV97XFV/YfXA3mdbnwEcCKwBCj/NaCAFbTGmOySpqYDVV2PO8GBqu4UkUKgZSrH8lqj7aGqx6XyBsYYk1FJ9DoQkf5A/7hVQ1V1aAXbtQE6Ax8BpwM3i8i1wDycWu/Wqt7HaxvthyJiBa0xJvslMXqXqg5V1a5xS0WFbB3gVeBWVd0BPAMcBXTCqfE+liiS1xrtKJzCdgNJjt5ljDEZlcZeByKSh1PIjim/JqWqG+NeHwYkHPLPa0E7HLgGWMS3bbSByK+Rz9iJw8jPzyc3N4c3J01jyMDngoxUob1793HdTXeyr7SUaFmU83qdwc2/uoY/3vs3lnxWRG5uLiccdzR/vev35OVmXy+7p5/5G70v7MXmzV/Ro1v2D94WtrxgmX2TpkFlxJn2ezhQqKqD4tY3d9tvAX4MLE50LK9NB5tVdaKqrlLVL8qXpJOnwb69+7j2shvp0+sq+vT6GWedcxqdTj4hiChVys/PY8SQR3lt5D8ZP/JpZn30MQsXF/Kj83sx6aVhvP7iM+zdu49XJ70ZdNQKjRk9nssu/UXQMTwLW16wzL5J38Dfp+NUMM85oCvXQBFZJCKfAr2A2xIdyGtV6hMRGQtMwmk6AILr3rW75BsAcvNyyc3LzcZR0RARCgpqAVBWVkZZWRkiwlmndd+/Tcdjf8DGTVuCililD2bN5YgjUrrAGoiw5QXL7Js0de9S1fdxmkkPNCXZY3ktaGvhFLDnx+cgoO5dkUiE/0wbzRFtWzNm+DgWzk9Ycw9ENBql3w2/Z03xOq667GJOPP6Y/a+VlpUx6a1p3H3LjQEmNKYaCutYB6qa1N8K8V0mmtQ5gsNqNk4hWuVisRh9ev2MuvXq8M+Rj9HhmKMo+mxFWt8jHXJycnh15NPs2LmLW/7fAxStXE2Hdm0AePAfT3PySSdwcqfsa/YwJsw0C2/BrbKgFZEncWquFVLV31eyfigwFKBDk5N9+8N+545dfPT+PM4657SsLGjL1atbh+5dTuT92fPo0K4N/xwxhq3btvPXhwcEHc2Y6id9d4alTaIa7byMpEhCw0b1KS0tY+eOXdSoWYPTep7CsCEjg451kK+3biM3N5d6deuwZ+9ePpz7CTf8/ArGT3yTWR99zPAhjxCJ2FATxqRd2MajVdWsK8GaNG3MwKfuIxLJIRIR/jvhHaZPnRl0rINs/morf3rwH0RjMTSmXHDOmfQ8/RROOutHNG96OFf3/wMA5559Gr+54eqA0x5sxAuDOePMU2jUqAGFy2fx8IODeXHUuKBjVSpsecEy+yYLa7SiHi7Zi0gT4I/AcUDN8vWqek6iff1sOvDD0sJXgo6QtDBON25MRXaUrKzoKn9SSv5ypecyp/b9Lx/y+3nh9W/XMUAh0Ba4D1gNzPUpkzHGpC5NwySmk9eCtpGqDgdKVfV/qnoDkLA2a4wxGRdT70uGeO1HW+r+v15EfgSsAxr6E8kYY1IXuu5dcR4UkcOA24EngXrArb6lMsaYVGXhxTCvTQdX4Fw4W6yqvYDzcAZTMMaY7BLipoMTVXVb+RNV/VpEOvuUyRhjUhfWW3CBiIg0KB9FXEQaJrGvMcZkTBrnDEsbr4XlYzgDf5d3Mr0CeMifSMYYcwjCWtCq6igRmce3XbouU9Wl/sUyxpgUhbjXAW7BaoWrMSa7hbVGa4wxoWEFrTHG+EujIW46SNWq7Rv8fou0qtXizKAjJK3kk1FBR0ham9N/F3SEpHSqe2TQEZI2deOnQUcIhtVojTHGX2Hu3mWMMeFgBa0xxvgs+5poraA1xlQvWpZ9Ja0VtMaY6iX7yllvo3eJyO9EpIHfYYwx5lBpTD0vmeJ1mMSmwFwRGScivUUkI/PsGGNM0mJJLFUQkdYiMl1ElorIEhG5xV3fUESmikiR+3/CSqinglZVBwAdgOHA9UCRiDwsIkd52d8YYzIljTXaMuB2VT0O6AHcJCLHAXcD01S1AzDNfV4lrzVa1Jkud4O7lAENgPEiMtDrMYwxxndpqtGq6npVne8+3okzQW1LoC8w0t1sJHBpokieLoa5VeZrgS3Av4A7VbVURCJAEXCXl+MYY4zftMz7tiLSH+gft2qoqg6tYLs2QGfgI6Cpqq53X9qA07RaJa+9DhriDI34RfxKVY2JyMUej2GMMb5LZhZxt1A9qGCNJyJ1gFeBW1V1R/wlKlVVEUnYBuF1PNq/ikgXEekLKDArrkpd6OUYxhiTEWns3iUieTiF7BhVfc1dvVFEmqvqehFpDmxKdByv3bv+jNMW0QhoDDwvIgNSi26MMf7RmPelKm7vquFAoaoOintpInCd+/g6YEKiTF6bDn4OnKSqe9wAjwILgAc97m+MMRmRTNNBAqcD1wCLRGSBu+4e4FFgnIj8EvgC6JfoQF4L2nVATWCP+7wGUJxM4nS64PyeDBp0PzmRCCOef4mBf386qCieZXvmvftK+cWAQewrLSMai3HuqZ256cqLWbtxC3cNGsH2nSUc1641D99yPXl52XdDYYuWzRjy7CM0adIYVWX0yHH869nRQcdKqHa92tw68FaO/MGRqCqP3/E4n83/LOhYlcr2zzGARtPTzV9V3wcqO9gPkzmWOL22Emwk8h+gGzAVp432PGAOsNYN9PvK9s3Nb5nW2y8ikQiFS2bS+6KrWLt2PbM/nMLPr/kthYVF6XybtPI7czrGo1VVvtmzl4JaNSkti3Ldnx7jjzdcwYuTpvHDHp248IyuPPDsWI5u04qf9j7rkN8v3ePRHt60MU2bNWHRwkJq1yngrffGc8PVv2P5shVpOb5f49HePuh2Fs9ZzFsvv0VuXi41atWgZEdJWo6d7vFoM/GzV7av+JBLyQ1n9fRc5jSb8V5Gbr7y2o/2dZwq83TgPeBPOO0SH7tLxnTv1pkVK1azatUaSktLGTduAn0uuSCTEZIWhswiQkGtmgCURaOUlUURgTmLlnHeqZ0B6NOrB9PnLAwyZqU2bdzCooXOddmSXbspWr6SZs0PDzhV1QrqFnDCKSfw1stvAVBWWpa2QtYPYfgcA2hMPC+Z4rXXwUgRyQeOwanRLlPVfb4mq0SLls34cu26/c/XFq+ne7fOQUTxLCyZo9EYV975KGs2bObK3mfRulkT6tYuIDcnB4Cmjeqz8attAadMrNURLejY8Vjmf5zdMww0a92M7V9v5w+D/kC7Y9tRtKiIZ//6LHu/2Rt0tAqF5XOcxjbatPHa6+AiYAUwBHgK+FxELqxi+/4iMk9E5sVi2fsb2nxXTk6EVwbdw9RhD7H489WsKg7XNEQABbULGD5qMH+55xF27czuz15Obg7tT2jPG6Pe4OYLb2bP7j30uynhdRWTgKp4XjLFa9PBIKCXqvZU1bOBXsDjlW2sqkNVtauqdo1Eaqcj537rijfQulWL/c9btWzOunXZXSCELXO92gV0O+EHLFy2ip0luymLRgHY+NU2mjaqH3C6yuXm5jJ81BO89spkpkx6J+g4CW1Zv4Ut67ewbMEyAN6f8j7tT2gfcKrKheVznK7uXenktaDdqaqfxz1fCez0IU9Cc+ctoH37trRp05q8vDz69evLpMlvBxHFszBk/nr7TnaU7AZgz959fLiwkHatmtHthKOZ+uEnAEycPpue3U4MMmaVBj31AEXLV/Lc0yMTb5wFtm7eyub1m2nZriUAnU7vxJqiNQGnqlwYPscAsah4XjLFaz+deSIyBRiH00Z7Bc6wiZcBxN0x4btoNMottw5gyhtjyYlEeGHkv1m6dHmm3j4lYci8Zet2Bjw5imgsRiymXHD6yZzdtSNHtWrOXYOG89TYSRzTthWXnXta0FEr1L1HF664si9Llyxj6kzn4/jI/U/w7tQZASer2jN/foa7nryLvLw81q9Zz+O3V/qHYuDC8DkGMnqRyyuv3buer+JlVdUbKnsx3d27zMFsunH/2XTjmZGO7l2rO53nucxps2BqRkplr70OfuF3EGOMSQcPdceM8zpMYk3gl8DxOHeIAVBVTdYYY4KQjU0HXi+GvQg0Ay4A/ge0IqCLYcYYU5Vs7N7l9WJYe1W9QkT6ujcvjAVm+hnMGGNSEc1gbwKvvBa0pe7/20TkBJxRxbP7/kZjzPdSJmuqXnktaIe6Mz0OwBmLsQ7wZ99SGWNMirKxjdZrQfsi8BOgDd9OSpZwnhxjjMm00PY6wBmpazvOSF3ZOeKFMcYQ7hptK1Xt7WsSY4xJg2jMa2eqzPGa6AMR6ehrEmOMSQNV70umVFmjFZFFOGMb5AK/EJGVOE0HgnPrbfaOMGKM+V6KhbDXwcUZSWGMMWkSuu5dqvpFpoIYY0w6hLnXQcoK8mr4/Rbfe026/zroCEkr/sUxQUdISsvns3dm2sr8uHnXoCMEIoxNB8YYEyrZ2OvAClpjTLWShS0HVtAaY6qXbGw6yL46tjHGHIJ0DpMoIiNEZJOILI5bd6+IFIvIAne5KNFxrKA1xlQrsSQWD14AKror9nFV7eQuUxIdxJoOjDHVipK+pgNVnSEibQ71OFajNcZUK2UqnhcR6S8i8+KW/h7f5mYR+dRtWmiQaGMraI0x1Yoi3hfVoaraNW4Z6uEtngGOAjoB64HHEu1gTQfGmGrFY9trylR1Y/ljERkGTE60j9VojTHVSjI12lSISPO4pz8GFle2bTmr0RpjqpV01mhF5CWgJ9BYRNYCfwV6ikgnnHsjVgP/l+g4VtAaY6qVaHp7HVxVwerhyR7H63i0lYWw8WiNMVklC2ey8Twe7U3u/y+6/1/tT5zEnn7mb/S+sBebN39Fj24XBhUjKZbZH1K/MTWvvR2p2wBQSme9Sel7E6hx6Q3knHAKRMuIbVnPntGPwzclQcc9SBjOcbwW7Vpy21N37H9++BHN+PegsUwZMSnAVAeLpbFGmy5VXgxT1S/cMWnPU9W7VHWRu9wNnJ+ZiN81ZvR4Lrv0F0G8dcoss09iUfa+9i92P3Qju//xB/LPuphIs9aUffYJux/+DbsfuYnYpmLyz+8XdNIKheIcx1m3spg7L7qNOy+6jT9efDv7vtnLnLdmBx3rIJrEkileex2IiJwe9+S0JPZNqw9mzWXr19uCeOuUWWZ/6I6txNaucJ7s/YbohjVI/cZEP/sEYs4lkdiqz4jUbxxgysqF4RxX5oTTT2TDmg1sKd4cdJSDpPkW3LTwejHsl8AIETkMZ76wrcANvqUyJknS8HByWh3FntXfHaA779TzKZ0/I6BU1dfpfc5k1sTsPK8xyb6mA08Frap+DJzkFrSo6vaqtndvY+sPUCO/Efm59Q41pzGVy69JrV/9ib2vDoU933y7+oKforEoZXOnBxiu+snNy6Xrud0Z+7dRQUepUDToABXw3L1LRH4EHA/UFPc3hqreX9G27m1sQwHq1W6XjePwmuoikkOtX/+J0nnvUbbwg/2rc085l9wTurN7yD0BhqueOvXswqrFK9i+pcr6VmDC2OsAABF5FigAegH/Ai4H5viYyxhPal59K7ENX1L67uv71+UcezL5517ON4PvgtK9Aaarns7ocxbvT5wZdIxKha7XQZzTVPVaYKuq3gecChztX6zKjXhhMO9Mf5UOHdpRuHwW11ybnVeU41lmf+S0O468U35IztEnUXD3kxTc/SQ5x3WlZr/fIDVrUevmhyi4+0lqXHlz0FErFIZzfKAatWpw4pknMefND4OOUqls7HUg6mFuXhGZo6rdRWQ2cBnwNbBYVdsn2teaDkxFbBZc/13QuGPQEZL2yhcTDrk6Oqrlzz2XOdcWj85I9ddrG+0kEakP/B2Yj/PLYJhvqYwxJkWZ7LblldeC9jMgqqqvishxQBfgP/7FMsaY1ESzr4nWcxvtn1V1p4icAZyDc0HsGf9iGWNMarLxhgWvBW1517QfAcNU9Q0g359IxhiTujAXtMUi8hzwU2CKiNRIYl9jjMkYFe9LpngtLPsBbwEXqOo2oCFwp2+pjDEmRdlYo/V6C+5u4LW45+txJiUzxpisEupbcI0xJgxCewuuMcaERZj70RpjTChYQWuMMT7Lxnv+raA1xlQr1kZrjDE++172Otht44H6rknBYUFHSFqDoQuDjpCUnf/9a9ARklb3wvuCjhCIWBY2HliN1hhTrWTjxTC7jdYYU62kc+BvERkhIptEZHHcuoYiMlVEitz/GyQ6jhW0xphqJc234L4A9D5g3d3ANFXtAExzn1fJClpjTLVSJup5SURVZ+DMKBOvLzDSfTwSuDTRcaygNcZUK8k0HYhIfxGZF7f09/AWTd3xXgA2AE0T7WAXw4wx1UoyF8NUdSgwNNX3UlUVSVw19lSjFZHfeWnwNcaYoMVQz0uKNopIcwD3/02JdvDadNAUmCsi40Skt4hk4b0XxhiTkenGJwLXuY+vAyYk2sFTQauqA4AOwHDgeqBIRB4WkaNSy2mMMf5IZ68DEXkJ+BD4gYisFZFfAo8C54lIEXCu+7xKntto3baIDTiNv2VAA2C8iExV1bu8HscYY/wUTeOdYap6VSUv/TCZ43gqaEXkFuBaYAvODLh3qmqpiESAIsAKWmNMVsjGO8O81mgbAJep6hfxK1U1JiIXpz+WMcakRrNwrIOEbbQikgNceWAhW05VC9OeyhhjUpSNkzMmLGhVNQosE5EjMpDHkwvO78mSxTP4bOn73HXnTUHH8SRMmVu0bMb4Sc/zv9mTeO/Difzqxp8HHcmTMJzjDV/v4FePj+Oy+5/nsgdeYMy78wH47MtNXDNwLP0eHsXPHh3NotXZOfdpGM5xBrp3JS2ZpoMlIjIHKClfqap9fElVhUgkwpDBD9H7oqtYu3Y9sz+cwqTJb1NYWJTpKJ6FLXNZWRn3DRjIooWF1K5TwFvvjWfG9A9ZvmxF0NEqFZZznJMT4fafnM2xRzSlZM8+rnp0ND2OPZInXp/B//3oVM44vi0zF6/kiddnMPy2nwYd9zvCco6zr+HAe0H7Z19TJKF7t86sWLGaVavWADBu3AT6XHJB1n2z44Ut86aNW9i0cQsAJbt2U7R8Jc2aH57VBW1YznGTw+rQ5LA6ANSumU+7Zg3ZtG0nIlDyjTN2865v9u7fJpuE5RyXZWFR66mgVdX/+R3EqxYtm/Hl2nX7n68tXk/3bp0DTJRYGDOXa3VECzp2PJb5H38adJQqhfEcF3+1nc++3ETHNs258/Je/PapVxn02v+IKYy8o7JeRcEJyzkO5cUwABHZKSI7Dli+FJHXRaRdBdvvH6ghFiup6JAmBApqFzB81GD+cs8j7Npp38d02r1nH3cMncidl/eiTq0avDJzIXdc3pO3Hv4/7ri8J/eNfivoiKEVyothrieAO4GWQCvgDmAs8DIw4sCNVXWoqnZV1a6RSO10ZQVgXfEGWrdqsf95q5bNWbduQ1rfI93CmDk3N5fho57gtVcmM2XSO0HHSShM57g0GuX2YRO5qPux/LBzBwAmzV7CDzs5j8/vcjSLv8i+7GE5x5rEv0zxWtD2UdXnVHWnqu5wR7y5QFX/jXOhLGPmzltA+/ZtadOmNXl5efTr15dJk9/OZISkhTHzoKceoGj5Sp57emTijbNAWM6xqnLfi2/Ttlkjrvlh1/3rmxxWh3lFawGYs2wNRzSpH1TESoXlHGdjjdbrxbDdItIPGO8+vxzY4z7OaININBrlllsHMOWNseREIrww8t8sXbo8kxGSFrbM3Xt04Yor+7J0yTKmznwNgEfuf4J3p84IOFnlwnKOF6woZvKcpXRo0Zh+D48C4Hd9zuAvV5/HwFemE40p+Xk5/Pnq8wNOerCwnOOoZl8braiHUG477GDgVJyCdTZwG1AMnKyq71e2b25+y+z7qquZMM6Cu3n39qAjJMVmwc2Msn3Fhzwy4M+O/LHnMmfsF69nZCRCr70OVgKXVPJypYWsMcZkWjb2OvA6qEwT4NdAm/h9VPUGf2IZY0xqwjyozARgJvAOEPUvjjHGHJpM3lrrldeCtkBV/+hrEmOMSYNsbDrw2r1rsohc5GsSY4xJg6iq5yVTvNZobwHuEZG9QCkgOJMu1PMtmTHGpCC0TQeqWldEGuLMG1bT30jGGJO60F4ME5Ff4dRqW7tezc0AAA8RSURBVAELgB7AByQ5b44xxvgtzG20twDdgC9UtRfQGQhXj3NjzPdCmAf+3qOqe0QEEamhqp+JyA98TWaMMSnwcrdrpnktaNeKSH3gP8BUEdkKVDiHmDHGBCmd042ni9eLYT92H94rItOBw4A3fUtljDEpCm2vg3jZNNuCMcYcKMxNB98bBXk1go7wvXBqk2OCjpCUE698LugISdtdNCnoCIFIZ41WRFYDO3GGHihT1a5V71ExK2iNMdWKD927eqnqlkM5gBW0xphqJRsH/vbaj9YYY0IhmX608RPJukv/Aw6nwNsi8nEFr3lmNVpjTLWSTButO//h0Co2OUNVi0XkcJyurZ+patJzOlmN1hhTraiq58XDsYrd/zcBrwPdU8lUaY1WRHZS8cSLNnKXMSZrpavXgYjUBiKqutN9fD5wfyrHqrSgVdW6KeYzxpjApLHXQVPgdREBp6wcq6op3aiVsI1WRI6oaL2qrknlDY0xxk9RTc9Aie6ktCel41heLoa9Efe4JtAWWAYcn44AxhiTTqG8M0xVO8Y/F5EuwG99S2SMMYeguox1MF9ETvEjjDHGHKpsHPjbSxvtH+KeRoAuwDrfEhljzCGIhbHpAIjvfVCG02b7qj9xjDHm0ISqRisiL6rqNcA2VR2cwUzGGJOydPU6SKeqarQni0gL4AYRGYVzo8J+qvq1r8mqcMH5PRk06H5yIhFGPP8SA//+dFBRPHn6mb/R+8JebN78FT26XRh0nIRatGzGkGcfoUmTxqgqo0eO41/Pjg46VkLjZo9h967dxGIxomVRfn1Rdl+zza+Rz9iJw8jPzyc3N4c3J01jyMDsGo5x7759XH/7/ewrLSUajXLemadw07VXMHbCW4x+/b98uW4jM155jgaHZc/9S2FrOngWmAa0Az7muwWtuuszLhKJMGTwQ/S+6CrWrl3P7A+nMGny2xQWFgURx5Mxo8cz9LlRPDfsH0FH8aSsrIz7Bgxk0cJCatcp4K33xjNj+ocsX7Yi6GgJ3XLF7WzfuiPoGJ7s27uPay+7kd0l35Cbm8vLk4czY9osFny8OOho++Xn5TF84AAKatWktKyM6267lzO6daLz8Udz9ilduOHOlG6U8lU2Nh1UOtaBqg5R1WOBEaraTlXbxi2BFLIA3bt1ZsWK1axatYbS0lLGjZtAn0suCCqOJx/MmsvWr7cFHcOzTRu3sGhhIQAlu3ZTtHwlzZofHnCq6ml3yTcA5OblkpuXS7ZVxkSEglo1ASgri1IWjSIIx7ZvS8tmTQJOV7GYquclU6q8GCYiOUCvDGXxpEXLZny59ttOD2uL19O9W+cAE1VvrY5oQceOxzL/40+DjpKQqjLopYGoKhNGT2bSmDcS7xSwSCTCf6aN5oi2rRkzfBwL52dPbbZcNBrjpzfdw5p1G7iyz/mceGz7oCNVKRtrtFUWtKoaFZFlInJEMrfcuuM29geQnMOIRGofYkwThILaBQwfNZi/3PMIu3aWBB0noZt+fCtbNmyhfqP6PP7yQNZ8voaFHy0KOlaVYrEYfXr9jLr16vDPkY/R4ZijKPosu5pocnIijH/2UXbsKuHW+wZRtOpLOrRtHXSsSkU1GnSEg3gZJrEBsEREponIxPKlqh1UdaiqdlXVrukuZNcVb6B1qxb7n7dq2Zx16zak9T0M5ObmMnzUE7z2ymSmTHon6DiebNngzDay7attzPjv+xzbKTzzku3csYuP3p/HWeecFnSUStWrU5tuJx3HrHkLg45SpXQOk5guXgraPwMX4wwP9ljcEoi58xbQvn1b2rRpTV5eHv369WXS5LeDilNtDXrqAYqWr+S5p0cGHcWTmrVqUqt2rf2Pu53dlZXLVgcbKoGGjepTt14dAGrUrMFpPU9hZdHqYEMd4OttO9ixy/lrZs/efcyev4i2rVsk2CtYycywkClexjrIqunFo9Eot9w6gClvjCUnEuGFkf9m6dLlQceq0ogXBnPGmafQqFEDCpfP4uEHB/PiqHFBx6pU9x5duOLKvixdsoypM18D4JH7n+DdqUkPLJ8xDZo04OHh9wGQk5PD1P9MY857cwNOVbUmTRsz8Kn7iERyiESE/054h+lTZwYd6zs2f72VAX9/hmgshsaU88/uwdk9ujDm9TcZ8cokvvp6Gz/5vz9yZvfO3PeHlGd6SatsHFRGEoUSkR7Ak8CxQD6QA5R4Hfg7N79l9n3VVQjjdOO182oGHSFp7Ws3DzpCUjbsC0+vkXJL5g0LOkLS8o/sIom3qlrz+sd5LnPWb1t6yO/nhZdbcJ8CrgReAboC1wJH+xnKGGNSlY29DjzNGaaqnwM5qhpV1eeB3v7GMsaY1EQ15nnJFC812t0ikg8sEJGBwHpsUkdjTJbKxjZaLwXmNe52NwMlQGvgJ36GMsaYVIXuzjAAVf1CRGoBzVX1vgxkMsaYlIWyRisilwALgDfd550S3bBgjDFBycZ+tF6aDu4FugPbAFR1Ac4EjcYYk3Wy8c4wLxfDSlV1uzu3ebnsq5sbYwzhG/i73BIR+RmQIyIdgN8DH/gbyxhjUpONA39X2nQgIi+6D1cAxwN7gZeAHcCt/kczxpjkha3poHwqm5/ijEkbP5BMAbDHz2DGGJOKdN4ZJiK9gcE4Qw/8S1UfTeU4XqeymRf/3gQ4lY0xxlQlXTVVd+KDp4HzgLXAXBGZqKpLkz1WpQWtqg4BhojIM6r6m5TTGmNMBqWxjbY78LmqrgQQkZeBvkD6Ctpyh1rIlu0r9m10HBHpr6pD/Tp+uoUtL4Qvc9jygmVOt2TKnPjZYFxD476ulsCXca+tBU5JJVPYxyzIjgEwvQtbXghf5rDlBcscmPjZYNzFl18eYS9ojTHGL8U4Y7uUa+WuS5oVtMYYU7G5QAcRaeuOYHglkNLwA15uWMhmWdlGVIWw5YXwZQ5bXrDMWUlVy0TkZuAtnO5dI1R1SSrHSjiVjTHGmENjTQfGGOMzK2iNMcZnoS5oRaSNO+BNKvvuSnceD+95vYg8FcD7thGRxZl+32xi5+BgIvJ7ESkUkTGZOlYQP3fZIOwXw9oAPwPGHviCiOSqalnGExmTRj5/jn8LnKuqa1M9QFy+Qz5WdRZIjdatXRSKyDARWSIib4tILRE5SkTeFJGPRWSmiBzjbv+CiFwet3/5b8VHgTNFZIGI3ObWGCeKyLvANBGpIyLTRGS+iCwSkb4+fT3XisinIrJQRF4UkUtE5CMR+URE3hGRphXs84KIPCMis0VkpYj0FJER7nl5wYeYORWc71+LyFw396siUhCX7VkRmSciy0XkYnf99SIyQUTeE5EiEfmru/5+Edk/opuIPCQit/jwNSAitUXkDTfzYhH5qYj8xf06FovIUHEHTxaRk93tFgI3+ZGngnz/cT+/S9y7jhCRXe45Weh+v5u6649yny8SkQfLP9fuZ2GmODOZLPXj/IrIszjjlfxXRP7kfvbmuJ/Zvu42bdwc893ltEryxR/rNhG5V0TuiHuvxSLS5lDyhl4yQ4qla8GpiZYBndzn44Cf4wxi08Fddwrwrvv4BeDyuP13uf/3BCbHrb8e5za5hu7zXKCe+7gx8Dnf9rTYlaav5XhgOdDYfd4QaBD3Pr8CHovL91Tc1/QyziA9fXGGn+yI88vv4/Jz4/P5bhS3zYPA7+Kyvelm6eCe05pu/vVAI6AWsBjo6h5/vrtvBGdozUbpyn/A1/ITYFjc88PKv9/u8xeBS9zHnwJnuY//DizOwGe7/LNXfn4a4QzCVJ5pIDDAfTwZuMp9fOMBn+sSoG3c9y/t5xdY7f5cPAz83F1X3/0818YZpa+mu74DMK+ifPHHch/fC9wR99pioE06f+7CtgTZdLBKnWlxwClY2gCnAa/It7M51EjhuFNV9Wv3sQAPi8hZQAzn3uWmwIZUQ1fgHOAVVd0CoKpfi0hH4N8i0hzIB1ZVsu8kVVURWQRsVNVFACKyBOd8LKhkv1RUdL5PEJEHcX646uD0Fyw3TlVjQJGIrASOcddPVdWv3JyvAWeo6hMi8pWIdMY5v5+Ub+ODRcBjIvI3nF+yM0XkJyJyF07B0BBnsPqZQH1VneHu9yJwoU+Z4v1eRH7sPm6NU0DtwylUwTn357mPTwUudR+PBf4Rd5w5qroKQFVX+3x+zwf6xNVCawJHAOuAp0SkExAFjq4on0ksyIJ2b9zjKM4HaJuqdqpg2zLcZg4RieAUXpUpiXt8NdAEOFlVS0VkNc6HyG9PAoNUdaKI9MT5DV+R8nMQ47vnI0b6vzcHnu9aODXXS1V1oYhcj1NTKXdgB2tNsP5fODXeZsCIQ05bCVVdLiJdgIuAB0VkGk6zQFdV/VJE7iUz3+ODuN/rc4FTVXW3iLznZilVtzqHc+69fG9LDnju5/kV4Cequuw7K51zuRE4CefnL34M6gPzxdv/8+oK5PuRTbKp18EOYJWIXAEgjpPc11YDJ7uP+wB57uOdQN0qjnkYsMktZHsBR6Y9NbwLXCEijQBEpKH7vuX3RF/nw3umS11gvYjk4fxSineFiERE5Cic9rfyH8LzRKShOFPQXwrMcte/DvQGuvHdmnFaiTMY/W5VHY3THNDFfWmLiNQBLgdQ1W3ANhE5w339wK/PD4cBW91C9higR4LtZ+M0hYBze2dV/Dy/bwG/i2vb7uyuPwxY7/5lcw3O3VFerMb9vri/FL/3k7lmW6+Dq4FnRGQATmH6MrAQGAZMcC9qvMm3v00/BaLu+heArQccbwwwyf3TfB7wWboDq+oSEXkI+J+IRIFPcGqwr4jIVpyCOFs/aH8GPgI2u//H/9JaA8wB6gE3quoe9+dwDvAqzgAbo1V1HoCq7hOR6Th/lUR9zNwR+LuIxIBS4Dc4Bf5inCahuXHb/gIYISIKvO1jpnJvAjeKSCHOL6bZCba/FRgtIn9y991e2YY+n98HgCeAT92/GFcBFwP/BF4VkWv57s9dIq8C17pNYB/htPl+r9ktuOYg4vR6mKyq4w9Yfz3On+g3V7BPBJgPXKGqRZnIGXbi9PL4xm2nvxLnwliFPWPs/IZbNjUdmJASkeNwenRMs0IgKScDC0TkU5x+qLdXtJGd3/CzGq0xxvjMarTGGOMzK2iNMcZnVtAaY4zPrKA1xhifWUFrjDE++/8SCRwh2ot8swAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LGpgwFQqkpyU"
      },
      "execution_count": 37,
      "outputs": []
    }
  ]
}